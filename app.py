# === å…±é€šå‡¦ç†é–¢æ•° ===
def update_prob_buffer(prob_df):
    curr_probs = []
    if prob_df is not None:
        for _, r in prob_df.iterrows():
            for key in ["P_up", "P_dn"]:
                val = r.get(key, None)
                if isinstance(val, (int, float)) and not np.isnan(val):
                    curr_probs.append(float(val))
    if curr_probs:
        st.session_state.prob_buffer.extend(curr_probs)
        st.session_state.prob_buffer = st.session_state.prob_buffer[-3000:]
    return curr_probs

def calc_psi_and_exrate(curr_probs, baseline_probs, theta_up, theta_dn):
    psi_val, sev, ex_rate = float('nan'), "n/a", float('nan')
    if baseline_probs is not None and len(st.session_state.prob_buffer) >= 200:
        curr = np.array(st.session_state.prob_buffer[-1000:], dtype=float)
        try:
            psi_val, _ = compute_psi(baseline_probs, curr, edges=None)
            sev = psi_severity(psi_val)
        except Exception as e:
            print(f"PSIè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        try:
            theta_rep = float(np.median([theta_up, theta_dn]))
            ex_rate = threshold_exceed_rate(np.array(curr_probs, float), theta_rep)
        except Exception as e:
            print(f"Î¸è¶…éç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
    return psi_val, sev, ex_rate
# ãƒ†ã‚¹ãƒˆç”¨: make_features_for_levelã®ç›´æ¥å‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«ç§»å‹•ï¼‰
if __name__ == "__main__":
    import pandas as pd
    from datetime import datetime
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    df = pd.DataFrame({
        "close": [100, 101, 102, 103, 104],
        "high": [101, 102, 103, 104, 105],
        "low": [99, 100, 101, 102, 103],
        "open": [100, 100, 101, 102, 103],
    }, index=pd.date_range("2025-09-01", periods=5, freq="D"))
    ts = df.index[-1]
    level = 102
    dir_sign = 1
    touch_buffer = 0.5

# ãƒ†ã‚¹ãƒˆç”¨: make_features_for_levelã®ç›´æ¥å‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«ç§»å‹•ï¼‰
if __name__ == "__main__":
    import pandas as pd
    from datetime import datetime
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    df = pd.DataFrame({
        "close": [100, 101, 102, 103, 104],
        "high": [101, 102, 103, 104, 105],
        "low": [99, 100, 101, 102, 103],
        "open": [100, 100, 101, 102, 103],
    }, index=pd.date_range("2025-09-01", periods=5, freq="D"))
    ts = df.index[-1]
    level = 102
    dir_sign = 1
    touch_buffer = 0.5
from monitoring import compute_psi, psi_severity, threshold_exceed_rate, healthcheck, safe_call
## --- ç™ºæ³¨ã®æœ€çµ‚ã‚²ãƒ¼ãƒˆã« enable_trading ã‚’åæ˜  ---
# pred_df, windows_dfãŒæƒã£ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä»¥ä¸‹ã‚’å¿…ãšé€šã™
# pred_df: [timestamp, proba, theta, signal, ...]
# windows_df: [start, end]ï¼ˆJSTï¼‰
# ä¾‹:
# in_news = is_in_any_window(pred_df["timestamp"], windows_df[["start","end"]])
# pred_df["trade_ok"] = (pred_df["signal"] == 1) & (~in_news)
# â†“ pred_dfç”Ÿæˆå¾Œã€trade_okåˆ—ãŒã§ããŸç›´å¾Œã«ä»¥ä¸‹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
# pred_df["trade_ok"] = pred_df["trade_ok"] & st.session_state.enable_trading
#
# live_rows = pred_df.loc[pred_df["trade_ok"]]
# if live_rows.empty:
#     st.info("ç¾åœ¨ã€ç™ºæ³¨æ¡ä»¶ã‚’æº€ãŸã™ã‚·ã‚°ãƒŠãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆEVã‚²ãƒ¼ãƒˆ/ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ‘åˆ¶/Î¸é©ç”¨å¾Œï¼‰ã€‚")
# else:
#     # live_rows ã‚’ç™ºæ³¨ãƒ•ãƒƒã‚¯/é€šçŸ¥ã¸
#     pass  # ã“ã“ã«ç™ºæ³¨å‡¦ç†ã‚’è¨˜è¿°


import streamlit as st
from build_level_break_prob_table import build_level_break_prob_table
st.set_page_config(page_title="FX è‡ªå‹•ãƒ©ã‚¤ãƒ³æç”» - å®Œå…¨ç‰ˆ", page_icon="ğŸ“ˆ", layout="wide")
from inference_break import load_break_meta

# --- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç‡ã®èª­ã¿è¾¼ã¿ï¼ˆåˆæœŸåŒ–ï¼‰ ---
import json
try:
    with open("models/break_meta.json", "r", encoding="utf-8") as f:
        _break_meta = json.load(f)
    baseline_proba = float(_break_meta.get("baseline_proba", 0.5))
    # --- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç‡åˆ†å¸ƒï¼ˆé…åˆ—ï¼‰ã‚‚ãƒ­ãƒ¼ãƒ‰ ---
    with open("reports/break_calibration.json", "r", encoding="utf-8") as f:
        _calib = json.load(f)
    baseline_probs = _calib.get("prob_mean", None)
    if baseline_probs is not None:
        import numpy as np
        baseline_probs = np.array(baseline_probs, dtype=float)
    else:
        baseline_probs = None
except Exception:
    baseline_proba = 0.5  # èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    baseline_probs = None

# === æ¨å¥¨è¡Œå‹•ï¼ˆæ„æ€æ±ºå®šãƒãƒªã‚·ãƒ¼ï¼‰é–¢é€£ ===
from decision_policy import DecisionParams, EVConfig, recommend_action
from is_in_any_window import is_in_any_window  # æ—¢ã«åˆ©ç”¨ä¾‹ã‚ã‚Š

# ---- EV Gate: åˆæœŸåŒ–ï¼ˆæœ€åˆã®1å›ã ã‘ï¼‰ ----
if "enable_trading" not in st.session_state:
    try:
        meta = load_break_meta("models/break_meta.json")
        ev = float(meta.get("ev_per_trade", 0.0))
    except Exception:
        ev = 0.0  # èª­ã¿è¾¼ã¿å¤±æ•—ï¼å®‰å…¨å´ã§OFF
    st.session_state.enable_trading = (ev > 0)

# ---- UIè¡¨ç¤ºï¼šä»¥é™ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã§ä¸Šæ›¸ãå¯èƒ½ ----
col1, col2 = st.columns([1,1])
with col1:
    enable_trading = st.toggle(
        "é‹ç”¨ãƒ¢ãƒ¼ãƒ‰ï¼ˆEVã‚²ãƒ¼ãƒˆï¼‰",
    key="enable_trading"
    )
with col2:
    try:
        meta = load_break_meta("models/break_meta.json")
        ev = float(meta.get("ev_per_trade", float("nan")))
    except Exception:
        ev = float("nan")
    st.metric("EV per trade", f"{ev:.4f}" if ev == ev else "N/A")  # NaNå¯¾å¿œ
# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç¢ºèªç”¨ï¼ˆUI/æ¤œè¨¼ã‚¿ãƒ–ç­‰ã§åˆ©ç”¨ï¼‰ ---
# pred_dfã«[session, signal]åˆ—ãŒã‚ã‚‹å‰æ
# ä¾‹: cov_by_sess = pred_df.groupby("session")["signal"].mean()
# 0.25ã€œ0.35ã«åã¾ã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã€å¤–ã‚Œã‚‹å ´åˆã¯æ¬¡å›å­¦ç¿’ã§target_covã‚’å¾®èª¿æ•´
# --- ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ‘åˆ¶ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æ³¨æ–‡ã‚²ãƒ¼ãƒˆã«åæ˜  ---
from is_in_any_window import is_in_any_window

# pred_df, windows_dfãŒæƒã£ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ä»¥ä¸‹ã‚’å¿…ãšé€šã™
# pred_df: [timestamp, proba, theta, signal, ...]
# windows_df: [start, end]ï¼ˆJSTï¼‰
# ä¾‹:
# in_news = is_in_any_window(pred_df["timestamp"], windows_df[["start","end"]])
# pred_df["trade_ok"] = (pred_df["signal"] == 1) & (~in_news)
from inference_break import load_break_model, load_break_meta, predict_with_session_theta

import streamlit as st

# === app.py ã«è¿½åŠ ï¼ˆimport ã®ä¸‹ã‚ãŸã‚Šï¼‰ ===
import pandas as pd

# å­¦ç¿’æ™‚ã¨åŒã˜é–¢æ•°ã‚’ä½¿ã†
from ml.time_consistency import build_features
from ai_train_break import augment_features  # â† Step3-Aã§è¿½åŠ ã—ãŸé–¢æ•°

def prepare_df_feats_for_inference(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    raw_df: åˆ—ã« timestamp, open, high, low, close, volume ã‚’å«ã‚€ DataFrameï¼ˆæ˜‡é †ï¼‰
    æˆ»ã‚Šå€¤: df_featsï¼ˆå­¦ç¿’ã¨åŒã˜ç‰¹å¾´é‡åˆ— + timestampï¼‰ã€‚æ¬ æã¯0ã§åŸ‹ã‚ã€‚
    """
    if not {"timestamp","open","high","low","close"}.issubset(set(c.lower() for c in raw_df.columns)):
        # å¤§æ–‡å­—ã‚±ãƒ¼ã‚¹ã‹ã‚‰æ¨™æº–åŒ–
        rename_map = {}
        for c in raw_df.columns:
            lc = c.lower()
            if lc in ["timestamp","open","high","low","close","volume"]:
                rename_map[c] = lc
        raw_df = raw_df.rename(columns=rename_map)

    # æ™‚åˆ»ã®æ•´å½¢ã¨æ˜‡é †ä¿è¨¼
    raw_df = raw_df.copy()
    raw_df["timestamp"] = pd.to_datetime(raw_df["timestamp"])
    raw_df = raw_df.sort_values("timestamp").reset_index(drop=True)
    assert raw_df["timestamp"].is_monotonic_increasing

    # 1) åŸºæœ¬ç‰¹å¾´
    base_feats = build_features(raw_df)
    # 2) è¿½åŠ ç‰¹å¾´ï¼ˆãƒªãƒ¼ã‚¯ãªã—ï¼‰
    raw_l = raw_df.rename(columns=str.lower)
    df_feats = augment_features(base_feats, raw_l)

    # æ¨è«–ã§ä½¿ã†ã®ã§æ¬ æã¯0åŸ‹ã‚ï¼ˆå­¦ç¿’å´ã¨åŒã˜æ–¹é‡ï¼‰
    df_feats = df_feats.fillna(0.0)

    # é‡è¦ï¼štimestamp ã¯æ®‹ã™ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¤å®šã«ä½¿ã†ï¼‰
    if "timestamp" not in df_feats.columns:
        df_feats.insert(0, "timestamp", raw_df["timestamp"].values)

    return df_feats

@st.cache_resource
def _load_model_and_meta():
    model, use_cols = load_break_model("models/break_model.joblib")
    meta = load_break_meta("models/break_meta.json")
    return model, use_cols, meta

# --- å¾©å…ƒç”¨ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ï¼ˆå‘¼ã³å‡ºã•ãªã„ï¼‰---

import numpy as np
from typing import Iterator, Tuple
import json, datetime as dt
import pandas as pd

def _pick_session_from_ts(ts: pd.Timestamp):
    h = ts.hour
    if 9 <= h < 15:  return "Tokyo"
    if 16<= h < 24:  return "London"
    if h>=22 or h<5: return "NY"
    return None

def pick_theta_for_now(meta):
    # æ™‚é–“å¸¯ã”ã¨Î¸ â†’ ãªã‘ã‚Œã°ã‚°ãƒ­ãƒ¼ãƒãƒ«Î¸ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    now = pd.Timestamp.now(tz="Asia/Tokyo")
    sess = _pick_session_from_ts(now)
    tbs  = meta.get("theta_by_session", {})
    if sess and sess in tbs and "theta" in tbs[sess]:
        return float(tbs[sess]["theta"])
    return float(meta.get("threshold", 0.93))

class PurgedTimeSeriesSplit:
    def __init__(self, n_splits: int = 5, test_size: int | None = None, embargo: int = 0):
        self.n_splits = int(n_splits)
        self.test_size = None if test_size is None else int(test_size)
        self.embargo = max(0, int(embargo))
    def get_n_splits(self, X=None, y=None, groups=None) -> int:
        return self.n_splits
    def split(self, X, y=None, groups=None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        # ã‚¢ãƒ—ãƒªã§ã¯å­¦ç¿’åˆ†å‰²ã¯ä½¿ã„ã¾ã›ã‚“
        raise RuntimeError("PurgedTimeSeriesSplit is a pickle shim for unpickling only.")

# --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---

# --- å…±é€šãƒ­ãƒ¼ãƒ€ã¨æ¨è«–ä¸€æ‹¬é–¢æ•°ã«çµ±ä¸€ ---
@st.cache_resource
def _load_model_and_meta():
    model, use_cols = load_break_model("models/break_model.joblib")
    meta = load_break_meta("models/break_meta.json")
    return model, use_cols, meta

# æ¨è«–æ™‚ã®åˆ©ç”¨ä¾‹
model, use_cols, meta = _load_model_and_meta()
# raw_df = ...ï¼ˆæ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†ï¼‰
# df_feats = prepare_df_feats_for_inference(raw_df)
# pred = predict_with_session_theta(df_feats, model, use_cols, meta)
import requests
import pandas as pd
# ======== ãƒ­ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæ™‚ç‚¹ã”ã¨å†è¨ˆç®—ï¼‰ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ========
def _build_windows_until(t_end: pd.Timestamp, imp_threshold: int) -> pd.DataFrame:
    """
    t_endï¼ˆå«ã‚€ï¼‰ã¾ã§ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‹ã‚‰ã€é‡è¦åº¦åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æ§‹ç¯‰
    """
    if news_df is None or news_df.empty:
        return pd.DataFrame(columns=["start","end","importance","title"])
    past_events = news_df[news_df["time"] <= t_end]
    if past_events.empty:
        return pd.DataFrame(columns=["start","end","importance","title"])
    return build_event_windows(past_events, imp_threshold=imp_threshold, mapping=imp_map)

def _is_suppressed_at(ts: pd.Timestamp, win_df: pd.DataFrame, news_win_minutes: int,
                      imp_min: int, mode_label: str) -> bool:
    """
    ãƒ•ã‚£ãƒ«ã‚¿æ–¹å¼ã«å¿œã˜ã¦æŠ‘åˆ¶åˆ¤å®šï¼ˆã™ã¹ã¦éå»é™å®šï¼‰
    """
    if news_filter_mode == "é‡è¦åº¦åˆ¥ï¼ˆèµ¤å½±ã¨åŒã˜ï¼‰":
        if win_df is None or win_df.empty:
            return False
        return is_suppressed(ts, win_df)
    else:
        if news_df is None or news_df.empty:
            return False
        win = pd.Timedelta(minutes=news_win_minutes)
        cond = (news_df["time"] <= ts) & (news_df["importance"] >= imp_min) & news_df["time"].between(ts - win, ts + win)
        return bool(cond.any())

def backtest_rolling(df: pd.DataFrame,
                     fwd_n: int,
                     break_buffer_arg: float,
                     spread_pips: float,
                     news_win: int,
                     news_imp_min: int,
                     apply_news: bool,
                     signal_mode: str,
                     retest_wait_k_arg: int,
                     touch_buffer: float
                     ) -> pd.DataFrame:
    """
    â˜… æœªæ¥æƒ…å ±ãƒªãƒ¼ã‚±ãƒ¼ã‚¸ã‚’é™¤å»ã—ãŸãƒ­ãƒ¼ãƒªãƒ³ã‚°ç‰ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
      - å„æ™‚ç‚¹ i ã§ã€éå»ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰ï¼š
          * ãƒ”ãƒœãƒƒãƒˆâ†’æ°´å¹³ç·šï¼ˆDBSCANï¼‰
          * å›å¸°ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒãƒ£ãƒãƒ«
          * ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ‘åˆ¶ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        ã‚’å†è¨ˆç®—ã—ã¦åˆ¤å®šã—ã¾ã™ã€‚
    """
    rows = []
    if len(df) <= fwd_n + 2:
        return pd.DataFrame(columns=["time","mode","level_or_val","dir","entry","exit","ret_pips","retest_index","retest_hit"])

    pv_local = pip_value("USDJPY")
    close_s = df["close"]

    # i ã®é–‹å§‹ä½ç½®ï¼ˆæœ€ä½é™ã€ãƒ”ãƒœãƒƒãƒˆ/DBSCAN/å›å¸°ãŒå®‰å®šã™ã‚‹åˆ†ã ã‘é€²ã‚ã‚‹ï¼‰
    min_start = max(2, reg_lookback, look * 4)

    for i in range(min_start, len(df) - fwd_n):
        # ---- éå»ã®ã¿æŠ½å‡º
        past = df.iloc[:i+1]
        t = past.index[-1]
        c  = float(past["close"].iloc[-1])
        l1 = float(past["low"].iloc[-2]); h1 = float(past["high"].iloc[-2])

        # ---- ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ‘åˆ¶ï¼ˆéå»ã®ã¿ï¼‰
        win_df_past = _build_windows_until(t, news_imp_min) if apply_news else pd.DataFrame()

        # æŠ‘åˆ¶ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ¤å®šã®â€œåŸºæº–æ™‚åˆ»â€ã§è¦‹ã‚‹ï¼‰
        if apply_news and _is_suppressed_at(t, win_df_past, news_win, news_imp_min, signal_mode):
            continue

        # ---- éå»ã®ã¿ã§ãƒ¬ãƒ™ãƒ«ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰å†è¨ˆç®—
        try:
            piv_hi_past, piv_lo_past = swing_pivots(past, look)
            lvls_past = horizontal_levels(piv_hi_past, piv_lo_past, eps=eps, min_samples=min_samples)
        except Exception:
            lvls_past = []

        tr_past = regression_trend(past, reg_lookback, use="low")

        # ---- ãƒ¢ãƒ¼ãƒ‰åˆ¥åˆ¤å®š
        if signal_mode == "æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
            for lv in lvls_past:
                if (c > lv + break_buffer_arg) and (l1 <= lv):
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, lv, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="æ°´å¹³ãƒ–ãƒ¬ã‚¤ã‚¯ä¸Š", level_or_val=float(lv),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if (c < lv - break_buffer_arg) and (h1 >= lv):
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, lv, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="æ°´å¹³ãƒ–ãƒ¬ã‚¤ã‚¯ä¸‹", level_or_val=float(lv),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))

        elif signal_mode == "ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
            if tr_past:
                tl = tr_past["y1"]
                if (c > tl + break_buffer_arg) and (l1 <= tl):
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, tl, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="TLãƒ–ãƒ¬ã‚¤ã‚¯ä¸Š", level_or_val=float(tl),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if (c < tl - break_buffer_arg) and (h1 >= tl):
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, tl, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="TLãƒ–ãƒ¬ã‚¤ã‚¯ä¸‹", level_or_val=float(tl),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))

        elif signal_mode == "ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘/ä¸‹æŠœã‘(çµ‚å€¤)":
            if tr_past and tr_past["sigma"] > 0:
                up = tr_past["y1"] + chan_k * tr_past["sigma"]
                dn = tr_past["y1"] - chan_k * tr_past["sigma"]
                if c > up + break_buffer_arg:
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, up, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘", level_or_val=float(up),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if c < dn - break_buffer_arg:
                    entry = c; exitp = float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, dn, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="ãƒãƒ£ãƒãƒ«ä¸‹æŠœã‘", level_or_val=float(dn),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv_local - spread_pips,
                                     retest_index=ri, retest_hit=rh))

        elif signal_mode == "ãƒªãƒ†ã‚¹ãƒˆæŒ‡å€¤(æ°´å¹³ç·š)":
            K = int(retest_wait_k_arg)
            for lv in lvls_past:
                up_break = (c > lv + break_buffer_arg) and (l1 <= lv)
                dn_break = (c < lv - break_buffer_arg) and (h1 >= lv)

                # ä¸Šæ–¹å‘ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã€K æœ¬ä»¥å†…ã«ãƒªãƒ†ã‚¹ãƒˆâ†’ãã®â€œãƒªãƒ†ã‚¹ãƒˆæ™‚åˆ»â€ã§ç´„å®š
                if up_break:
                    for j in range(i+1, min(i+K, len(df)-fwd_n)):
                        t_j = df.index[j]
                        # ãƒªãƒ†ã‚¹ãƒˆæ™‚åˆ»ã§ã‚‚ãƒ‹ãƒ¥ãƒ¼ã‚¹æŠ‘åˆ¶ï¼ˆéå»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ï¼‰
                        if apply_news:
                            win_df_j = _build_windows_until(t_j, news_imp_min)
                            if _is_suppressed_at(t_j, win_df_j, news_win, news_imp_min, signal_mode):
                                continue
                        if abs(float(df["close"].iloc[j]) - lv) <= touch_buffer:
                            entry = float(df["close"].iloc[j]); exitp = float(df["close"].iloc[j+fwd_n])
                            ri, rh = compute_retest(close_s, lv, i, K, float(touch_buffer))
                            rows.append(dict(time=t_j, mode="ãƒªãƒ†ã‚¹ãƒˆ(L)", level_or_val=float(lv),
                                             dir="long", entry=entry, exit=exitp,
                                             ret_pips=(exitp-entry)/pv_local - spread_pips,
                                             retest_index=ri, retest_hit=rh))
                            break

                # ä¸‹æ–¹å‘ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã®ãƒªãƒ†ã‚¹ãƒˆ
                if dn_break:
                    for j in range(i+1, min(i+K, len(df)-fwd_n)):
                        t_j = df.index[j]
                        if apply_news:
                            win_df_j = _build_windows_until(t_j, news_imp_min)
                            if _is_suppressed_at(t_j, win_df_j, news_win, news_imp_min, signal_mode):
                                continue
                        if abs(float(df["close"].iloc[j]) - lv) <= touch_buffer:
                            entry = float(df["close"].iloc[j]); exitp = float(df["close"].iloc[j+fwd_n])
                            ri, rh = compute_retest(close_s, lv, i, K, float(touch_buffer))
                            rows.append(dict(time=t_j, mode="ãƒªãƒ†ã‚¹ãƒˆ(S)", level_or_val=float(lv),
                                             dir="short", entry=entry, exit=exitp,
                                             ret_pips=(entry-exitp)/pv_local - spread_pips,
                                             retest_index=ri, retest_hit=rh))
                            break

    return pd.DataFrame(rows)
# -*- coding: utf-8 -*-
# Streamlit FX Auto Lines - å®Œå…¨ç‰ˆ + News Shading + Flag/Pennant + H&S + Ghost Projection
# é»’èƒŒæ™¯ãƒ»é‡è¦åº¦åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦èµ¤å½±ãƒ»ã‚½ãƒ•ãƒˆæŠ‘åˆ¶ãƒ»è‡ªå‹•ãƒ©ã‚¤ãƒ³/ãƒ‘ã‚¿ãƒ¼ãƒ³/EV/ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ãƒ»æ‰‹å‹•å†å­¦ç¿’

import os, math, json, subprocess, sys, pathlib, re, warnings
from datetime import timedelta
import streamlit as st
import pandas as pd
import numpy as np
import yfinance as yf

def load_yf(symbol="JPY=X", period="60d", interval="15m"):
    df = yf.download(symbol, period=period, interval=interval, auto_adjust=False, progress=False)
    df = df.rename(columns={"Open":"open","High":"high","Low":"low","Close":"close","Volume":"volume"})
    df = df.reset_index().rename(columns={"Datetime":"timestamp"})  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœã®indexâ†’åˆ—ã¸
    return df

raw_df = load_yf("JPY=X", "60d", "15m")
import plotly.graph_objects as go
from sklearn.cluster import DBSCAN
from dataclasses import dataclass
import pytz, joblib
from dotenv import load_dotenv
from openai import OpenAI

warnings.filterwarnings("ignore")

# --- æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºé–¢æ•° ---
def show_calibration_report():
    st.header("ğŸ”§ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆï¼šCalibration")
    path = "reports/break_calibration.json"
    png  = "reports/break_calibration.png"
    if not os.path.exists(path):
        st.info("ãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå¾Œã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚")
        return
    with open(path, "r", encoding="utf-8") as f:
        payload = json.load(f)
    cal = payload["calibration"] if "calibration" in payload else payload
    meta= payload.get("meta", {})
    st.write(f"**Brier**: `{cal['brier']:.6f}`  /  **ECE**: `{cal['ece']:.6f}`")
    st.write("**Meta**:", meta)
    # è¡¨
    df = pd.DataFrame({
        "bin_left": cal["bin_edges"][:-1],
        "bin_right": cal["bin_edges"][1:],
        "prob_mean": cal["prob_mean"],
        "frac_pos": cal["frac_pos"],
        "count": cal["counts"],
        # NEW: CI åˆ—ï¼ˆå¾Œæ–¹äº’æ›ã®ãŸã‚ get ã§å–å¾—ï¼‰
        "frac_lo": cal.get("frac_lo", [None]* (len(cal["bin_edges"])-1)),
        "frac_hi": cal.get("frac_hi", [None]* (len(cal["bin_edges"])-1)),
    }).astype({"count": int}, errors="ignore")
    st.dataframe(df, use_container_width=True)
    # ç”»åƒ
    if os.path.exists(png):
        st.image(png, caption="Reliability Curve", use_container_width=True)


JST = pytz.timezone("Asia/Tokyo")

# --- ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–æ§‹æˆ ---
tabs = st.tabs(["ãƒˆãƒ¬ãƒ¼ãƒ‰", "æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ"])
with tabs[0]:
    # ...æ—¢å­˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‰UIã‚³ãƒ¼ãƒ‰...
    pass  # æ—¢å­˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‰UIã¯ã“ã“ã«å±•é–‹ã•ã‚Œã¦ã„ã‚‹ã¯ãš
with tabs[1]:
    show_calibration_report()

# ---------------- ãƒ€ãƒ¼ã‚¯é…è‰² ----------------
COLOR_BG = "#0b0f14"
COLOR_GRID = "#263238"
COLOR_TEXT = "#e0f2f1"
COLOR_LEVEL = "#00e5ff"            # æ°´å¹³ç·š
COLOR_TREND = "#ff9800"            # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆã‚ªãƒ¬ãƒ³ã‚¸ï¼‰
COLOR_CH_UP = "#e53935"            # ãƒãƒ£ãƒãƒ«ä¸Šï¼ˆèµ¤ï¼‰
COLOR_CH_DN = "#1e88e5"            # ãƒãƒ£ãƒãƒ«ä¸‹ï¼ˆé’ï¼‰
# --- ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨è‰² ---
COLOR_TRIANGLE = "#8e24aa"          # ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ã‚°ãƒ«ï¼ˆç´«ï¼‰
COLOR_RECTANGLE = "#43a047"         # ãƒ¬ã‚¯ã‚¿ãƒ³ã‚°ãƒ«ï¼ˆç·‘ï¼‰
COLOR_DOUBLE_TOP = "#d81b60"        # ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ï¼ˆãƒ”ãƒ³ã‚¯ï¼‰
COLOR_DOUBLE_BOTTOM = "#1976d2"     # ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ ï¼ˆé’ï¼‰
COLOR_FLAG = "#fbc02d"              # ãƒ•ãƒ©ãƒƒã‚°/ãƒšãƒŠãƒ³ãƒˆï¼ˆé»„ï¼‰
COLOR_HS = "#6d4c41"                # ãƒ˜ãƒƒãƒ‰ï¼†ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ã‚ºï¼ˆèŒ¶ï¼‰
COLOR_CANDLE_UP_BODY = "#26a69a"
COLOR_CANDLE_UP_EDGE = "#66fff9"
COLOR_CANDLE_DN_BODY = "#ef5350"
COLOR_CANDLE_DN_EDGE = "#ff8a80"

# ---------------- Intradayåˆ¶ç´„ã‚¯ãƒ©ãƒ³ãƒ— ----------------
INTRADAY_SET = {"1m","2m","5m","15m","30m","60m","90m"}
MAX_PERIOD_BY_INTERVAL = {
    "1m":"7d","2m":"60d","5m":"60d","15m":"60d","30m":"60d",
    "60m":"730d","90m":"730d"
}
def clamp_period_for_interval(period: str, interval: str) -> str:
    if interval not in INTRADAY_SET: return period
    maxp = MAX_PERIOD_BY_INTERVAL.get(interval)
    if not maxp: return period
    def to_days(p: str) -> int:
        p = p.strip().lower()
        if p.endswith("d"):  return int(p[:-1])
        if p.endswith("mo"): return int(p[:-2]) * 30
        if p.endswith("y"):  return int(p[:-1]) * 365
        return 999999
    return maxp if to_days(period) > to_days(maxp) else period

# ======== APIã‚­ãƒ¼èª­è¾¼ï¼ˆ.env / secrets ä¸¡å¯¾å¿œï¼‰========
@st.cache_resource(show_spinner=False)
def _load_openai_client():
    api_key = None

    # 1) Streamlit Secrets
    try:
        if "OPENAI_API_KEY" in st.secrets:
            api_key = st.secrets["OPENAI_API_KEY"]
    except Exception:
        pass

    # 2) .env / ç’°å¢ƒå¤‰æ•°
    if not api_key:
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        st.error("OpenAI API Key ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`secrets.toml` ã‹ `.env` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ãƒ¢ãƒ‡ãƒ«åã¯ secrets / env ã§ä¸Šæ›¸ãå¯èƒ½ã€‚æœªè¨­å®šãªã‚‰å®‰å…¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€‚
    model = None
    try:
        model = st.secrets.get("OPENAI_MODEL", None)
    except Exception:
        pass
    if not model:
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

    client = OpenAI(api_key=api_key)
    return client, model

client, DEFAULT_OAI_MODEL = _load_openai_client()

# ======== çŠ¶æ…‹åé›†ï¼ˆã‚ãªãŸã®æ—¢å­˜å¤‰æ•°åã«åˆã‚ã›ã¦é©å®œä¿®æ­£ï¼‰========
def collect_state_for_ai():
    return {
        "ticker": st.session_state.get("ticker", "JPY=X"),
        "period": st.session_state.get("period", "60d"),
        "interval": st.session_state.get("interval", "15m"),
        "H": st.session_state.get("H_pred", 12),
        "theta": st.session_state.get("theta", 0.60),
        "break_buffer": st.session_state.get("break_buf", 0.05),
        "K_retest": st.session_state.get("K", 10),
        "news": {
            "importance_threshold": st.session_state.get("imp_th", 3),
            "suppress_window_min": st.session_state.get("suppress_win", 30),
            "upcoming": st.session_state.get("upcoming_events", []),
        },
        "topk_levels": st.session_state.get("topk_levels", []),
    }

# ======== è¿”ç­”æŠ½å‡ºã®å®‰å…¨ç‰ˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ========
def _extract_text_from_responses(resp) -> str:
    """OpenAI Responses API / Chat Completions ãªã©ã®è¿”å´ã‚’å®‰å…¨ã«æ–‡å­—åˆ—åŒ–"""
    if resp is None:
        return ""

    # æ–°SDKï¼ˆResponses APIï¼‰: ã¾ãš output_text ã‚’å„ªå…ˆ
    txt = getattr(resp, "output_text", None)
    if isinstance(txt, str) and txt.strip():
        return txt.strip()

    # æ–°SDKã§ output ã®ä¸­ã« content ãŒåˆ†å‰²ã•ã‚Œã‚‹å½¢
    out = getattr(resp, "output", None)
    if out:
        chunks = []
        for item in out:
            content = getattr(item, "content", None) or []
            for part in content:
                # dict ã§ã‚‚ pydantic obj ã§ã‚‚æ‹¾ã†
                if isinstance(part, dict):
                    if part.get("type") in ("output_text", "text") and "text" in part:
                        val = part["text"]
                        if isinstance(val, str):
                            chunks.append(val)
                else:
                    ptype = getattr(part, "type", None)
                    if ptype in ("output_text", "text"):
                        t = getattr(part, "text", None)
                        if isinstance(t, str):
                            chunks.append(t)
                        else:
                            # ä¸€éƒ¨ã§ text.value ã«å…¥ã‚‹ã‚±ãƒ¼ã‚¹
                            v = getattr(t, "value", None)
                            if isinstance(v, str):
                                chunks.append(v)
        if chunks:
            return "\n".join(c for c in chunks if c).strip()

    # Chat Completions äº’æ›
    try:
        v = resp.choices[0].message.content
        if isinstance(v, str) and v.strip():
            return v.strip()
    except Exception:
        pass

    # Completions äº’æ›
    try:
        v = resp.choices[0].text
        if isinstance(v, str) and v.strip():
            return v.strip()
    except Exception:
        pass

    # dict åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ä¿é™º
    try:
        d = resp if isinstance(resp, dict) else resp.model_dump()
        for k in ("output_text", "output", "text", "content"):
            v = d.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip()
    except Exception:
        pass

    return ""

def _fallback_answer(user_q: str) -> str:
    return (
        "ï¼ˆè‡ªå‹•å¿œç­”ï¼‰ã†ã¾ãå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å°‘ã—å…·ä½“åŒ–ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚\n"
        "ãƒ»è³ªå•ã‚’çŸ­ãè¦ç‚¹ã”ã¨ã«åˆ†ã‘ã‚‹\n"
        "ãƒ»ãƒãƒƒãƒˆ/APIã‚­ãƒ¼/ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®çŠ¶æ…‹ã‚’ç¢ºèª\n"
    )

# ======== ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå‘¼ã³å‡ºã—ï¼ˆä¾‹å¤–ã‚’UIã«å‡ºã™ï¼‰========
def ask_copilot(app_state: dict, user_question: str) -> str:
    system = (
        "You are an FX breakout trading copilot specialized in USDJPY 15m.\n"
        "Use the JSON app state to give short, practical guidance.\n"
        "Output format:\n"
        "- One-line conclusion\n"
        "- 3 bullet reasons (concise)\n"
        "- Next action (one line)\n"
        "- Disclaimer (one line, no guarantees)\n"
        "Never guarantee profits. Be cautious around news windows."
    )

    # --- å‘¼ã³å‡ºã— & æŠ½å‡º ---
    raw = None
    ans = ""
    error_msg = None
    model_used = st.secrets.get("OPENAI_MODEL", os.getenv("OPENAI_MODEL", DEFAULT_OAI_MODEL))

    try:
        raw = client.responses.create(
            model=model_used,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": f"APP_STATE_JSON:\n{json.dumps(app_state, ensure_ascii=False)}"},
                {"role": "user", "content": f"QUESTION:\n{user_question or '(ç©º)'}"},
            ],
            max_output_tokens=800,
        )
        ans = _extract_text_from_responses(raw).strip()
        if not ans:
            raise RuntimeError("AIå¿œç­”ãŒç©ºã§ã—ãŸï¼ˆoutput_text / output / choices ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ä¸å¯ï¼‰ã€‚")
    except Exception as e:
        error_msg = f"{type(e).__name__}: {e}"
        ans = _fallback_answer(user_question)

    # --- ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆexpanderã‚’ä½¿ã‚ãš container ã§ï¼‰---
    debug_dict = {
        "model": model_used,
        "question": user_question,
        "app_state_keys": list(app_state.keys()) if isinstance(app_state, dict) else None,
        "extracted_len": len(ans) if isinstance(ans, str) else None,
        "had_error": error_msg is not None,
        "error": error_msg,
    }
    with st.container():
        st.markdown("**ãƒ‡ãƒãƒƒã‚°æƒ…å ±**")
        st.write(debug_dict)

    return ans

# ======== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šå…¥åŠ›ã ã‘ã€‚è¡¨ç¤ºã¯ãƒ¡ã‚¤ãƒ³ã«å›ã™ ========
with st.sidebar.expander("ğŸ¤– ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆï¼ˆgpt-5-miniï¼‰", expanded=False):
    user_q = st.text_area("ç›¸è«‡å†…å®¹", height=90, placeholder="ä¾‹ï¼‰ã“ã®è¨­å®šã§NYæ™‚é–“ã¯ Î¸ ã‚’ä¸Šã’ã‚‹ã¹ãï¼Ÿ")
    if st.button("AIã«ç›¸è«‡"):
        if not (user_q or "").strip():
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("AIãŒåˆ†æä¸­..."):
                app_state = collect_state_for_ai()
                ans = ask_copilot(app_state, user_q)
                st.session_state["copilot_answer"] = ans  # â† çŠ¶æ…‹ã«ä¿å­˜
    st.markdown("---")
    st.subheader("ãƒˆãƒ¬ãƒ¼ãƒ‰æç”»ãƒ„ãƒ¼ãƒ«")
    if "trade_points" not in st.session_state:
        st.session_state["trade_points"] = []
    # ãƒœã‚¿ãƒ³ç¾¤
    if st.button("è²·ã„ãƒã‚¤ãƒ³ãƒˆæç”»"):
        # æœ€æ–°è¶³ã®closeã‚’åŸºæº–
        price = float(df["close"].iloc[-1])
        atr_val = float(atr(df).iloc[-1])
        tp = price + atr_val * 1.5
        sl = price - atr_val * 1.0
        st.session_state["trade_points"].append({"type": "buy", "price": price, "tp": tp, "sl": sl, "time": df.index[-1]})
    if st.button("å£²ã‚Šãƒã‚¤ãƒ³ãƒˆæç”»"):
        price = float(df["close"].iloc[-1])
        atr_val = float(atr(df).iloc[-1])
        tp = price - atr_val * 1.5
        sl = price + atr_val * 1.0
        st.session_state["trade_points"].append({"type": "sell", "price": price, "tp": tp, "sl": sl, "time": df.index[-1]})
    if st.button("åˆ©ç¢ºãƒ©ã‚¤ãƒ³æç”»"):
        if st.session_state["trade_points"]:
            last = st.session_state["trade_points"][-1]
            st.session_state["trade_points"].append({"type": "tp", "price": last["tp"], "time": last["time"]})
    if st.button("æåˆ‡ã‚Šãƒ©ã‚¤ãƒ³æç”»"):
        if st.session_state["trade_points"]:
            last = st.session_state["trade_points"][-1]
            st.session_state["trade_points"].append({"type": "sl", "price": last["sl"], "time": last["time"]})

# ======== ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ã§â€œç›´è¿‘ã®å›ç­”â€ã‚’è¡¨ç¤º ========
if st.session_state.get("copilot_answer"):
    st.subheader("ğŸ¤– ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã®å›ç­”")
    st.write(st.session_state["copilot_answer"])

# ======== å…è²¬ã¯å¾“æ¥ã©ãŠã‚Šï¼ˆå¸¸æ™‚è¡¨ç¤ºï¼‰ ========
st.markdown("""
---
**å…è²¬äº‹é …**ï¼šæœ¬ã‚³ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã®ææ¡ˆã¯æ•™è‚²ãƒ»å‚è€ƒç›®çš„ã§ã™ã€‚å°†æ¥ã®åˆ©ç›Šã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚  
æœ€çµ‚çš„ãªæŠ•è³‡åˆ¤æ–­ã¯ã”è‡ªèº«ã®è²¬ä»»ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
""")

# ---------------- Utils ----------------
def ensure_jst_index(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty: return df
    idx = df.index
    if getattr(idx, "tz", None) is None:
        idx = idx.tz_localize("UTC")
    return df.copy().set_index(idx.tz_convert(JST))

def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = pd.concat([(high-low), (high-prev_close).abs(), (low-prev_close).abs()], axis=1).max(axis=1)
    return tr.rolling(period).mean()

def in_sessions(ts: pd.Timestamp) -> str:
    h = ts.hour
    if 9 <= h < 15: return "Tokyo"
    if 16<= h < 24: return "London"
    if h>=22 or h<5: return "NY"
    return "Other"

def pip_value(pair="USDJPY"):
    return 0.01  # USDJPYæƒ³å®š

def _select_first(values) -> str:
    return list(dict.fromkeys(map(str, values)))[0]

def normalize_ohlcv(df: pd.DataFrame, symbol: str | None) -> pd.DataFrame:
    """yfinanceã®å˜å±¤/å¤šå±¤åˆ—ã‚’ open/high/low/close ã«çµ±ä¸€"""
    if not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame()

    if not isinstance(df.columns, pd.MultiIndex):
        df = df.rename(columns=lambda c: str(c).strip()).rename(columns=str.lower)
        need = {"open","high","low","close"}
        if not need.issubset(set(df.columns)):
            colmap = {}
            for want in ["Open","High","Low","Close","Adj Close","Volume"]:
                cand = [c for c in df.columns if c.lower().startswith(want.lower())]
                if cand: colmap[cand[0]] = want.lower()
            df = df.rename(columns=colmap)
            if not need.issubset(set(df.columns)):
                raise ValueError(f"å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {need - set(df.columns)}")
        return df

    # MultiIndexå¯¾å¿œ
    cols = df.columns
    lvl0, lvl1 = set(map(str, cols.get_level_values(0))), set(map(str, cols.get_level_values(1)))
    fields = {"Open","High","Low","Close","Adj Close","Volume"}
    if fields & lvl0 and not (fields & lvl1):
        df = df.swaplevel(0,1,axis=1)
        cols = df.columns

    if fields & set(map(str, cols.get_level_values(1))):
        tickers = list(dict.fromkeys(map(str, cols.get_level_values(0))))
        pick_ticker = symbol if (symbol and symbol in tickers) else _select_first(cols.get_level_values(0))
        sub = df.xs(pick_ticker, axis=1, level=0)
        if isinstance(sub.columns, pd.MultiIndex):
            try: sub = sub.droplevel(0, axis=1)
            except Exception: sub.columns = ["_".join(map(str,c)) for c in sub.columns]
        want = [c for c in ["Open","High","Low","Close","Adj Close","Volume"] if c in sub.columns]
        if not {"Open","High","Low","Close"}.issubset(set(want)):
            def _find(name):
                for c in sub.columns:
                    if str(c).lower()==name.lower(): return c
                for c in sub.columns:
                    if name.lower() in str(c).lower(): return c
                return None
            pick = {k:_find(k) for k in ["Open","High","Low","Close","Adj Close","Volume"]}
            if not all(pick[k] for k in ["Open","High","Low","Close"]):
                raise ValueError("OHLCVåˆ—ã®æŠ½å‡ºã«å¤±æ•—ï¼ˆç•°ä¾‹ã®MultiIndexï¼‰ã€‚")
            want = [pick[k] for k in ["Open","High","Low","Close"] if pick[k] is not None]
            if pick.get("Adj Close"): want.append(pick["Adj Close"])
            if pick.get("Volume"):    want.append(pick["Volume"])
        out = sub[want].copy()
        out.columns = [str(c).lower() for c in out.columns]
        return out

    df_flat = df.copy()
    df_flat.columns = ["_".join(map(str, c)) for c in df_flat.columns]
    mapping = {}
    for fld in ["Open","High","Low","Close","Adj Close","Volume"]:
        cand = [c for c in df_flat.columns if c.split("_")[-1].lower()==fld.lower() or fld.lower() in c.lower()]
        if cand: mapping[fld]=cand[0]
    if not {"Open","High","Low","Close"}.issubset(mapping.keys()):
        raise ValueError("OHLCVåˆ—ã®æŠ½å‡ºã«å¤±æ•—ï¼ˆæœªçŸ¥ã®åˆ—æ§‹æˆï¼‰ã€‚")
    cols_ordered = [mapping[k] for k in ["Open","High","Low","Close"] if k in mapping]
    if "Adj Close" in mapping: cols_ordered.append(mapping["Adj Close"])
    if "Volume" in mapping:    cols_ordered.append(mapping["Volume"])
    out = df_flat[cols_ordered].copy()
    out.columns = [c.split("_")[-1].lower() for c in out.columns]
    return out

# ---------------- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------------
st.sidebar.title("è¨­å®š")
symbol = st.sidebar.text_input("ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆUSDJPYï¼‰", value="JPY=X", help="yfinanceã§USDJPYã¯ 'JPY=X'")
period_raw = st.sidebar.selectbox("å–å¾—æœŸé–“", ["7d","14d","30d","60d","90d","180d","1y"], index=2)
interval = st.sidebar.selectbox("è¶³ç¨®", ["5m","15m","30m","60m","1d"], index=1)

# === æ¨å¥¨è¡Œå‹•ï¼ˆæ„æ€æ±ºå®šãƒãƒªã‚·ãƒ¼ï¼‰ã‚µã‚¤ãƒ‰ãƒãƒ¼ ===
with st.sidebar.expander("ğŸ§­ æ¨å¥¨è¡Œå‹•ï¼ˆæ„æ€æ±ºå®šãƒãƒªã‚·ãƒ¼ï¼‰", expanded=False):
    news_mode = st.selectbox("ãƒ‹ãƒ¥ãƒ¼ã‚¹æ™‚ã®åŸºæœ¬å‹•ä½œ", ["hard","soft"], index=0,
                             help="hard=è¦‹é€ã‚Šå›ºå®š / soft=Î¸ã‚’ä¸Šã’ã‚‹")
    min_ev_r = st.number_input("EV/Rã®ä¸‹é™", value=0.00, step=0.01, format="%.2f")
    spread_max = st.number_input("è¨±å®¹ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆåŒå˜ä½ï¼‰", value=0.03, step=0.01)
    wick_ratio_max = st.number_input("é•·ãƒ’ã‚²é–¾å€¤ï¼ˆãƒ’ã‚²/å®Ÿä½“ï¼‰", value=2.5, step=0.5)
    prefer_limit = st.toggle("ãƒªãƒ†ã‚¹ãƒˆæŒ‡å€¤ã‚’å„ªå…ˆï¼ˆâ€œå…¥ã‚‰ãªã„å‹‡æ°—â€ï¼‰", value=True)
    bump_low  = st.number_input("Î¸è£œæ­£: lowãƒœãƒ© +", value=0.00, step=0.01, format="%.2f")
    bump_mid  = st.number_input("Î¸è£œæ­£: midãƒœãƒ© +", value=0.02, step=0.01, format="%.2f")
    bump_high = st.number_input("Î¸è£œæ­£: highãƒœãƒ© +", value=0.03, step=0.01, format="%.2f")

params = DecisionParams(
    min_ev_r=min_ev_r,
    theta_bump_by_regime={"low":bump_low,"mid":bump_mid,"high":bump_high},
    theta_bump_in_news=0.03,
    news_mode=news_mode,
    spread_max=spread_max,
    wick_ratio_max=wick_ratio_max,
    prefer_limit_retest=prefer_limit,
)


# --- OANDA APIè¨­å®š ---
st.sidebar.markdown("---")
with st.sidebar.expander("OANDA APIè¨­å®š", expanded=False):
    oanda_token = st.text_input("OANDA APIãƒˆãƒ¼ã‚¯ãƒ³", type="password", help="OANDAã®APIãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›")
    oanda_account = st.text_input("OANDAã‚¢ã‚«ã‚¦ãƒ³ãƒˆç•ªå·", help="OANDAã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç•ªå·ã‚’å…¥åŠ›")
    oanda_env = st.selectbox("OANDAç’°å¢ƒ", ["practice", "live"], index=0, help="practice=ãƒ‡ãƒ¢, live=æœ¬ç•ª")

st.sidebar.markdown("---")
st.sidebar.subheader("å–å¼•ã‚³ã‚¹ãƒˆè¨­å®šï¼ˆpipsï¼‰")
fee_commission = st.sidebar.number_input("æ‰‹æ•°æ–™ï¼ˆå¾€å¾©ï¼‰", min_value=0.0, max_value=5.0, value=0.00, step=0.01)
fee_slippage  = st.sidebar.number_input("ã‚¹ãƒªãƒƒãƒšãƒ¼ã‚¸ï¼ˆå¹³å‡ï¼‰", min_value=0.0, max_value=5.0, value=0.20, step=0.01)
fee_gap       = st.sidebar.number_input("ã‚®ãƒ£ãƒƒãƒ—æ§é™¤ï¼ˆæœŸå¾…å€¤ï¼‰", min_value=0.0, max_value=10.0, value=0.00, step=0.01)
extra_cost_pips = float(fee_commission + fee_slippage + fee_gap)

st.sidebar.subheader("è‡ªå‹•æ›´æ–°")
auto_refresh = st.sidebar.checkbox("è‡ªå‹•ã§å†å–å¾—ï¼ˆãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿ï¼‰", value=True)
refresh_secs = st.sidebar.slider("æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰", 30, 600, 600, help="15åˆ†è¶³ã¯60ã€œ180ç§’ãŒç›®å®‰")
try:
    from streamlit_autorefresh import st_autorefresh
    if auto_refresh:
        st_autorefresh(interval=refresh_secs * 1000, limit=None, key="fx_autorefresh")
except Exception:
    if auto_refresh:
        from streamlit.components.v1 import html
        html(f"""<script>setTimeout(function(){{window.location.reload();}}, {int(refresh_secs*1000)});</script>""", height=0)

st.sidebar.markdown("---")
st.sidebar.subheader("ã‚·ã‚°ãƒŠãƒ«æ¡ä»¶")
signal_mode = st.sidebar.selectbox(
    "ç¨®åˆ¥ã‚’é¸æŠ",
    ["æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)", "ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)", "ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘/ä¸‹æŠœã‘(çµ‚å€¤)", "ãƒªãƒ†ã‚¹ãƒˆæŒ‡å€¤(æ°´å¹³ç·š)"],
    index=0
)
retest_wait_k_base = st.sidebar.slider("ãƒªãƒ†ã‚¹ãƒˆå¾…æ©Ÿæœ¬æ•°K", 3, 30, 10)
st.sidebar.caption("ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã€Kæœ¬ä»¥å†…ã«ãƒ©ã‚¤ãƒ³/ãƒãƒ³ãƒ‰ã¸æˆ»ã£ãŸã‹ã§ã€ãƒªãƒ†ã‚¹ãƒˆã‚ã‚Š/ãªã—ã€ã‚’åˆ¤å®šï¼ˆæŒ‡æ•°0ã€œ1ã‚‚ç®—å‡ºï¼‰")

st.sidebar.markdown("---")
st.sidebar.subheader("æ¥µå€¤æ¤œå‡ºï¼ˆã‚¹ã‚¤ãƒ³ã‚°ï¼‰")
look = st.sidebar.slider("å·¦å³ã®çª“å¹…", 3, 15, 5)

st.sidebar.markdown("---")
st.sidebar.subheader("æ°´å¹³ã‚µãƒãƒ¬ã‚¸ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰")
eps = st.sidebar.number_input("DBSCAN epsï¼ˆä¾¡æ ¼ï¼‰", value=0.08, step=0.01)
min_samples = st.sidebar.slider("min_samples", 3, 12, 4)

st.sidebar.markdown("---")
st.sidebar.subheader("ãƒˆãƒ¬ãƒ³ãƒ‰ï¼†ãƒãƒ£ãƒãƒ«")
reg_lookback = st.sidebar.slider("å›å¸°ã«ä½¿ã†ç›´è¿‘æœ¬æ•°", 30, 300, 40)
chan_k = st.sidebar.slider("ãƒãƒ£ãƒãƒ«å¹…ï¼ˆÏƒã®å€ç‡ï¼‰", 0.5, 3.0, 2.0, 0.5)

st.sidebar.markdown("---")
st.sidebar.subheader("åˆ¤å®šãƒãƒƒãƒ•ã‚¡")
touch_buffer = st.sidebar.number_input("æ¥è§¦ãƒãƒƒãƒ•ã‚¡ï¼ˆä¾¡æ ¼ï¼‰", value=0.05, step=0.01)
break_buffer_base = st.sidebar.number_input("ãƒ–ãƒ¬ã‚¤ã‚¯ãƒãƒƒãƒ•ã‚¡ï¼ˆä¾¡æ ¼ï¼‰", value=0.05, step=0.01)

st.sidebar.markdown("---")
st.sidebar.subheader("é‡è¦åº¦ã‚¹ã‚³ã‚¢ã®é‡ã¿")
w_touch = st.sidebar.slider("æ¥è§¦å›æ•°", 0.0, 1.0, 0.30, 0.05)
w_recent = st.sidebar.slider("ç›´è¿‘è·é›¢ï¼ˆè¿‘ã„ã»ã©â†‘ï¼‰", 0.0, 1.0, 0.30, 0.05)
w_session = st.sidebar.slider("æ™‚é–“å¸¯ï¼ˆä¸»è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§â†‘ï¼‰", 0.0, 1.0, 0.20, 0.05)
w_vol = st.sidebar.slider("ãƒœãƒ©ï¼ˆATRï¼‰", 0.0, 1.0, 0.20, 0.05)
w_sum = max(1e-9, w_touch + w_recent + w_session + w_vol)
w_touch, w_recent, w_session, w_vol = [w/w_sum for w in (w_touch, w_recent, w_session, w_vol)]


# ---------- ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æŒ‡æ¨™ãƒ•ã‚£ãƒ«ã‚¿ & èµ¤å½± ----------
st.sidebar.markdown("---")
with st.sidebar.expander("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»æŒ‡æ¨™ãƒ•ã‚£ãƒ«ã‚¿ / èµ¤å½±", expanded=False):
    news_file = st.file_uploader("ãƒ‹ãƒ¥ãƒ¼ã‚¹CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰", type=["csv"])
    st.caption("å—ç†åˆ—: time/timestamp/datetime ã¾ãŸã¯ date+timeã€importance[, title]ï¼ˆJSTæ¨å¥¨ï¼‰")

    # ãƒ•ã‚£ãƒ«ã‚¿æ–¹å¼
    news_filter_mode = st.radio(
        "ãƒ•ã‚£ãƒ«ã‚¿æ–¹å¼",
        ["ä¸€å¾‹Â±åˆ†", "é‡è¦åº¦åˆ¥ï¼ˆèµ¤å½±ã¨åŒã˜ï¼‰"],
        index=1, horizontal=True
    )
    news_win = st.slider("ä¸€å¾‹Â±åˆ†ï¼ˆä¸Šã‚’é¸ã‚“ã ã¨ãã®ã¿ä½¿ç”¨ï¼‰", 0, 120, 30)
    news_imp_min = st.slider("é‡è¦åº¦ã—ãã„å€¤ (>=)", 1, 5, 3)

    # é‡è¦åº¦â†’Â±åˆ†ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆèµ¤å½±/é‡è¦åº¦åˆ¥ãƒ•ã‚£ãƒ«ã‚¿ã§ä½¿ç”¨ï¼‰
    st.caption("é‡è¦åº¦åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå·¦å³Â±åˆ†ï¼‰")
    map_5 = st.number_input("â˜…5 â†’ Â±åˆ†", value=90, step=5)
    map_4 = st.number_input("â˜…4 â†’ Â±åˆ†", value=30, step=5)
    map_3 = st.number_input("â˜…3 â†’ Â±åˆ†", value=20, step=5)
    map_2 = st.number_input("â˜…2 â†’ Â±åˆ†", value=0, step=5)
    map_1 = st.number_input("â˜…1 â†’ Â±åˆ†", value=0, step=5)
    use_news_shade = st.checkbox("ãƒãƒ£ãƒ¼ãƒˆã«èµ¤å½±ã‚’é‡ã­ã¦è¡¨ç¤º", value=True)

    # ãƒãƒ¼ãƒ‰/ã‚½ãƒ•ãƒˆæŠ‘åˆ¶
    apply_news_filter = st.checkbox("ãƒãƒ¼ãƒ‰æŠ‘åˆ¶ï¼ˆçª“å†…ã®ã‚·ã‚°ãƒŠãƒ«ç„¡åŠ¹åŒ–ï¼‰", value=True)
    use_soft_suppress = st.checkbox("ã‚½ãƒ•ãƒˆæŠ‘åˆ¶ï¼ˆçª“å†…ã ã‘åˆ¤å®šã‚’å³ã—ã‚ã«ï¼‰", value=True)
    soft_break_add = st.number_input("ã‚½ãƒ•ãƒˆ: ãƒ–ãƒ¬ã‚¤ã‚¯ãƒãƒƒãƒ•ã‚¡ è¿½åŠ ", value=0.02, step=0.01, format="%.2f")
    soft_K_add = st.slider("ã‚½ãƒ•ãƒˆ: K è¿½åŠ ", 0, 10, 4)

# ---------------- ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ----------------
st.sidebar.markdown("---")
st.sidebar.subheader("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
fwd_n = st.sidebar.slider("ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œ N æœ¬ï¼ˆæç›Šåˆ¤å®šï¼‰", 5, 120, 20)
spread_pips = st.sidebar.number_input("æƒ³å®šã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆpipsï¼‰", value=0.5, step=0.1)
run_bt = st.sidebar.button("â–¶ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")

# ---------------- ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ / EV ----------------
st.sidebar.markdown("---")
st.sidebar.subheader("ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ï¼ˆä»Šã‹ã‚‰ï¼‰")
show_break_prob = st.sidebar.checkbox("ä»Šã‹ã‚‰ã®æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ã‚’è¡¨ç¤º", value=True)
break_prob_topk = st.sidebar.slider("è¡¨ç¤ºã™ã‚‹ä¸Šä½ãƒ©ã‚¤ãƒ³æ•°", 1, 20, 10)
break_prob_h = st.sidebar.slider("å…ˆèª­ã¿Hï¼ˆå‚è€ƒè¡¨ç¤ºï¼‰", 3, 50, 12)
prob_model_path = st.sidebar.text_input("ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹", value="models/break_model.joblib")

st.sidebar.markdown("---")
st.sidebar.subheader("æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç¢ºç‡ Ã— æœŸå¾…pipsï¼‰")
show_ev_rank = st.sidebar.checkbox("æ°´å¹³ç·šã®æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡¨ç¤º", value=True)
ev_level_min_samples = st.sidebar.slider("ãƒ¬ãƒ™ãƒ«åˆ¥ã®æœ€ä½ã‚µãƒ³ãƒ—ãƒ«æ•°", 1, 20, 3)

# ---------- æ‰‹å‹•å†å­¦ç¿’ãƒœã‚¿ãƒ³ ----------
st.sidebar.markdown("---")
st.sidebar.subheader("ãƒ¢ãƒ‡ãƒ«ã®æ‰‹å‹•å†å­¦ç¿’")
proj_dir = str(pathlib.Path(__file__).resolve().parent)
train_script = "ai_train_break.py"
model_path = "models/break_model.joblib"
colA, colB = st.sidebar.columns([1,1])
with colA:
    retrain_now = st.button("å†å­¦ç¿’ã‚’å®Ÿè¡Œ", type="primary")
with colB:
    show_log = st.checkbox("ãƒ­ã‚°ã‚’è¡¨ç¤º", value=True)

def _format_ts(ts: float) -> str:
    return pd.Timestamp.fromtimestamp(ts, tz=JST).strftime("%Y-%m-%d %H:%M:%S %Z")

def run_retrain(script_path: str, workdir: str) -> tuple[bool, str]:
    py = sys.executable
    try:
        proc = subprocess.run(
            [py, script_path],
            cwd=workdir,
            capture_output=True,
            text=True,
            timeout=60*20
        )
        ok = proc.returncode == 0
        log = (proc.stdout or "") + "\n" + (proc.stderr or "")
        return ok, log
    except Exception as e:
        return False, f"Exception: {e}"

if retrain_now:
    with st.spinner("å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œä¸­..."):
        ok, log = run_retrain(train_script, proj_dir)
    model_file = pathlib.Path(proj_dir, model_path)
    if model_file.exists():
        ts = model_file.stat().st_mtime
        st.success(f"å­¦ç¿’å®Œäº†ï¼š{model_path} ã‚’æ›´æ–°ï¼ˆ{_format_ts(ts)}ï¼‰")
        try:
            st.cache_resource.clear()
        except Exception:
            pass
    else:
        st.error("å­¦ç¿’ã¯çµ‚äº†ã—ã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¿å­˜ãƒ‘ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    if show_log:
        st.subheader("å†å­¦ç¿’ãƒ­ã‚°")
        st.code((log or "").strip()[:200000], language="bash")
else:
    mf = pathlib.Path(proj_dir, model_path)
    if mf.exists():
        st.caption(f"ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æ›´æ–°: {_format_ts(mf.stat().st_mtime)}")
    else:
        st.caption("ãƒ¢ãƒ‡ãƒ«æœªä½œæˆï¼ˆå…ˆã«å†å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰")



# ---------------- ãƒ‡ãƒ¼ã‚¿å–å¾— ----------------
@st.cache_data(show_spinner=False, ttl=60)
def load_data(sym: str, period: str, interval: str,
              oanda_token: str = "", oanda_account: str = "", oanda_env: str = "practice") -> pd.DataFrame:
    """
    OANDAã®APIã‚­ãƒ¼/AccountãŒè¨­å®šã•ã‚Œã¦ã„ã‚Œã°OANDAã‹ã‚‰å–å¾—ã€
    ãã†ã§ãªã‘ã‚Œã°yfinanceã‚’ä½¿ç”¨ã€‚
    """
    use_oanda_feed = bool(oanda_token and oanda_account)
    if use_oanda_feed:
        try:
            gran_map = {
                "1m": "M1", "2m": "M2", "5m": "M5", "15m": "M15",
                "30m": "M30", "60m": "H1", "90m": "H1",  # OANDAã«90åˆ†è¶³ã¯ãªã„ã®ã§H1ã«fallback
                "1d": "D"
            }
            gran = gran_map.get(interval, "M15")

            inst = "USD_JPY"

            period_map = {"7d": 7*24*4, "14d": 14*24*4, "30d": 30*24*4,
                          "60d": 60*24*4, "90d": 90*24*4, "180d": 180*24*4, "1y": 365*24*4}
            count = min(period_map.get(period, 2000), 5000)

            base = "https://api-fxpractice.oanda.com" if oanda_env=="practice" else "https://api-fxtrade.oanda.com"
            url = f"{base}/v3/instruments/{inst}/candles"
            headers = {"Authorization": f"Bearer {oanda_token}"}
            params = {"count": count, "granularity": gran, "price": "M", "alignmentTimezone": "UTC"}

            r = requests.get(url, headers=headers, params=params, timeout=10)
            r.raise_for_status()
            candles = r.json().get("candles", [])

            rows = []
            for c in candles:
                if not c.get("complete", False):
                    continue
                t = pd.to_datetime(c["time"])
                rows.append(dict(
                    time=t,
                    open=float(c["mid"]["o"]),
                    high=float(c["mid"]["h"]),
                    low=float(c["mid"]["l"]),
                    close=float(c["mid"]["c"]),
                    volume=int(c.get("volume", 0))
                ))
            if not rows:
                return pd.DataFrame()
            df = pd.DataFrame(rows).set_index("time").sort_index()
            df.index = df.index.tz_convert(JST)
            return df

        except Exception as e:
            st.warning(f"OANDAãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e} â†’ yfinanceã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")

    # fallback: yfinance
    adj_period = clamp_period_for_interval(period, interval)
    try:
        df = yf.Ticker(sym).history(period=adj_period, interval=interval, auto_adjust=False)
    except Exception:
        df = pd.DataFrame()
    if df is None or df.empty:
        df = yf.download(sym, period=adj_period, interval=interval, auto_adjust=False, progress=False)
    if df is None or df.empty:
        return pd.DataFrame()
    return normalize_ohlcv(df, sym)


with st.spinner("ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—éƒ¨ï¼ˆfetch_prices/safe_callãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»å®Ÿç”¨é››å½¢ï¼‰
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®šå€¤ã‚’å–å¾—
    symbol = st.session_state.get("symbol", "JPY=X")
    period_raw = st.session_state.get("period_raw", "60d")
    interval = st.session_state.get("interval", "15m")
    oanda_token = st.session_state.get("oanda_token", "")
    oanda_account = st.session_state.get("oanda_account", "")
    oanda_env = st.session_state.get("oanda_env", "practice")

    def fetch_prices(symbol, period_raw, interval, oanda_token, oanda_account, oanda_env):
        return load_data(symbol, period_raw, interval, oanda_token, oanda_account, oanda_env)

    df, err = safe_call(fetch_prices, symbol, period_raw, interval, oanda_token, oanda_account, oanda_env)
    if err or df is None or df.empty:
        st.error(f"[price] å–å¾—å¤±æ•—: {err if err else 'ãƒ‡ãƒ¼ã‚¿ãªã—'}")
        st.stop()
    # --- ã“ã“ã§JSTçµ±ä¸€ ---
    df = ensure_jst_index(df)

# ---------------- æ¥µå€¤ & ãƒ¬ãƒ™ãƒ« ----------------
def swing_pivots(df: pd.DataFrame, look: int):
    highs = df["high"].rolling(look, center=True).max()
    lows  = df["low"].rolling(look, center=True).min()
    pivot_high = df[(df["high"] == highs)].dropna(subset=["high"])
    pivot_low  = df[(df["low"]  == lows )].dropna(subset=["low"])
    return pivot_high, pivot_low

def horizontal_levels(pivot_high: pd.DataFrame, pivot_low: pd.DataFrame, eps: float, min_samples: int):
    prices = np.r_[pivot_high["high"].values, pivot_low["low"].values].reshape(-1,1)
    if len(prices) == 0: return []
    # epsã‚’è‡ªå‹•èª¿æ•´: éå»ä¾¡æ ¼ã®æ¨™æº–åå·®ã®5%ç¨‹åº¦ã‚’åˆæœŸå€¤ã«
    auto_eps = float(np.std(prices)) * 0.05 if eps is None or eps <= 0 else eps
    auto_min_samples = max(3, min_samples)
    labels = DBSCAN(eps=auto_eps, min_samples=auto_min_samples).fit(prices).labels_
    levels = []
    for lab in set(labels) - {-1}:
        lv = prices[labels==lab].mean()
        levels.append(float(lv))
    # è¿‘ã™ãã‚‹æ°´æº–ã¯é–“å¼•ã
    levels = sorted(set([round(lv, 3) for lv in levels]))
    return levels

pivot_high, pivot_low = swing_pivots(df, look)
levels = horizontal_levels(pivot_high, pivot_low, eps=eps, min_samples=min_samples)

# ---------------- å›å¸°ãƒˆãƒ¬ãƒ³ãƒ‰ & ãƒãƒ£ãƒãƒ« ----------------
def regression_trend(df: pd.DataFrame, lookback: int, use="low"):
    sub = df.tail(lookback)
    y = sub[use].values
    x = np.arange(len(y))
    if len(x) < 2: return None
    m, b = np.polyfit(x, y, 1)
    t0, t1 = sub.index[0], sub.index[-1]
    y0, y1 = b, m*(len(x)-1) + b
    resid = y - (m*x + b)
    sigma = float(np.std(resid))
    return dict(x0=t0, y0=y0, x1=t1, y1=y1, slope=m, intercept=b, sigma=sigma, n=len(x))

trend = regression_trend(df, reg_lookback, use="low")

# ---------------- ãƒ¬ãƒ™ãƒ«é‡è¦åº¦ã‚¹ã‚³ã‚¢ ----------------
def compute_level_scores(df: pd.DataFrame, levels: list, touch_buffer: float,
                         w_touch: float, w_recent: float, w_session: float, w_vol: float) -> pd.DataFrame:
    if not levels:
        return pd.DataFrame(columns=["level","score","touches","session_ratio"])
    _atr = atr(df, 14)
    atr_recent = float(_atr.iloc[-1]) if not _atr.empty else 0.0
    rec_close = float(df["close"].iloc[-1])

    rows=[]
    for lv in levels:
        touch_mask = ((df["low"] <= lv) & (df["high"] >= lv)) | (df["close"].sub(lv).abs() <= touch_buffer)
        touches = int(touch_mask.sum())
        dist = abs(rec_close - lv)
        near = 1.0 / (dist + 1e-6)
        if touches > 0:
            ts_idx = df.index[touch_mask]
            sess_hits = sum(1 for ts in ts_idx if in_sessions(ts) in ("Tokyo","London","NY"))
            session_ratio = sess_hits / touches
        else:
            session_ratio = 0.0
        atr_norm = atr_recent / max(1e-6, rec_close)
        rows.append(dict(level=float(lv), touches=touches, near=near, session_ratio=session_ratio, atr_norm=atr_norm))
    df_sc = pd.DataFrame(rows)
    # æ­£è¦åŒ–ã‚’å¼·åŒ–: min-maxã ã‘ã§ãªãåˆ†æ•£ãŒå°ã•ã„å ´åˆã¯ãƒ©ãƒ³ã‚¯åŒ–
    for col in ["touches","near","session_ratio","atr_norm"]:
        colmin, colmax = df_sc[col].min(), df_sc[col].max()
        if math.isclose(colmin, colmax):
            # å·®ãŒå°ã•ã„å ´åˆã¯é †ä½ã§ã‚¹ã‚³ã‚¢åŒ–
            df_sc[col+"_n"] = df_sc[col].rank(method="average") / len(df_sc)
        else:
            df_sc[col+"_n"] = (df_sc[col]-colmin)/(colmax-colmin)
    df_sc["score"] = (
        w_touch*df_sc["touches_n"] + w_recent*df_sc["near_n"] +
        w_session*df_sc["session_ratio_n"] + w_vol*df_sc["atr_norm_n"]
    ) * 100.0
    return df_sc.sort_values("score", ascending=False)

score_df = compute_level_scores(df, levels, touch_buffer, w_touch, w_recent, w_session, w_vol)

# ---------------- ãƒ‹ãƒ¥ãƒ¼ã‚¹CSVï¼ˆå …ç‰¢ãƒ‘ãƒ¼ã‚µï¼‰ ----------------
def parse_news_csv(file) -> pd.DataFrame:
    if file is None:
        return pd.DataFrame(columns=["time","importance","title"])
    try:
        df = pd.read_csv(file, dtype=str)
    except Exception:
        file.seek(0); df = pd.read_csv(file, dtype=str, header=None)
    df = df.astype(str).fillna("")
    def norm(s: str) -> str:
        s = s.strip().replace("\u3000"," "); s = s.lower()
        return re.sub(r"[\s\-_/()]", "", s)
    TIME_CANDS = {"time","timestamp","datetime","date_time","timejst","datetimejst","æ—¥æ™‚","æ—¥ä»˜æ™‚åˆ»","ç™ºè¡¨æ™‚åˆ»","ç™ºè¡¨æ™‚é–“","æ™‚åˆ»","æ™‚é–“","date","when"}
    DATE_ONLY = {"date","æ—¥ä»˜","ç™ºè¡¨æ—¥"}
    CLOCK_ONLY= {"time","æ™‚åˆ»","ç™ºè¡¨æ™‚åˆ»","ç™ºè¡¨æ™‚é–“","æ™‚é–“"}
    IMP_CANDS  = {"importance","é‡è¦åº¦","impact","rank","priority","å„ªå…ˆåº¦","star","stars"}
    TITLE_CANDS= {"title","ã‚¤ãƒ™ãƒ³ãƒˆ","æŒ‡æ¨™å","headline","event","name","å†…å®¹","subject"}
    orig_cols = list(df.columns)
    norm_cols = [norm(c) for c in orig_cols]
    col_map = dict(zip(norm_cols, orig_cols))
    time_col=imp_col=title_col=None
    for nc in norm_cols:
        if nc in {norm(x) for x in TIME_CANDS} and nc not in {norm(x) for x in DATE_ONLY}:
            time_col = col_map[nc]; break
    if time_col is None:
        date_col = None; clock_col=None
        for nc in norm_cols:
            if nc in {norm(x) for x in DATE_ONLY}: date_col = col_map[nc]
            if nc in {norm(x) for x in CLOCK_ONLY}: clock_col = col_map[nc]
        if date_col and clock_col:
            df["_tmp_time"] = (df[date_col].astype(str).str.strip()+" "+df[clock_col].astype(str).str.strip()).str.strip()
            time_col = "_tmp_time"
    if time_col is None:
        best_col=None; best_ok=-1
        for c in df.columns:
            tryconv = pd.to_datetime(df[c], utc=True, errors="coerce", infer_datetime_format=True)
            ok = tryconv.notna().sum()
            if ok > best_ok and ok>0:
                best_ok = ok; best_col = c
        if best_col is not None: time_col = best_col
    for nc in norm_cols:
        if nc in {norm(x) for x in IMP_CANDS}:
            imp_col = col_map[nc]; break
    if imp_col is None:
        best_col=None; best_numeric=-1
        for c in df.columns:
            if c == time_col: continue
            s = pd.to_numeric(df[c], errors="coerce")
            numeric_ok = s.notna().sum()
            if numeric_ok > best_numeric and numeric_ok>0:
                best_numeric = numeric_ok; best_col=c
        if best_col is not None: imp_col = best_col
    for nc in norm_cols:
        if nc in {norm(x) for x in TITLE_CANDS}:
            title_col = col_map[nc]; break
    if title_col is None:
        cand = [c for c in df.columns if c not in {time_col, imp_col}]
        title_col = cand[0] if cand else None

    if time_col is None or imp_col is None:
        raise ValueError("ãƒ‹ãƒ¥ãƒ¼ã‚¹CSVã« 'time'ï¼ˆã¾ãŸã¯ date+timeï¼‰ã¨ 'importance' ãŒå¿…è¦ã§ã™ã€‚")

    def parse_dt_series(s: pd.Series) -> pd.Series:
        # ã¾ãšUTCã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        dt = pd.to_datetime(s, utc=True, errors="coerce", infer_datetime_format=True)
        bad = dt.isna()
        if bad.any():
            fmt_list = ["%Y-%m-%d %H:%M","%Y/%m/%d %H:%M","%Y-%m-%d %H:%M:%S","%Y/%m/%d %H:%M:%S"]
            raw = s[bad].astype(str).str.strip()
            fixed = pd.Series([pd.NaT]*len(raw), index=raw.index)
            for fmt in fmt_list:
                try:
                    parsed = pd.to_datetime(raw, format=fmt, utc=True, errors="coerce")
                    fixed = fixed.fillna(parsed)
                except Exception:
                    pass
            dt.loc[bad] = fixed
        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è‡ªå‹•åˆ¤å®šãƒ»æŸ”è»Ÿå¤‰æ›
        if getattr(dt.dt, 'tz', None) is None:
            # JSTã§è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆä¾‹: 0æ™‚ï½23æ™‚ã®ã¿ï¼‰
            hours = dt.dt.hour.dropna()
            if (hours.max() <= 23) and (hours.min() >= 0):
                # JSTã¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚º
                dt = dt.dt.tz_localize("Asia/Tokyo")
            else:
                # UTCã¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚ºâ†’JSTå¤‰æ›
                dt = dt.dt.tz_localize("UTC").dt.tz_convert("Asia/Tokyo")
        bad = dt.isna()
        if bad.any():
            raw = s[bad].astype(str).str.strip()
            def numparse(x):
                try:
                    v = float(x)
                    if v > 10_000_000_000: return pd.to_datetime(v, unit="ms", utc=True)
                    return pd.to_datetime(v, unit="s", utc=True)
                except Exception:
                    return pd.NaT
            dt.loc[bad] = raw.apply(numparse)
        return dt

    dt_jst = parse_dt_series(df[time_col])
    if dt_jst.notna().sum() == 0:
        return pd.DataFrame()  # æ—¥æ™‚è§£é‡ˆã§ããªã„å ´åˆã¯ç©ºDataFrameã‚’è¿”ã™
    imp = pd.to_numeric(df[imp_col], errors="coerce").fillna(0).astype(int)
    ttl = df[title_col] if (title_col in df.columns) else ""
    out = pd.DataFrame({"time": dt_jst, "importance": imp, "title": ttl}).dropna(subset=["time"])
    return out.sort_values("time").reset_index(drop=True)

news_df = parse_news_csv(news_file)

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰CSVãŒç©ºã¾ãŸã¯æ—¥æ™‚è§£é‡ˆä¸å¯ãªã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
if news_file is not None:
    if news_df is None or news_df.empty:
        st.info("æœ¬æ—¥ã®ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆã¾ãŸã¯CSVã®æ—¥æ™‚ã‚’è§£é‡ˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰")

# ---- é‡è¦åº¦åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç”Ÿæˆ & èµ¤å½±æç”»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----
def build_event_windows(events_df: pd.DataFrame, imp_threshold: int, mapping: dict[int,int]) -> pd.DataFrame:
    if events_df.empty:
        return pd.DataFrame(columns=["start","end","importance","title"])
    rows=[]
    for _, r in events_df.iterrows():
        imp = int(r["importance"])
        if imp < imp_threshold: 
            continue
        minutes = mapping.get(imp, 0)
        start = r["time"] - timedelta(minutes=minutes)
        end   = r["time"] + timedelta(minutes=minutes)
        rows.append({"start": start, "end": end, "importance": imp, "title": r.get("title", "")})
    windows = pd.DataFrame(rows)
    return windows.sort_values("start").reset_index(drop=True)

def is_suppressed(ts: pd.Timestamp, windows_df: pd.DataFrame) -> bool:
    if windows_df.empty: return False
    return bool(((windows_df["start"] <= ts) & (ts <= windows_df["end"])).any())

def add_news_shading_to_fig(fig: go.Figure, windows_df: pd.DataFrame) -> go.Figure:
    if windows_df.empty: return fig
    color_map = {5:"rgba(255,0,0,0.18)", 4:"rgba(255,0,0,0.12)", 3:"rgba(255,0,0,0.08)"}
    shapes = list(fig.layout.shapes) if fig.layout.shapes else []
    for _, r in windows_df.iterrows():
        col = color_map.get(int(r["importance"]), "rgba(255,0,0,0.08)")
        shapes.append(dict(type="rect", xref="x", x0=r["start"], x1=r["end"],
                           yref="paper", y0=0, y1=1, fillcolor=col, line=dict(width=0), layer="below"))

    fig.update_layout(
        dragmode="pan",                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ã€Œãƒ‘ãƒ³ã€ã«
        xaxis=dict(rangeslider=dict(visible=False)),  # ä¸‹ã®å°ã•ãªãƒ¬ãƒ³ã‚¸ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’æ¶ˆã™
        yaxis=dict(fixedrange=False),  # ç¸¦æ–¹å‘ã‚‚ã‚ºãƒ¼ãƒ å¯èƒ½ã«
    )
    return fig

# ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
imp_map = {5:int(map_5), 4:int(map_4), 3:int(map_3), 2:int(map_2), 1:int(map_1)}
windows_df = pd.DataFrame()
if news_df is not None and not news_df.empty:
    df_w, err = safe_call(build_event_windows, news_df, imp_threshold=news_imp_min, mapping=imp_map)
    if err is None and df_w is not None and {"start","end"}.issubset(df_w.columns):
        windows_df = df_w
    else:
        st.warning(f"[event windows] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™: {err or 'åˆ—ä¸è¶³'}")
else:
    st.info("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ãªã—ï¼ˆæŠ‘åˆ¶ã¯ç„¡åŠ¹ï¼‰")

# ---------------- ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆTriangle / Rectangle / Double / Flag / Pennant / H&Sï¼‰ ----------------
@dataclass
class Pattern:
    kind: str
    t_start: pd.Timestamp
    t_end: pd.Timestamp
    params: dict
    quality: float
    direction_bias: str

def _fit_line(xs, ys):
    if len(xs) < 2: 
        return None
    m,b = np.polyfit(xs, ys, 1)
    return m,b


from typing import Optional, List, Dict, Tuple
import numpy as np
import pandas as pd

# ========= å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _atr(high, low, close, window=14):
    high = np.asarray(high, dtype=float)
    low  = np.asarray(low,  dtype=float)
    close= np.asarray(close,dtype=float)
    prev_close = np.roll(close, 1)
    tr = np.maximum(high - low, np.maximum(np.abs(high - prev_close), np.abs(low - prev_close)))
    tr[0] = high[0] - low[0]
    alpha = 2.0 / (window + 1.0)
    atr = np.empty_like(tr)
    atr[0] = tr[:window].mean() if len(tr) >= window else tr[0]
    for i in range(1, len(tr)):
        atr[i] = alpha * tr[i] + (1 - alpha) * atr[i-1]
    return atr

def _pivots(series, lb=2, ub=2):
    x = np.asarray(series, dtype=float)
    n = len(x)
    is_max = np.zeros(n, dtype=bool)
    is_min = np.zeros(n, dtype=bool)
    for i in range(lb, n-ub):
        window = x[i-lb:i+ub+1]
        if np.argmax(window) == lb and (window[lb] > window[:lb]).all() and (window[lb] > window[lb+1:]).all():
            is_max[i] = True
        if np.argmin(window) == lb and (window[lb] < window[:lb]).all() and (window[lb] < window[lb+1:]).all():
            is_min[i] = True
    return is_max, is_min

def _fit_line(xs, ys) -> Tuple[float,float,float]:
    if len(xs) < 2: return 0.0, 0.0, 0.0
    x = np.asarray(xs, dtype=float)
    y = np.asarray(ys, dtype=float)
    x0 = x - x.mean()
    s, b = np.polyfit(x0, y, 1)
    intercept = b - s * (-x.mean())
    yhat = s * x0 + b
    ss_res = np.sum((y - yhat)**2)
    ss_tot = np.sum((y - y.mean())**2)
    r2 = 1 - ss_res/ss_tot if ss_tot > 1e-12 else 0.0
    return float(s), float(intercept), float(r2)

def _line_y(slope, intercept, x):
    return slope * x + intercept

def _norm_slope(slope, price_scale):
    return 0.0 if price_scale <= 0 else slope / float(price_scale)

# ========= Triangle Detector =========
def detect_triangles(
    df: pd.DataFrame,
    high_col: str = "high",
    low_col: str = "low",
    close_col: str = "close",
    atr_window: int = 14,
    cons_min_bars: int = 12,
    cons_max_bars: int = 40,
    pivot_lb: int = 2,
    pivot_ub: int = 2,
    flat_tol_norm: float = 0.0012,
    converge_min: float = 0.20,
    width_max_atr: float = 3.5,
    r2_min: float = 0.20,
    parallel_tol: float = 0.22,
    breakout_buffer_atr: float = 0.25,
    confirm_bars: int = 1,
    pretrend_win: int = 24,
    require_breakout: bool = False,
    last_N: int | None = 3000,
    e_step: int = 2,
    len_step: int = 2,
) -> List[Dict]:

    if any(c not in df.columns for c in [high_col, low_col, close_col]):
        raise ValueError("DataFrame must have high/low/close columns")

    # 1) ç›´è¿‘Næœ¬ã ã‘è¦‹ã‚‹
    if last_N is not None and len(df) > last_N:
        df = df.iloc[-last_N:]

    highs = df[high_col].to_numpy(dtype=float)
    lows  = df[low_col].to_numpy(dtype=float)
    close = df[close_col].to_numpy(dtype=float)
    n = len(df)
    if n < max(atr_window + cons_max_bars + 5, 80):
        return []

    atr = _atr(highs, lows, close, window=atr_window)
    is_h, _ = _pivots(highs, pivot_lb, pivot_ub)
    _, is_l = _pivots(lows,  pivot_lb, pivot_ub)
    patterns: List[Dict] = []

    # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—é–¢æ•°
    def _pretrend_slope(end_idx, win: int) -> float:
        j1 = max(0, end_idx - cons_min_bars)  # ä¸‰è§’é–‹å§‹å‰ä»˜è¿‘ã‚’ç‹™ã†
        j0 = max(0, j1 - win)
        if j1 - j0 < 5: return 0.0
        x = np.arange(j1-j0+1, dtype=float)
        y = close[j0:j1+1]
        xm, ym = x.mean(), y.mean()
        den = ((x-xm)**2).sum()
        if den <= 0: return 0.0
        return float(((x-xm)*(y-ym)).sum()/den)

    # ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºå®šãƒã‚§ãƒƒã‚¯
    def _confirm_break(e_idx: int, dir_side: str, m_up, b_up, m_lo, b_lo) -> Tuple[bool, Optional[int], float, float]:
        """dir_side: 'up' or 'down'; æˆ»ã‚Š: (ç¢ºå®š?, ç¢ºå®šãƒãƒ¼idx, entry_price, stop_suggest)"""
        seq = 0
        j = e_idx
        while j < n:
            up_y = _line_y(m_up, b_up, j)
            lo_y = _line_y(m_lo, b_lo, j)
            thr = breakout_buffer_atr * atr[j]
            c = close[j]
            if dir_side == "up":
                ok = (c >= up_y + thr)
            else:
                ok = (c <= lo_y - thr)
            seq = seq + 1 if ok else 0
            if seq >= max(1, int(confirm_bars)):
                # entry/stopï¼ˆå€™è£œï¼‰
                if dir_side == "up":
                    entry = max(up_y + thr, c)  # çµ‚å€¤ç¢ºå®šæ™‚ç‚¹ã®ä¾¡æ ¼ã‚’å„ªå…ˆ
                    stop  = lo_y - 0.25 * atr[j]
                else:
                    entry = min(lo_y - thr, c)
                    stop  = up_y + 0.25 * atr[j]
                return True, j, float(entry), float(stop)
            # ç›´è¿‘æ•°æœ¬ã®ã¿ç¢ºèªï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—å›é¿ï¼‰
            if j - e_idx > 3: break
            j += 1
        return False, None, np.nan, np.nan


    # 2) ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰
    for e in range(atr_window + cons_min_bars, n, e_step):
        best = None  # (quality, dict)
        # è¤‡æ•°é•·ã•ã§æ¢ç´¢ã—ã€æœ€è‰¯ã ã‘æ¡ç”¨
        for cons_len in range(cons_min_bars, cons_max_bars+1, len_step):
            s = e - cons_len + 1
            if s < atr_window:
                continue

            # 3) äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå›å¸°å‰ã®æ—©è½ã¨ã—ï¼‰
            width_raw = highs[s:e+1].max() - lows[s:e+1].min()
            if width_raw > width_max_atr * atr[e] * 1.5:
                continue
            atr_cons = atr[s:e+1].mean(); atr_prev = atr[max(s-14,0):s].mean()
            if atr_cons > atr_prev * 0.95:
                continue

            idxs = np.arange(s, e+1)
            hi_idx = idxs[is_h[s:e+1]]
            lo_idx = idxs[is_l[s:e+1]]

            # ãƒ”ãƒœãƒƒãƒˆãŒè¶³ã‚Šãªã„å ´åˆã¯æ¥µå€¤ã§è£œå®Œ
            if len(hi_idx) < 2 or len(lo_idx) < 2:
                k = min(4, len(idxs))
                if k < 2:
                    continue
                top_hi_idx = idxs[np.argsort(highs[s:e+1])[-k:]]
                top_lo_idx = idxs[np.argsort(lows[s:e+1])[:k]]
                hi_idx = np.sort(top_hi_idx[:max(2, len(top_hi_idx)//2)])
                lo_idx = np.sort(top_lo_idx[:max(2, len(top_lo_idx)//2)])

            uh_slope, uh_inter, uh_r2 = _fit_line(hi_idx, highs[hi_idx])
            lh_slope, lh_inter, lh_r2 = _fit_line(lo_idx, lows[lo_idx])

            price_scale = close[s:e+1].mean()
            uh_n = _norm_slope(uh_slope, price_scale)
            lh_n = _norm_slope(lh_slope, price_scale)

            # å½“ã¦ã¯ã¾ã‚Šæœ€ä½é™
            if (uh_r2 + lh_r2)/2.0 < r2_min:
                continue

            # é–‹å§‹å¹…ã¨çµ‚äº†å¹…ï¼ˆåæŸãƒã‚§ãƒƒã‚¯ï¼‰
            width_s = _line_y(uh_slope, uh_inter, s) - _line_y(lh_slope, lh_inter, s)
            width_e = _line_y(uh_slope, uh_inter, e) - _line_y(lh_slope, lh_inter, e)
            if width_s <= 0 or width_e <= 0:
                continue
            # å¹…ãŒé¦¬é¹¿ãƒ‡ã‚«ã„ã‚‚ã®ã‚’å¼¾ãï¼ˆçµ‚ç›¤å¹…ï¼‰
            if width_e > width_max_atr * atr[e]:
                continue

            # åæŸç‡ï¼ˆã©ã‚Œã ã‘ç‹­ã¾ã£ãŸã‹ï¼‰: (width_s - width_e)/width_s
            converge = (width_s - width_e) / max(width_s, 1e-9)
            if converge < converge_min:
                continue

            # å‹åˆ¤å®š
            tri_type = None
            # ä¸Šæ˜‡ä¸‰è§’
            if abs(uh_n) <= flat_tol_norm and lh_n > flat_tol_norm:
                tri_type = "ascending_triangle"
            # ä¸‹é™ä¸‰è§’
            elif abs(lh_n) <= flat_tol_norm and uh_n < -flat_tol_norm:
                tri_type = "descending_triangle"
            else:
                # ã‚·ãƒ³ãƒ¡ãƒˆãƒªã‚«ãƒ«
                if np.sign(uh_n) != np.sign(lh_n) and np.sign(uh_n)!=0 and np.sign(lh_n)!=0:
                    rel = abs(abs(uh_n) - abs(lh_n)) / max(abs(uh_n), abs(lh_n), 1e-9)
                    if rel <= parallel_tol:
                        tri_type = "sym_triangle"

            if tri_type is None:
                continue

            # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆä¸Šæ˜‡/ä¸‹é™/ä¸­ç«‹ã§åŠ ç‚¹ï¼‰
            pre_slope = _pretrend_slope(e, pretrend_win)
            pre_bias = 0.0
            if tri_type == "ascending_triangle":
                pre_bias = 1.0 if pre_slope > 0 else 0.0
            elif tri_type == "descending_triangle":
                pre_bias = 1.0 if pre_slope < 0 else 0.0
            else:
                pre_bias = 0.5  # ä¸­ç«‹

            # ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºå®šï¼ˆä»»æ„ï¼‰
            breakout_idx = None
            entry = np.nan
            stop  = np.nan
            target= np.nan
            broken = False
            # æ–¹å‘ä»®å®š
            if tri_type == "ascending_triangle":
                expect = "up"
            elif tri_type == "descending_triangle":
                expect = "down"
            else:
                # ã‚·ãƒ³ãƒ¡ãƒˆãƒªã‚«ãƒ«ã¯ã€Œç›´è¿‘ã®çµ‚å€¤ä½ç½®ã€ã§æš«å®šæ–¹å‘ã‚’ä»®å®š
                mid_y = (_line_y(uh_slope, uh_inter, e) + _line_y(lh_slope, lh_inter, e)) * 0.5
                expect = "up" if close[e] >= mid_y else "down"

            # ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºèª
            broken, b_idx, entry_cand, stop_cand = _confirm_break(
                e, expect, uh_slope, uh_inter, lh_slope, lh_inter
            )
            if broken:
                breakout_idx = b_idx
                entry = entry_cand
                stop  = stop_cand

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ¸¬å®šå¹…ï¼é–‹å§‹å¹…ã‚’ãƒ™ãƒ¼ã‚¹ï¼‰
            height = width_s
            if not np.isnan(entry):
                target = entry + height if expect == "up" else entry - height
            else:
                # æœªç¢ºå®šã§ã‚‚ãƒ©ã‚¤ãƒ³éš›ã®å‚è€ƒå€¤
                up_e = _line_y(uh_slope, uh_inter, e)
                lo_e = _line_y(lh_slope, lh_inter, e)
                entry = up_e if expect == "up" else lo_e
                stop  = lo_e - 0.25*atr[e] if expect == "up" else up_e + 0.25*atr[e]
                target= entry + height if expect == "up" else entry - height

            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0â€“1ï¼‰
            fit_q   = max(0.0, min(1.0, (uh_r2 + lh_r2)/2.0))
            conv_q  = max(0.0, min(1.0, (converge - converge_min) / max(1e-9, 1.0 - converge_min)))
            flat_q  = 1.0 - min(1.0, abs(uh_n)/flat_tol_norm) if tri_type=="ascending_triangle" else \
                      1.0 - min(1.0, abs(lh_n)/flat_tol_norm) if tri_type=="descending_triangle" else \
                      1.0 - min(1.0, abs(abs(uh_n)-abs(lh_n))/max(abs(uh_n),abs(lh_n),1e-9))
            touch_q = min(1.0, (len(hi_idx)>=3) * 0.5 + (len(lo_idx)>=3) * 0.5)
            pre_q   = pre_bias
            quality = float(np.clip(0.35*fit_q + 0.30*conv_q + 0.15*flat_q + 0.10*touch_q + 0.10*pre_q, 0, 1))

            pat = {
                "type": tri_type,
                "dir": "bull" if expect=="up" else "bear",
                "start_idx": int(s),
                "end_idx": int(e),
                "breakout_idx": int(breakout_idx) if breakout_idx is not None else None,
                "upper_line": (float(uh_slope), float(uh_inter), float(uh_r2)),
                "lower_line": (float(lh_slope), float(lh_inter), float(lh_r2)),
                "width_start": float(width_s),
                "width_end": float(width_e),
                "converge_ratio": float(converge),
                "touches_upper": int(len(hi_idx)),
                "touches_lower": int(len(lo_idx)),
                "quality_score": quality,
                "entry": float(entry),
                "stop": float(stop),
                "target": float(target),
            }

            if require_breakout and breakout_idx is None:
                continue

            # åŒä¸€çµ‚ç«¯eã§æœ€è‰¯ã ã‘æ¡ç”¨
            cand = (quality, pat)
            if best is None or cand[0] > best[0]:
                best = cand

        if best is not None:
            patterns.append(best[1])

    return patterns

def detect_triangles_df(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    pats = detect_triangles(df, **kwargs)
    if not pats:
        return pd.DataFrame(columns=[
            "type","dir","start_idx","end_idx","breakout_idx",
            "upper_slope","upper_intercept","upper_r2",
            "lower_slope","lower_intercept","lower_r2",
            "width_start","width_end","converge_ratio",
            "touches_upper","touches_lower",
            "quality_score","entry","stop","target"
        ])
    rows = []
    for p in pats:
        rows.append([
            p["type"], p["dir"], p["start_idx"], p["end_idx"], p["breakout_idx"],
            p["upper_line"][0], p["upper_line"][1], p["upper_line"][2],
            p["lower_line"][0], p["lower_line"][1], p["lower_line"][2],
            p["width_start"], p["width_end"], p["converge_ratio"],
            p["touches_upper"], p["touches_lower"],
            p["quality_score"], p["entry"], p["stop"], p["target"]
        ])
    cols = ["type","dir","start_idx","end_idx","breakout_idx",
            "upper_slope","upper_intercept","upper_r2",
            "lower_slope","lower_intercept","lower_r2",
            "width_start","width_end","converge_ratio",
            "touches_upper","touches_lower",
            "quality_score","entry","stop","target"]
    return pd.DataFrame(rows, columns=cols)


from typing import Optional, List, Dict, Tuple
import numpy as np
import pandas as pd

# ===== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ—¢ã«å®šç¾©æ¸ˆã¿ãªã‚‰é‡è¤‡å¯ï¼‰ =====
def _atr(high, low, close, window=14):
    high = np.asarray(high, dtype=float)
    low  = np.asarray(low,  dtype=float)
    close= np.asarray(close,dtype=float)
    prev_close = np.roll(close, 1)
    tr = np.maximum(high - low, np.maximum(np.abs(high - prev_close), np.abs(low - prev_close)))
    tr[0] = high[0] - low[0]
    alpha = 2.0 / (window + 1.0)
    atr = np.empty_like(tr)
    atr[0] = tr[:window].mean() if len(tr) >= window else tr[0]
    for i in range(1, len(tr)):
        atr[i] = alpha * tr[i] + (1 - alpha) * atr[i-1]
    return atr

def _pivots(series, lb=2, ub=2):
    x = np.asarray(series, dtype=float)
    n = len(x)
    is_max = np.zeros(n, dtype=bool)
    is_min = np.zeros(n, dtype=bool)
    for i in range(lb, n-ub):
        window = x[i-lb:i+ub+1]
        if np.argmax(window) == lb and (window[lb] > window[:lb]).all() and (window[lb] > window[lb+1:]).all():
            is_max[i] = True
        if np.argmin(window) == lb and (window[lb] < window[:lb]).all() and (window[lb] < window[lb+1:]).all():
            is_min[i] = True
    return is_max, is_min

def _fit_line(xs, ys) -> Tuple[float,float,float]:
    if len(xs) < 2: return 0.0, 0.0, 0.0
    x = np.asarray(xs, dtype=float)
    y = np.asarray(ys, dtype=float)
    x0 = x - x.mean()
    s, b = np.polyfit(x0, y, 1)
    intercept = b - s * (-x.mean())
    yhat = s * x0 + b
    ss_res = np.sum((y - yhat)**2)
    ss_tot = np.sum((y - y.mean())**2)
    r2 = 1 - ss_res/ss_tot if ss_tot > 1e-12 else 0.0
    return float(s), float(intercept), float(r2)

def _line_y(slope, intercept, x): return slope * x + intercept
def _norm_slope(slope, price_scale): return 0.0 if price_scale <= 0 else slope / float(price_scale)

# ===== Rectangle Detector =====
def detect_rectangles(
    df: pd.DataFrame,
    high_col: str = "high",
    low_col: str = "low",
    close_col: str = "close",
    atr_window: int = 14,
    win_min_bars: int = 10,
    win_max_bars: int = 48,
    pivot_lb: int = 2,
    pivot_ub: int = 2,
    flat_tol_norm: float = 0.0009,     # ã€Œæ°´å¹³ã€è¨±å®¹ã®æ­£è¦åŒ–å‚¾ã
    drift_tol_norm: float = 0.0007,    # ä¸­å¿ƒãƒ‰ãƒªãƒ•ãƒˆï¼ˆä¸Šä¸‹ç·šä¸­ç‚¹ã®å‚¾ãï¼‰è¨±å®¹
    width_max_atr: float = 3.5,        # çµ‚ç›¤å¹…ãŒ ATRÃ—ã“ã®å€ä»¥ä¸‹
    width_stability_max: float = 0.28, # å¹…ã®å®‰å®šåº¦ï¼ˆstd/meanï¼‰ä¸Šé™
    min_touches_each: int = 3,         # å„è¾ºã®æœ€å°ã‚¿ãƒƒãƒå›æ•°
    touch_tol_atr: float = 0.25,       # ã‚¿ãƒƒãƒåˆ¤å®šã®è¨±å®¹ï¼ˆATRå€ï¼‰
    r2_min: float = 0.12,              # ãƒ©ã‚¤ãƒ³å½“ã¦ã¯ã¾ã‚Šã®æœ€ä½RÂ²
    breakout_buffer_atr: float = 0.30, # ãƒ–ãƒ¬ã‚¤ã‚¯åˆ¤å®šã®ATRãƒãƒƒãƒ•ã‚¡
    confirm_bars: int = 1,             # ãƒ–ãƒ¬ã‚¤ã‚¯çµ‚å€¤ã®é€£ç¶šç¢ºå®šæœ¬æ•°
    require_breakout: bool = False,    # Trueãªã‚‰ç¢ºå®šã®ã¿æ¡ç”¨
    # é€Ÿåº¦å¯¾ç­–
    last_N: Optional[int] = 3000,      # ç›´è¿‘Næœ¬ã«é™å®š
    e_step: int = 1,                   # çµ‚ç«¯ã®ã‚¹ãƒ†ãƒƒãƒ—
    len_step: int = 1,                 # çª“é•·ã®ã‚¹ãƒ†ãƒƒãƒ—
) -> List[Dict]:
    if any(c not in df.columns for c in [high_col, low_col, close_col]):
        raise ValueError("DataFrame must have high/low/close columns")

    if last_N is not None and len(df) > last_N:
        df = df.iloc[-last_N:].copy()

    highs = df[high_col].to_numpy(dtype=float)
    lows  = df[low_col].to_numpy(dtype=float)
    close = df[close_col].to_numpy(dtype=float)
    n = len(df)
    if n < max(atr_window + win_max_bars + 5, 80):
        return []

    atr = _atr(highs, lows, close, window=atr_window)
    is_h, _ = _pivots(highs, pivot_lb, pivot_ub)
    _, is_l = _pivots(lows,  pivot_lb, pivot_ub)

    patterns: List[Dict] = []

    def _confirm_break(e_idx: int, side: str, m_up, b_up, m_lo, b_lo):
        """side: 'up'|'down'"""
        seq = 0
        j = e_idx
        while j < n:
            up = _line_y(m_up, b_up, j)
            lo = _line_y(m_lo, b_lo, j)
            thr = breakout_buffer_atr * atr[j]
            c = close[j]
            ok = (c >= up + thr) if side == "up" else (c <= lo - thr)
            seq = seq + 1 if ok else 0
            if seq >= max(1,int(confirm_bars)):
                entry = max(up+thr, c) if side=="up" else min(lo-thr, c)
                stop  = lo - 0.25*atr[j] if side=="up" else up + 0.25*atr[j]
                return True, j, float(entry), float(stop)
            if j - e_idx > 3: break
            j += 1
        return False, None, np.nan, np.nan

    for e in range(atr_window + win_min_bars, n, e_step):
        best = None  # (quality, dict)
        for L in range(win_min_bars, win_max_bars+1, len_step):
            s = e - L + 1
            if s < atr_window: 
                continue
            idxs = np.arange(s, e+1)

            # ãƒ”ãƒœãƒƒãƒˆæŠ½å‡º
            hi_idx = idxs[is_h[s:e+1]]
            lo_idx = idxs[is_l[s:e+1]]
            # ã‚¿ãƒƒãƒä¸è¶³ãªã‚‰æ¥µå€¤è£œå®Œï¼ˆè»½é‡ï¼‰
            if len(hi_idx) < 2 or len(lo_idx) < 2:
                k = min(4, len(idxs))
                if k < 2: 
                    continue
                top_hi = idxs[np.argsort(highs[s:e+1])[-k:]]
                bot_lo = idxs[np.argsort(lows[s:e+1])[:k]]
                hi_idx = np.sort(top_hi[:max(2, len(top_hi)//2)])
                lo_idx = np.sort(bot_lo[:max(2, len(bot_lo)//2)])

            # ç·šå½“ã¦
            uh_s, uh_b, uh_r2 = _fit_line(hi_idx, highs[hi_idx])
            lh_s, lh_b, lh_r2 = _fit_line(lo_idx, lows[lo_idx])

            price_scale = close[s:e+1].mean()
            uh_n = _norm_slope(uh_s, price_scale)
            lh_n = _norm_slope(lh_s, price_scale)
            mid_n = _norm_slope((uh_s + lh_s)/2.0, price_scale)

            # æ°´å¹³åº¦ã¨å½“ã¦ã¯ã¾ã‚Š
            if abs(uh_n) > flat_tol_norm or abs(lh_n) > flat_tol_norm:
                continue
            if (uh_r2 + lh_r2)/2.0 < r2_min:
                continue
            if abs(mid_n) > drift_tol_norm:
                continue

            # å¹…ã¨å®‰å®šæ€§
            width = _line_y(uh_s, uh_b, idxs) - _line_y(lh_s, lh_b, idxs)
            if np.any(width <= 0):
                continue
            w_mean = float(width.mean())
            w_std  = float(width.std(ddof=0))
            if w_mean > width_max_atr * atr[e]:
                continue
            w_stab = (w_std / max(w_mean, 1e-9))
            if w_stab > width_stability_max:
                continue

            # ã‚¿ãƒƒãƒåˆ¤å®šï¼ˆãƒ©ã‚¤ãƒ³Â±tolã«å…¥ã£ãŸãƒ”ãƒœãƒƒãƒˆæ•°ï¼‰
            tol = touch_tol_atr * atr[e]
            up_vals = _line_y(uh_s, uh_b, hi_idx)
            lo_vals = _line_y(lh_s, lh_b, lo_idx)
            touch_up = int(np.sum(np.abs(highs[hi_idx] - up_vals) <= tol))
            touch_lo = int(np.sum(np.abs(lows[lo_idx]  - lo_vals) <= tol))
            if touch_up < min_touches_each or touch_lo < min_touches_each:
                continue

            # æœŸå¾…æ–¹å‘ã¯æœªç¢ºå®šã€‚ç›´è¿‘ã®ä½ç½®ã§ä»®å®šï¼ˆä¸ŠåŠåˆ†â†’upã€ä¸‹åŠåˆ†â†’downï¼‰
            up_e = float(_line_y(uh_s, uh_b, e))
            lo_e = float(_line_y(lh_s, lh_b, e))
            mid_e = (up_e + lo_e) * 0.5
            expect = "up" if close[e] >= mid_e else "down"

            # ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºèªï¼ˆä»»æ„ï¼‰
            broken, b_idx, entry, stop = _confirm_break(e, expect, uh_s, uh_b, lh_s, lh_b)
            if require_breakout and not broken:
                continue

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ãƒ¬ãƒ³ã‚¸é«˜ï¼ˆé–‹å§‹å¹… or å¹³å‡å¹…ï¼‰
            height = float(width[0])  # é–‹å§‹å¹…
            if np.isnan(entry):
                # æœªç¢ºå®šã§ã‚‚å‚è€ƒå€¤ï¼ˆãƒ©ã‚¤ãƒ³éš›ï¼‰
                entry = up_e if expect=="up" else lo_e
                stop  = lo_e - 0.25*atr[e] if expect=="up" else up_e + 0.25*atr[e]
            target = entry + height if expect=="up" else entry - height

            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0â€“1ï¼‰
            fit_q   = max(0.0, min(1.0, (uh_r2 + lh_r2)/2.0))
            flat_q  = 1.0 - min(1.0, max(abs(uh_n), abs(lh_n)) / flat_tol_norm)
            stab_q  = 1.0 - min(1.0, w_stab / width_stability_max)
            touch_q = min(1.0, 0.5*min(1.0, touch_up/min_touches_each) + 0.5*min(1.0, touch_lo/min_touches_each))
            drift_q = 1.0 - min(1.0, abs(mid_n)/drift_tol_norm)
            quality = float(np.clip(0.30*fit_q + 0.25*flat_q + 0.20*stab_q + 0.15*touch_q + 0.10*drift_q, 0, 1))

            pat = {
                "type": "rectangle",
                "dir": "bull" if expect=="up" else "bear",  # æœŸå¾…æ–¹å‘ï¼ˆæš«å®šï¼ç¢ºå®šã§ä¸Šæ›¸ãå¯ï¼‰
                "start_idx": int(s),
                "end_idx": int(e),
                "breakout_idx": int(b_idx) if b_idx is not None else None,
                "upper_line": (float(uh_s), float(uh_b), float(uh_r2)),
                "lower_line": (float(lh_s), float(lh_b), float(lh_r2)),
                "width_mean": float(w_mean),
                "width_std": float(w_std),
                "width_stability": float(w_stab),
                "touches_upper": int(touch_up),
                "touches_lower": int(touch_lo),
                "quality_score": quality,
                "entry": float(entry), "stop": float(stop), "target": float(target),
            }

            cand = (quality, pat)
            if best is None or cand[0] > best[0]:
                best = cand

        if best is not None:
            patterns.append(best[1])

    return patterns

def detect_rectangles_df(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    pats = detect_rectangles(df, **kwargs)
    if not pats:
        return pd.DataFrame(columns=[
            "type","dir","start_idx","end_idx","breakout_idx",
            "upper_slope","upper_intercept","upper_r2",
            "lower_slope","lower_intercept","lower_r2",
            "width_mean","width_std","width_stability",
            "touches_upper","touches_lower",
            "quality_score","entry","stop","target"
        ])
    rows = []
    for p in pats:
        rows.append([
            p["type"], p["dir"], p["start_idx"], p["end_idx"], p["breakout_idx"],
            p["upper_line"][0], p["upper_line"][1], p["upper_line"][2],
            p["lower_line"][0], p["lower_line"][1], p["lower_line"][2],
            p["width_mean"], p["width_std"], p["width_stability"],
            p["touches_upper"], p["touches_lower"],
            p["quality_score"], p["entry"], p["stop"], p["target"]
        ])
    cols = ["type","dir","start_idx","end_idx","breakout_idx",
            "upper_slope","upper_intercept","upper_r2",
            "lower_slope","lower_intercept","lower_r2",
            "width_mean","width_std","width_stability",
            "touches_upper","touches_lower",
            "quality_score","entry","stop","target"]
    return pd.DataFrame(rows, columns=cols)

def detect_double_top_bottom(
    df, piv_high, piv_low, *,
    lookback=200,
    # === æ¨å¥¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(USDJPY 15m) ===
    tol_mode="atr",       # "pct" or "atr"ï¼ˆãƒœãƒ©æ­£è¦åŒ–ï¼‰
    tol_pct=0.10,
    tol_atrK=0.55,        # 2å±±/2åº•ã®åŒå€¤è¨±å®¹ï¼ˆATRå€ï¼‰
    min_sep_bars=10,      # ãƒ”ãƒ¼ã‚¯é–“ã®æœ€ä½ãƒãƒ¼æ•°
    min_depth_atr=0.90,   # M/W ã®è°·/å±±ã®æœ€å°æ·±ã•ï¼ˆATRå€ï¼‰
    confirm_bars=2,       # ãƒãƒƒã‚¯çµ‚å€¤ãƒ–ãƒ¬ã‚¤ã‚¯ã®é€£ç¶šæœ¬æ•°
    neck_break_atr=0.15,  # ãƒãƒƒã‚¯æŠœã‘åˆ¤å®šï¼ˆATRå€ï¼‰
    retest_within=30,     # ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã®ãƒªãƒ†ã‚¹ãƒˆæ¢ç´¢çª“
    retest_tol_atr=0.25,  # ãƒªãƒ†ã‚¹ãƒˆã¯ãƒãƒƒã‚¯Â±ATR*ä¿‚æ•°å†…
    pretrend_win=24,      # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰å‚¾ãç®—å‡ºã®çª“ï¼ˆ15mÃ—24=ç´„6hï¼‰
    pretrend_min=0.0,     # 0ä»¥ä¸Šâ†’topã¯ä¸Šæ˜‡/ bottomã¯ä¸‹é™ã‚’å„ªå…ˆ
):
    import numpy as np, pandas as pd

    if len(df) < max(40, lookback // 2):
        return []

    # ---- å¯¾è±¡çª“ï¼ˆIndexå‰æï¼‰----
    sub = df.tail(lookback).copy()
    idx = sub.index
    c = sub["close"].astype(float)
    h = sub["high"].astype(float)
    l = sub["low"].astype(float)

    # ---- ATR14ï¼ˆfallbackä»˜ãï¼‰----
    def _atr14(_df):
        _h, _l, _c = _df["high"].astype(float), _df["low"].astype(float), _df["close"].astype(float)
        pc = _c.shift(1)
        tr = pd.concat([(_h - _l).abs(), (_h - pc).abs(), (_l - pc).abs()], axis=1).max(axis=1)
        return tr.rolling(14, min_periods=8).mean()
    a14 = _atr14(sub).ffill()
    a14_med = float(a14.median()) if a14.notna().any() else float((h - l).abs().rolling(14).mean().iloc[-1])

    # ---- å˜å›å¸°å‚¾ãï¼ˆäº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰----
    def _slope(s, win):
        x = np.arange(win, dtype=float)
        def _sl(arr):
            y = np.asarray(arr, float)
            xm, ym = x.mean(), y.mean()
            den = ((x - xm) ** 2).sum()
            if den <= 0: return 0.0
            return float(((x - xm) * (y - ym)).sum() / den)
        return s.rolling(win, min_periods=win).apply(_sl, raw=True)
    slope = _slope(c, pretrend_win)

    def _eq_peak(v1, v2):
        if tol_mode == "atr":
            return abs(v1 - v2) <= float(tol_atrK) * a14_med
        base = max(1e-9, (abs(v1) + abs(v2)) * 0.5)
        return abs(v1 - v2) <= base * float(tol_pct)

    def _confirm_break(i2, neck, side: str):
        # i2(å³è‚©bar) ä»¥é™ã§çµ‚å€¤ãŒ neckÂ±(ATR*neck_break_atr) ã‚’é€£ç¶š confirm_bars æœ¬
        thr = float(neck_break_atr) * a14_med
        seg = sub.loc[i2:]
        if seg.empty: return (False, None)
        cc = seg["close"].astype(float)
        if side == "down":
            hit = (cc <= (neck - thr)).rolling(confirm_bars).sum() >= confirm_bars
        else:
            hit = (cc >= (neck + thr)).rolling(confirm_bars).sum() >= confirm_bars
        if not hit.any(): return (False, None)
        # æœ€åˆã«æ¡ä»¶ã‚’æº€ãŸã—ãŸè¡Œãƒ©ãƒ™ãƒ«ï¼ˆIndexãƒ©ãƒ™ãƒ«ï¼‰ã‚’è¿”ã™
        j = hit[hit].index[0]
        return (True, j)

    def _retest_after(brk_label, neck, tol_atr, within):
        if brk_label is None: return (False, None)
        start_pos = idx.get_loc(brk_label) + 1
        end_pos = min(len(idx) - 1, start_pos + int(within))
        if start_pos >= end_pos: return (False, None)
        seg = sub.iloc[start_pos:end_pos + 1]
        tol = float(tol_atr) * a14_med
        hit = seg[(seg["high"] >= neck - tol) & (seg["low"] <= neck + tol)]
        if hit.empty: return (False, None)
        return (True, hit.index[0])

    # ===== å“è³ªã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ã¯ã“ã“ã§èª¿æ•´ï¼‰=====
    def _quality(eq_score, depth_z, gap_bars, confirmed, retested, trend_ok):
        q = 50.0
        q += 20.0 * max(0.0, min(1.0, eq_score))             # 2å±±/2åº•ã®åŒå€¤åº¦ï¼ˆæœ€é‡è¦ï¼‰
        q += min(20.0, 6.0 * max(0.0, depth_z - 0.5))        # æ·±ã•ï¼ˆ0.5ATRè¶…ã‹ã‚‰åšã‚ã«åŠ ç‚¹ï¼‰
        q += 8.0 if confirmed else 0.0                       # ãƒãƒƒã‚¯çµ‚å€¤ç¢ºå®š
        q += 7.0 if retested  else 0.0                       # ãƒªãƒ†ã‚¹ãƒˆç¢ºèª
        q += 5.0 if trend_ok  else 0.0                       # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰æ•´åˆ
        # ãƒ”ãƒ¼ã‚¯é–“éš”ï¼š12ã€œ28æœ¬ãŒç†æƒ³ã€å¤–ã‚Œã‚‹ã»ã©æ¸›ç‚¹ï¼ˆæœ€å¤§10ï¼‰
        q -= min(10.0, abs(int(gap_bars) - 20) * 0.4)
        return float(max(0.0, min(99.0, q)))

    out = []

    # -------- Double Top --------
    highs = piv_high.index.intersection(idx).sort_values()
    for i in range(len(highs) - 1):
        i1, i2 = highs[i], highs[i + 1]
        gap = idx.get_loc(i2) - idx.get_loc(i1)
        if gap < int(min_sep_bars): 
            continue
        h1, h2 = float(df.loc[i1, "high"]), float(df.loc[i2, "high"])
        if not _eq_peak(h1, h2):
            continue
        span = sub.loc[i1:i2]
        neck = float(span["low"].min())
        topv = (h1 + h2) * 0.5
        depth = topv - neck
        if depth < float(min_depth_atr) * a14_med:
            continue

        confirmed, c_lab = _confirm_break(i2, neck, side="down")
        retested, r_lab  = _retest_after(c_lab if confirmed else i2, neck, retest_tol_atr, retest_within)

        sl = float(slope.loc[i1]) if not np.isnan(slope.loc[i1]) else 0.0
        trend_ok = (sl > float(pretrend_min))

        eq_score = 1.0 - (abs(h1 - h2) / max(1e-9, abs(topv)))
        depth_z  = depth / max(1e-9, a14_med)
        q = _quality(eq_score, depth_z, gap, confirmed, retested, trend_ok)

        out.append(Pattern(
            "double_top",
            t_start=i1, t_end=i2,
            params={"top": float(topv), "neck": float(neck),
                    "confirmed": bool(confirmed), "confirm_ts": c_lab,
                    "retested": bool(retested), "retest_ts": r_lab},
            quality=q,
            direction_bias="down"
        ))

    # -------- Double Bottom --------
    lows = piv_low.index.intersection(idx).sort_values()
    for i in range(len(lows) - 1):
        i1, i2 = lows[i], lows[i + 1]
        gap = idx.get_loc(i2) - idx.get_loc(i1)
        if gap < int(min_sep_bars):
            continue
        l1, l2 = float(df.loc[i1, "low"]), float(df.loc[i2, "low"])
        if not _eq_peak(l1, l2):
            continue
        span = sub.loc[i1:i2]
        neck = float(span["high"].max())
        botv = (l1 + l2) * 0.5
        depth = neck - botv
        if depth < float(min_depth_atr) * a14_med:
            continue

        confirmed, c_lab = _confirm_break(i2, neck, side="up")
        retested, r_lab  = _retest_after(c_lab if confirmed else i2, neck, retest_tol_atr, retest_within)

        sl = float(slope.loc[i1]) if not np.isnan(slope.loc[i1]) else 0.0
        trend_ok = (sl < -float(pretrend_min))

        eq_score = 1.0 - (abs(l1 - l2) / max(1e-9, abs(botv)))
        depth_z  = depth / max(1e-9, a14_med)
        q = _quality(eq_score, depth_z, gap, confirmed, retested, trend_ok)

        out.append(Pattern(
            "double_bottom",
            t_start=i1, t_end=i2,
            params={"bottom": float(botv), "neck": float(neck),
                    "confirmed": bool(confirmed), "confirm_ts": c_lab,
                    "retested": bool(retested), "retest_ts": r_lab},
            quality=q,
            direction_bias="up"
        ))

    return out


def detect_flag_pennant(df, lookback=220, Npush=30, min_flag_bars=8, max_flag_bars=40, sigma_k=1.0, pole_min_atr=2.0):
    if len(df) < Npush + max_flag_bars + 5: return []
    sub = df.tail(lookback)
    cons = sub.tail(max_flag_bars)
    pole = sub.iloc[-(max_flag_bars+Npush):-max_flag_bars]
    if len(pole) < 5 or len(cons) < min_flag_bars: return []

    pole_len = float(pole["close"].iloc[-1] - pole["close"].iloc[0])
    from flag_pennant_detector import detect_flag_pennant
    pole_dir = "up" if pole_len > 0 else "down"
    pole_abs = abs(pole_len)

    atr_now = float(atr(sub, 14).iloc[-1]) if len(sub)>=14 else 0.0
    if atr_now <= 0: return []
    if pole_abs < pole_min_atr * atr_now:
        return []

    y = cons["close"].values
    x = np.arange(len(y))
    m, b = np.polyfit(x, y, 1)
    resid = y - (m*x + b)
    sigma = float(np.std(resid))
    if sigma <= 0: return []
    x_now = len(x)-1
    y_mid_now = m*x_now + b
    up_now = y_mid_now + sigma_k*sigma
    dn_now = y_mid_now - sigma_k*sigma

    kind = "pennant" if abs(m) < 1e-4 else ("flag_up" if m>0 else "flag_dn")
    direction_bias = "up" if pole_dir=="up" else "down"
    quality = 65.0
    quality += min(20.0, pole_abs/atr_now*3.0)
    quality += min(10.0, (1.0/(sigma/atr_now+1e-6)))

    return [Pattern(
        kind=kind,
        t_start=cons.index[0], t_end=cons.index[-1],
        params=dict(slope=m, intercept=b, sigma=sigma, band_k=float(sigma_k),
                    sub_start=cons.index[0], sub_end=cons.index[-1],
                    pole=float(pole_len), pole_abs=float(pole_abs),
                    upper_now=float(up_now), lower_now=float(dn_now)),
        quality=float(quality),
        direction_bias=direction_bias
    )]

def detect_head_shoulders(
    df: pd.DataFrame,
    piv_high: pd.DataFrame,
    piv_low: pd.DataFrame,
    *,
    lookback: int = 260,
    tol: float = 0.003,          # UIã®hs_tolï¼ˆè‚©ã®æ¯”ç‡è¨±å®¹ï¼‰ã€‚ATRåŸºæº–ã¨â€œORâ€ã§ç·©ã„æ–¹ã‚’æ¡ç”¨
    # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯USDJPY 15må‘ã‘ï¼‰
    min_sep_bars: int = 10,      # å·¦è‚©-é ­ / é ­-å³è‚© ã®æœ€å°ãƒãƒ¼é–“éš”
    shoulder_tol_atr: float = 0.60,  # è‚©ã®åŒå€¤è¨±å®¹ï¼ˆATRå€ï¼‰
    head_margin_atr: float = 0.80,   # ãƒ˜ãƒƒãƒ‰ãŒè‚©ã‚ˆã‚Šã©ã‚Œã ã‘çªå‡ºã—ã¦ã„ã‚‹ã‹ï¼ˆATRå€ï¼‰
    confirm_bars: int = 2,       # ãƒãƒƒã‚¯çµ‚å€¤ãƒ–ãƒ¬ã‚¤ã‚¯ã®é€£ç¶šæœ¬æ•°
    neck_break_atr: float = 0.15,# ãƒãƒƒã‚¯æŠœã‘åˆ¤å®šã®ATRå€
    retest_within: int = 30,     # ç¢ºå®šå¾Œã€ä½•æœ¬ä»¥å†…ã«ãƒªãƒ†ã‚¹ãƒˆæ¢ç´¢
    retest_tol_atr: float = 0.25,# ãƒªãƒ†ã‚¹ãƒˆæ™‚ã®ãƒãƒƒã‚¯Â±è¨±å®¹ï¼ˆATRå€ï¼‰
    pretrend_win: int = 24,      # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰è©•ä¾¡çª“ï¼ˆãƒãƒ¼ï¼‰
    pretrend_min: float = 0.0,   # ãƒˆãƒƒãƒ—å‹ã¯ +min ä»¥ä¸Šã€é€†H&Sã¯ -min ä»¥ä¸‹ã‚’å¥½è©•ä¾¡
    allow_incomplete: bool = False  # Trueãªã‚‰æœªç¢ºå®šã§ã‚‚å€™è£œåŒ–ï¼ˆå“è³ªã¯ä¸‹ã’ã‚‹ï¼‰
) -> list[Pattern]:
    import numpy as np, pandas as pd

    out: list[Pattern] = []

    if len(df) < max(40, lookback // 2):
        return out

    sub = df.tail(lookback).copy()
    idx = sub.index
    close = sub["close"].astype(float)
    high  = sub["high"].astype(float)
    low   = sub["low"].astype(float)

    # --- ATR14ï¼ˆfallbackä»˜ãï¼‰ ---
    def _atr14(_df):
        _h, _l, _c = _df["high"].astype(float), _df["low"].astype(float), _df["close"].astype(float)
        pc = _c.shift(1)
        tr = pd.concat([(_h-_l).abs(), (_h-pc).abs(), (_l-pc).abs()], axis=1).max(axis=1)
        return tr.rolling(14, min_periods=8).mean()
    a14 = _atr14(sub).ffill()
    a14_med = float(a14.median()) if a14.notna().any() else float((high - low).abs().rolling(14).mean().iloc[-1] or 0.0)
    if a14_med <= 0:
        # ã‚¹ã‚±ãƒ¼ãƒ«ãŒå–ã‚Œãªã„æ™‚ã¯ä¾¡æ ¼æ¯”ã§æœ€ä½é™å‹•ã
        a14_med = float(max(1e-6, (high.tail(20).max() - low.tail(20).min()) / 20.0))

    # --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
    idx_pos = {t:i for i,t in enumerate(idx)}

    def _lin_neck(x1, y1, x2, y2):
        """2ç‚¹ã‹ã‚‰ãƒãƒƒã‚¯ç›´ç·š m,b ã‚’è¿”ã™ï¼ˆxã¯ãƒãƒ¼ç•ªå·ï¼‰"""
        x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)
        if x2 == x1: 
            return 0.0, float((y1+y2)/2.0)
        m = (y2 - y1) / (x2 - x1)
        b = y1 - m * x1
        return m, b

    def _confirm_break(side: str, i_start_label) -> tuple[bool, pd.Timestamp|None]:
        """side: 'down' or 'up'ï¼ˆH&S=down, é€†H&S=upï¼‰ã€‚å³è‚©ç¢ºå®šä»¥é™ã§é€£ç¶šæœ¬ç¢ºå®šã‚’æ¢ã™ã€‚"""
        j0 = idx.get_loc(i_start_label)
        seq, hit_ts = 0, None
        for j in range(j0, len(idx)):
            ts = idx[j]
            c = float(close.loc[ts])
            # å‹•çš„ãƒãƒƒã‚¯å€¤
            neck_val = float(m_neck * j + b_neck)
            thr = neck_break_atr * a14_med
            if side == "down":
                ok = (c <= neck_val - thr)
            else:
                ok = (c >= neck_val + thr)
            seq = seq + 1 if ok else 0
            if seq >= int(confirm_bars):
                hit_ts = ts
                break
        return (hit_ts is not None), hit_ts

    def _retest_after(confirm_label, within: int, tol_atr: float, side: str) -> tuple[bool, pd.Timestamp|None]:
        if confirm_label is None:
            return (False, None)
        j0 = idx.get_loc(confirm_label) + 1
        j1 = min(len(idx)-1, j0 + int(within))
        tol = float(tol_atr) * a14_med
        for j in range(j0, j1+1):
            ts = idx[j]
            neck_val = float(m_neck * j + b_neck)
            # é«˜å®‰ã©ã¡ã‚‰ã‹ãŒãƒãƒƒã‚¯Â±tol ã«è§¦ã‚Œã‚Œã°ãƒªãƒ†ã‚¹ãƒˆæˆç«‹
            if (float(high.loc[ts]) >= neck_val - tol) and (float(low.loc[ts]) <= neck_val + tol):
                return (True, ts)
        return (False, None)

    def _pretrend_slope(end_label, win: int) -> float:
        j1 = idx.get_loc(end_label)
        j0 = max(0, j1 - int(win))
        if j1 - j0 < 3:
            return 0.0
        x = np.arange(j1-j0+1, dtype=float)
        y = close.iloc[j0:j1+1].values.astype(float)
        xm, ym = x.mean(), y.mean()
        den = ((x-xm)**2).sum()
        if den <= 0:
            return 0.0
        return float(((x-xm)*(y-ym)).sum()/den)

    # ========= 1) ãƒˆãƒƒãƒ—å‹ï¼ˆH&Sï¼‰ =========
    hs = piv_high.index.intersection(idx).sort_values()
    if len(hs) >= 3:
        for k in range(len(hs)-2):
            s1, h, s2 = hs[k], hs[k+1], hs[k+2]
            if (idx_pos[s2] - idx_pos[h] < min_sep_bars) or (idx_pos[h] - idx_pos[s1] < min_sep_bars):
                continue

            v1, vh, v2 = float(df.loc[s1,"high"]), float(df.loc[h,"high"]), float(df.loc[s2,"high"])

            # è‚©åŒå€¤ï¼šä¾¡æ ¼æ¯”(tol*vh) ã¾ãŸã¯ ATRåŸºæº–(shoulder_tol_atr*a14_med) ã®ç·©ã„æ–¹ã§åˆ¤å®š
            same_shoulder_ok = abs(v1 - v2) <= max(tol*max(vh,1e-9), shoulder_tol_atr*a14_med)
            # ãƒ˜ãƒƒãƒ‰ã®çªå‡ºï¼ˆè‚©ã‚ˆã‚Šé«˜ã„ï¼‰
            head_margin_ok   = vh >= max(v1, v2) + head_margin_atr*a14_med
            if not (same_shoulder_ok and head_margin_ok):
                continue

            # å·¦å³è°·ã®ç‰¹å®šï¼ˆãƒãƒƒã‚¯ã®2ç‚¹ï¼‰
            left_valley  = float(df.loc[s1:h, "low"].min())
            right_valley = float(df.loc[h:s2, "low"].min())
            x1, y1 = idx_pos[s1], left_valley
            x2, y2 = idx_pos[s2], right_valley
            m_neck, b_neck = _lin_neck(x1, y1, x2, y2)

            # ç¢ºå®šï¼ˆçµ‚å€¤ã§ã®ãƒãƒƒã‚¯å‰²ã‚Œ é€£ç¶šconfirm_barsï¼‰
            confirmed, c_lab = _confirm_break(side="down", i_start_label=s2)

            # ãƒªãƒ†ã‚¹ãƒˆ
            retested, r_lab = _retest_after(c_lab if confirmed else s2, retest_within, retest_tol_atr, side="down")

            # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå·¦è‚©ç›´å‰ã¾ã§ï¼‰
            sl = _pretrend_slope(s1, pretrend_win)
            pretrend_ok = (sl > float(pretrend_min))

            # æ·±ã•ï¼ˆãƒ˜ãƒƒãƒ‰âˆ’ãƒãƒƒã‚¯ä¸­å¿ƒï¼‰ã®ATRæ¯”
            neck_center = (y1 + y2) * 0.5
            depth_z = max(0.0, (vh - neck_center) / max(1e-9, a14_med))

            # æ™‚é–“å¯¾ç§°ï¼ˆè‚©é–“éš”ï¼‰
            gapL = idx_pos[h]  - idx_pos[s1]
            gapR = idx_pos[s2] - idx_pos[h]
            time_sym = 1.0 - abs(gapL - gapR) / max(1.0, max(gapL, gapR))

            # ãƒãƒƒã‚¯å‚¾ããŒæ€¥ã™ãã‚‹å ´åˆã¯æ¸›ç‚¹
            neck_slope_penalty = min(12.0, abs(m_neck) / max(1e-9, a14_med) * 6.0)

            # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0â€“99ï¼‰
            q = 45.0
            q += 20.0 * max(0.0, 1.0 - abs(v1 - v2) / max(1e-6, max(v1,v2)))   # è‚©åŒå€¤
            q += min(18.0, 6.0 * max(0.0, depth_z - 0.5))                      # æ·±ã•
            q += 8.0 if confirmed else (2.0 if allow_incomplete else 0.0)      # ç¢ºå®š
            q += 6.0 if retested  else 0.0                                      # ãƒªãƒ†ã‚¹ãƒˆ
            q += 5.0 if pretrend_ok else 0.0                                    # äº‹å‰ãƒˆãƒ¬ãƒ³ãƒ‰
            q += 5.0 * max(0.0, time_sym)                                       # æ™‚é–“å¯¾ç§°
            q -= neck_slope_penalty                                             # æ–œã‚éãæ¸›ç‚¹
            q = float(max(0.0, min(99.0, q)))

            out.append(Pattern(
                kind="head_shoulders",
                t_start=s1, t_end=s2,
                params={
                    "head": float(vh),
                    "left": float(v1),
                    "right": float(v2),
                    "neck": float(neck_center),       # äº’æ›ç”¨ï¼ˆå¹³å‡ï¼‰
                    "neck_left": float(y1),
                    "neck_right": float(y2),
                    "neck_m": float(m_neck),
                    "neck_b": float(b_neck),
                    "confirmed": bool(confirmed),
                    "confirm_ts": c_lab,
                    "retested": bool(retested),
                    "retest_ts": r_lab
                },
                quality=q,
                direction_bias="down"
            ))

    # ========= 2) é€†H&Sï¼ˆã‚¤ãƒ³ãƒãƒ¼ã‚¹ï¼‰ =========
    ls = piv_low.index.intersection(idx).sort_values()
    if len(ls) >= 3:
        for k in range(len(ls)-2):
            s1, h, s2 = ls[k], ls[k+1], ls[k+2]
            if (idx_pos[s2] - idx_pos[h] < min_sep_bars) or (idx_pos[h] - idx_pos[s1] < min_sep_bars):
                continue

            v1, vh, v2 = float(df.loc[s1,"low"]), float(df.loc[h,"low"]), float(df.loc[s2,"low"])

            same_shoulder_ok = abs(v1 - v2) <= max(tol*max(abs(vh),1e-9), shoulder_tol_atr*a14_med)
            head_margin_ok   = vh <= min(v1, v2) - head_margin_atr*a14_med
            if not (same_shoulder_ok and head_margin_ok):
                continue

            left_peak  = float(df.loc[s1:h, "high"].max())
            right_peak = float(df.loc[h:s2, "high"].max())
            x1, y1 = idx_pos[s1], left_peak
            x2, y2 = idx_pos[s2], right_peak
            m_neck, b_neck = _lin_neck(x1, y1, x2, y2)

            confirmed, c_lab = _confirm_break(side="up", i_start_label=s2)
            retested, r_lab  = _retest_after(c_lab if confirmed else s2, retest_within, retest_tol_atr, side="up")

            sl = _pretrend_slope(s1, pretrend_win)
            pretrend_ok = (sl < -float(pretrend_min))

            neck_center = (y1 + y2) * 0.5
            depth_z = max(0.0, (neck_center - vh) / max(1e-9, a14_med))

            gapL = idx_pos[h]  - idx_pos[s1]
            gapR = idx_pos[s2] - idx_pos[h]
            time_sym = 1.0 - abs(gapL - gapR) / max(1.0, max(gapL, gapR))
            neck_slope_penalty = min(12.0, abs(m_neck) / max(1e-9, a14_med) * 6.0)

            q = 45.0
            q += 20.0 * max(0.0, 1.0 - abs(v1 - v2) / max(1e-6, max(abs(v1),abs(v2))))
            q += min(18.0, 6.0 * max(0.0, depth_z - 0.5))
            q += 8.0 if confirmed else (2.0 if allow_incomplete else 0.0)
            q += 6.0 if retested  else 0.0
            q += 5.0 if pretrend_ok else 0.0
            q += 5.0 * max(0.0, time_sym)
            q -= neck_slope_penalty
            q = float(max(0.0, min(99.0, q)))

            out.append(Pattern(
                kind="inverse_head_shoulders",
                t_start=s1, t_end=s2,
                params={
                    "head": float(vh),
                    "left": float(v1),
                    "right": float(v2),
                    "neck": float(neck_center),
                    "neck_left": float(y1),
                    "neck_right": float(y2),
                    "neck_m": float(m_neck),
                    "neck_b": float(b_neck),
                    "confirmed": bool(confirmed),
                    "confirm_ts": c_lab,
                    "retested": bool(retested),
                    "retest_ts": r_lab
                },
                quality=q,
                direction_bias="up"
            ))

    return out


def measured_targets(p: Pattern):
    """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ¸¬å®šå€¤ï¼‰"""
    kind = p.get('kind') if isinstance(p, dict) else getattr(p, 'kind', None)
    params = p['params'] if isinstance(p, dict) else getattr(p, 'params', {})
    if kind and kind.startswith("triangle"):
        t = float(params.get("thickness", 0.0))
        return dict(up=+t, down=-t)
    if kind=="rectangle":
        h = float(params.get("height", 0.0))
        return dict(up=+h, down=-h)
    if kind in ("double_top","double_bottom"):
        neck = params.get("neck")
        ref  = params.get("top", params.get("bottom", None))
        if neck is not None and ref is not None:
            h = abs(float(ref) - float(neck))
            return dict(up=+h, down=-h)
    if kind in ("flag_up","flag_dn","pennant"):
        pole = float(params.get("pole_abs", 0.0))
        return dict(up=+pole, down=-pole)
    if kind in ("head_shoulders","inverse_head_shoulders"):
        neck = float(params.get("neck", 0.0))
        head = float(params.get("head", 0.0))
        h = abs(head - neck)
        return dict(up=+h, down=-h)
    return dict(up=0.0, down=0.0)

# ---- ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºè¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰----
st.sidebar.markdown("---")
st.sidebar.subheader("ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º")
enable_tri    = st.sidebar.checkbox("ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ã‚°ãƒ«ï¼ˆå¯¾ç§°/ä¸Šæ˜‡/ä¸‹é™ï¼‰", True)
enable_rect   = st.sidebar.checkbox("ãƒ¬ã‚¯ã‚¿ãƒ³ã‚°ãƒ«ï¼ˆãƒœãƒƒã‚¯ã‚¹ï¼‰", True)
enable_double = st.sidebar.checkbox("ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ— / ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ ", True)
enable_flag   = st.sidebar.checkbox("ãƒ•ãƒ©ãƒƒã‚° / ãƒšãƒŠãƒ³ãƒˆ", True)
enable_hs     = st.sidebar.checkbox("ãƒ˜ãƒƒãƒ‰ï¼†ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ã‚ºï¼ˆé€†å«ã‚€ï¼‰", True)

st.sidebar.subheader("ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®ç›´è¿‘æœ¬æ•°")
tri_lookback = st.sidebar.slider("ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ã‚°ãƒ«ï¼ˆå¯¾ç§°/ä¸Šæ˜‡/ä¸‹é™ï¼‰", 20, 600, 125, 5)
rect_lookback = st.sidebar.slider("ãƒ¬ã‚¯ã‚¿ãƒ³ã‚°ãƒ«ï¼ˆãƒœãƒƒã‚¯ã‚¹ï¼‰", 20, 600, 150, 5)
double_lookback = st.sidebar.slider("ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—/ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ ", 20, 600, 75, 5)
# â€” ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ï¼ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ  è¨­å®š â€”
st.sidebar.caption("â€” ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—ï¼ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ  è¨­å®š â€”")
dbl_tol_mode = st.sidebar.selectbox("è¨±å®¹å¹…ã®åŸºæº–", ["atr(ATRå€)", "pct(ï¼…)"], index=0)
dbl_tol_atrK = st.sidebar.number_input("ãƒ”ãƒ¼ã‚¯åŒå€¤è¨±å®¹ï¼ˆATRå€ï¼‰", value=0.55, step=0.10, min_value=0.0)
dbl_tol_pct  = st.sidebar.number_input("ãƒ”ãƒ¼ã‚¯åŒå€¤è¨±å®¹ï¼ˆï¼…ï¼‰", value=0.10, step=0.05, min_value=0.0)
dbl_min_sep  = st.sidebar.slider("2ç‚¹ã®æœ€ä½ãƒãƒ¼é–“éš”", 3, 80, 10, 1)
dbl_min_depth_atr = st.sidebar.number_input("è°·/å±±ã®æœ€å°æ·±ã•ï¼ˆATRå€ï¼‰", value=0.90, step=0.10, min_value=0.0)
dbl_require_confirm = st.sidebar.checkbox("ãƒãƒƒã‚¯ãƒ©ã‚¤ãƒ³ç¢ºå®šï¼ˆçµ‚å€¤ã§ãƒ–ãƒ¬ã‚¤ã‚¯ï¼‰ã‚’å¿…é ˆã«ã™ã‚‹", value=False)
show_only_latest_double = st.sidebar.checkbox("ãƒ€ãƒ–ãƒ«ãƒˆãƒƒãƒ—/ãƒ€ãƒ–ãƒ«ãƒœãƒˆãƒ ã¯ç›´è¿‘ã®ã¿è¡¨ç¤º", value=False)
flag_lookback = st.sidebar.slider("ãƒ•ãƒ©ãƒƒã‚°/ãƒšãƒŠãƒ³ãƒˆ", 20, 600, 100, 5)
hs_lookback = st.sidebar.slider("ãƒ˜ãƒƒãƒ‰ï¼†ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ã‚ºï¼ˆé€†å«ã‚€ï¼‰", 20, 600, 175, 5)
pat_min_touches = st.sidebar.slider("æœ€å°æ¥è§¦å›æ•°ï¼ˆç·šã¸ã®ã‚¿ãƒƒãƒæ•°ï¼‰", 2, 5, 2)
pat_tol_price = st.sidebar.number_input("ä¾¡æ ¼è¨±å®¹èª¤å·®ï¼ˆãƒ€ãƒ–ãƒ«/ãƒœãƒƒã‚¯ã‚¹ï¼‰", value=0.10, step=0.05)

# Flag/Pennant ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
st.sidebar.caption("â€” ãƒ•ãƒ©ãƒƒã‚°/ãƒšãƒŠãƒ³ãƒˆ è¨­å®š â€”")
flag_Npush = st.sidebar.slider("æ——ç«¿æ¨å®š Npushï¼ˆæœ¬ï¼‰", 10, 60, 30, 2)
flag_min_bars = st.sidebar.slider("èª¿æ•´ã‚¾ãƒ¼ãƒ³æœ€å°æœ¬æ•°", 6, 30, 10, 2)
flag_max_bars = st.sidebar.slider("èª¿æ•´ã‚¾ãƒ¼ãƒ³æœ€å¤§æœ¬æ•°", 10, 80, 40, 2)
flag_sigma_k = st.sidebar.slider("Ïƒãƒãƒ³ãƒ‰å€ç‡ï¼ˆå¢ƒç•Œï¼‰", 0.5, 3.0, 1.0, 0.5)
flag_pole_min_atr = st.sidebar.slider("æ——ç«¿ã®æœ€å°å¼·åº¦ï¼ˆATRå€ï¼‰", 1.0, 5.0, 2.0, 0.5)

# H&S ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
st.sidebar.caption("â€” ãƒ˜ãƒƒãƒ‰ï¼†ã‚·ãƒ§ãƒ«ãƒ€ãƒ¼ã‚º è¨­å®š â€”")
hs_tol = st.sidebar.slider("è‚©ã®é«˜ã•è¨±å®¹ï¼ˆæ¯”ç‡ï¼‰", 0.001, 0.02, 0.003, 0.001)

# ---- ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Ÿè¡Œ ----
patterns = []
try:
    double_patterns = []
    tri_df = None
    if enable_tri:
        tri_df = detect_triangles_df(
            df,
            high_col="high", low_col="low", close_col="close",
            atr_window=14,
            cons_min_bars=12, cons_max_bars=tri_lookback if 'tri_lookback' in locals() else 36,
            flat_tol_norm=0.0012,
            converge_min=0.25,
            width_max_atr=3.5,
            breakout_buffer_atr=0.30,
            confirm_bars=1,
            pretrend_win=24,
            require_breakout=False,
        )
        tri_df = tri_df[tri_df["quality_score"] >= 0.55].reset_index(drop=True)
        tri_df = tri_df.replace({None: np.nan})

        tol_atrK=0.55,
        min_sep_bars=10,
        min_depth_atr=0.90,
        confirm_bars=2,
        neck_break_atr=0.15,
        retest_within=30,
        retest_tol_atr=0.25,
        pretrend_win=24,
        pretrend_min=0.0,
    # ...existing code...
    patterns += [max(double_patterns, key=lambda p: p.t_end)] if (show_only_latest_double and double_patterns) else double_patterns


    if enable_flag:
        patterns_df = detect_flag_pennant(df)
        import pandas as pd
        if not isinstance(patterns_df, pd.DataFrame):
            try:
                patterns_df = pd.DataFrame(patterns_df)
            except Exception:
                patterns_df = pd.DataFrame()
        patterns += patterns_df.to_dict('records')
    if enable_hs:
        patterns += detect_head_shoulders(df, pivot_high, pivot_low, lookback=hs_lookback, tol=hs_tol)
except Exception as e:
    st.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    patterns = []

# ---- ã™ã¹ã¦ã®ãƒãƒ£ãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨ã‚’çµåˆã—ã¦è¡¨ç¤º ----
import pandas as pd
pattern_tables = []
if 'rect_df' in locals() and rect_df is not None and not rect_df.empty:
    pattern_tables.append(rect_df)
if 'tri_df' in locals() and tri_df is not None and not tri_df.empty:
    pattern_tables.append(tri_df)
if 'double_patterns' in locals() and double_patterns:
    double_df = pd.DataFrame(double_patterns)
    if not double_df.empty:
        pattern_tables.append(double_df)

# ãƒ•ãƒ©ãƒƒã‚°/ãƒšãƒŠãƒ³ãƒˆ


if patterns:
    all_patterns_df = pd.DataFrame(patterns)
    st.dataframe(all_patterns_df.style.format({
        "width_mean":"{:.3f}", "width_std":"{:.3f}", "width_stability":"{:.2f}",
        "quality_score":"{:.2f}",
        "entry":"{:.3f}", "stop":"{:.3f}", "target":"{:.3f}",
    }, na_rep="-"))

# ---- ã™ã¹ã¦ã®ãƒãƒ£ãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³è¡¨ã‚’çµåˆã—ã¦è¡¨ç¤º ----
import pandas as pd
pattern_tables = []
if 'rect_df' in locals() and rect_df is not None and not rect_df.empty:
    pattern_tables.append(rect_df)
if 'tri_df' in locals() and tri_df is not None and not tri_df.empty:
    pattern_tables.append(tri_df)
if 'double_patterns' in locals() and double_patterns:
    double_df = pd.DataFrame(double_patterns)
    if not double_df.empty:
        pattern_tables.append(double_df)

if pattern_tables:
    all_patterns_df = pd.concat(pattern_tables, ignore_index=True, sort=False)
    st.dataframe(all_patterns_df.style.format({
        "width_mean":"{:.3f}", "width_std":"{:.3f}", "width_stability":"{:.2f}",
        "quality_score":"{:.2f}",
        "entry":"{:.3f}", "stop":"{:.3f}", "target":"{:.3f}",
    }, na_rep="-"))

# ---------------- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆTTLä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ ----------------
@st.cache_resource(show_spinner=False, ttl=600)
def _load_break_model_cached(path: str):
    return joblib.load(path)

def load_break_model(path: str):
    try:
        obj = _load_break_model_cached(path)
        return obj["model"], obj.get("Xcols", None), obj.get("meta", {})
    except Exception:
        return None, None, None

def session_onehot_feat(ts):
    h = ts.hour
    return (1.0 if 9<=h<15 else 0.0,
            1.0 if 16<=h<24 else 0.0,
            1.0 if h>=22 or h<5 else 0.0)

def make_features_for_level(df, ts, level, dir_sign, touch_buffer, trend_look=150):
    feat = {}
    features_list = [
        "ret_1", "ret_4", "atr", "touch_density", "slope_long", "tokyo", "london", "ny", "ret_8", "ret_12",
        "atr14_norm", "atr_ratio", "d_atr14", "range_pct", "body_ratio", "wick_up_ratio", "wick_dn_ratio", "slope_short_6",
        "z_close_20", "sin_hour", "cos_hour", "touch_x_atr", "slope_long_x_ny", "ret_1_v", "ret_4_v", "ret_8_v", "ret_12_v",
        "range_pct_v", "slope_short_6_v", "atr14_norm_v", "atr_ratio_56", "rv20", "reg_atr_low", "reg_atr_mid", "reg_atr_high",
        "tokyo_x_range", "tokyo_x_touch", "london_x_range", "london_x_touch", "ny_x_range", "ny_x_touch", "touch_x_atr_v",
        "touch_x_slope_v", "z_x_atr_v", "reg_low_x_z", "reg_low_x_slope_v", "reg_mid_x_z", "reg_mid_x_slope_v", "reg_high_x_z",
        "reg_high_x_slope_v", "z_close_20_sq", "slope_short_6_v_sq", "ret_8_v_sq", "ret_12_v_sq", "dir", "dist_to_level",
        "atr_slope_dir", "rsi_div_dir", "high_low_ratio_20", "atr_change_10"
    ]
    print(f"CALL make_features_for_level: df={df.shape}, ts={ts}, level={level}, dir_sign={dir_sign}")
    print(f"features_list={features_list}")
    filtered_feat = {k: feat.get(k, 0.0) for k in features_list}
    print(f"filtered_feat keys={list(filtered_feat.keys())}, len={len(filtered_feat)}")
    with open("filtered_feat_debug.txt", "a", encoding="utf-8") as dbg:
        dbg.write(f"CALL make_features_for_level: df={getattr(df, 'shape', None)}, ts={ts}, level={level}, dir_sign={dir_sign}\n")
    feat = {}
    i = len(df)-1
    c  = float(df["close"].iloc[i])
    h  = float(df["high"].iloc[i])
    l  = float(df["low"].iloc[i])
    feat["dir"] = dir_sign
    feat["dist_to_level"] = (c - level) * dir_sign
    feat["diff_level_close"] = (c - level) * dir_sign
    feat["recent_high"] = float(df["high"].iloc[max(0, i-10):i+1].max()) * dir_sign
    feat["recent_low"] = float(df["low"].iloc[max(0, i-10):i+1].min()) * dir_sign
    feat["above_level"] = 1.0 if c > level else 0.0
    feat["below_level"] = 1.0 if c < level else 0.0
    a = atr(df, 14).fillna(0.0).iloc[i]
    feat["atr_norm"] = a / max(1e-6, c)
    feat["atr_slope_dir"] = (atr(df, 14).diff().fillna(0.0).iloc[i]) * dir_sign
    if "rsi" in df.columns:
        feat["rsi_div_dir"] = (df["rsi"].diff().fillna(0.0).iloc[i]) * dir_sign
    else:
        feat["rsi_div_dir"] = 0.0
    dist = abs(c - level)
    feat["near"] = 1.0 / (dist + 1e-6)
    Ntouch=200
    sub = df.iloc[max(0, i-Ntouch):i+1]
    touches = int((((sub["Low"]<=level)&(sub["High"]>=level)) | (sub["Close"].sub(level).abs()<=touch_buffer)).sum()) if "Low" in df.columns else int((((sub["low"]<=level)&(sub["high"]>=level)) | (sub["close"].sub(level).abs()<=touch_buffer)).sum())
    feat["touches"] = touches
    sess_tokyo, sess_london, sess_ny = session_onehot_feat(ts)
    feat["tokyo"] = sess_tokyo
    feat["london"] = sess_london
    feat["ny"] = sess_ny
    # --- ä¸è¶³ç‰¹å¾´é‡ã®è¿½åŠ  ---
    # z_close_20_sq: z_close_20ã®2ä¹—
    if "z_close_20" in feat:
        feat["z_close_20_sq"] = feat["z_close_20"] ** 2
    else:
        feat["z_close_20_sq"] = 0.0
    # slope_short_6_v_sq: slope_short_6_vã®2ä¹—
    if "slope_short_6_v" in feat:
        feat["slope_short_6_v_sq"] = feat["slope_short_6_v"] ** 2
    else:
        feat["slope_short_6_v_sq"] = 0.0
    # ret_8_v_sq: ret_8_vã®2ä¹—
    if "ret_8_v" in feat:
        feat["ret_8_v_sq"] = feat["ret_8_v"] ** 2
    else:
        feat["ret_8_v_sq"] = 0.0
    # ret_12_v_sq: ret_12_vã®2ä¹—
    if "ret_12_v" in feat:
        feat["ret_12_v_sq"] = feat["ret_12_v"] ** 2
    else:
        feat["ret_12_v_sq"] = 0.0
    # high_low_ratio_20: ç›´è¿‘20æœ¬ã®high/lowæ¯”
    if i >= 19:
        high20 = float(df["high"].iloc[i-19:i+1].max())
        low20 = float(df["low"].iloc[i-19:i+1].min())
        feat["high_low_ratio_20"] = high20 / max(low20, 1e-6)
    else:
        feat["high_low_ratio_20"] = 0.0
    # atr_change_10: ATRã®10æœ¬å¤‰åŒ–é‡
    atr_series = atr(df, 14).fillna(0.0)
    if i >= 10:
        feat["atr_change_10"] = atr_series.iloc[i] - atr_series.iloc[i-10]
    else:
        feat["atr_change_10"] = 0.0
    # --- å¿…é ˆç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆæ¼ã‚Œé˜²æ­¢ï¼‰ ---
    for k in ["z_close_20_sq", "slope_short_6_v_sq", "ret_8_v_sq", "ret_12_v_sq", "dir", "dist_to_level", "atr_slope_dir", "rsi_div_dir", "high_low_ratio_20", "atr_change_10"]:
        if k not in feat:
            feat[k] = 0.0
    # rsi_div_dir
    if "rsi_div_dir" not in feat:
        feat["rsi_div_dir"] = 0.0
    # high_low_ratio_20
    if "high_low_ratio_20" not in feat:
        feat["high_low_ratio_20"] = 0.0
    # atr_change_10
    if "atr_change_10" not in feat:
        feat["atr_change_10"] = 0.0
    feat = {}
    # ...æ—¢å­˜ã®ç‰¹å¾´é‡ç”Ÿæˆå‡¦ç†...
    # ã“ã“ã«åŸºæœ¬ç‰¹å¾´é‡ã®ä»£å…¥ãŒç¶šã
    feat["dir"] = dir_sign
    feat["dist_to_level"] = (float(df["close"].iloc[len(df)-1]) - level) * dir_sign
    # ...ï¼ˆä»–ã®ç‰¹å¾´é‡ç”Ÿæˆå‡¦ç†ï¼‰...

    # --- ä¸è¶³ç‰¹å¾´é‡ã®è¿½åŠ  ---
    # z_close_20_sq: z_close_20ã®2ä¹—
    if "z_close_20" in feat:
        feat["z_close_20_sq"] = feat["z_close_20"] ** 2
    else:
        feat["z_close_20_sq"] = 0.0

    # slope_short_6_v_sq: slope_short_6_vã®2ä¹—
    if "slope_short_6_v" in feat:
        feat["slope_short_6_v_sq"] = feat["slope_short_6_v"] ** 2
    else:
        feat["slope_short_6_v_sq"] = 0.0

    # ret_8_v_sq: ret_8_vã®2ä¹—
    if "ret_8_v" in feat:
        feat["ret_8_v_sq"] = feat["ret_8_v"] ** 2
    else:
        feat["ret_8_v_sq"] = 0.0

    # ret_12_v_sq: ret_12_vã®2ä¹—
    if "ret_12_v" in feat:
        feat["ret_12_v_sq"] = feat["ret_12_v"] ** 2
    else:
        feat["ret_12_v_sq"] = 0.0

    # high_low_ratio_20: ç›´è¿‘20æœ¬ã®high/lowæ¯”
    i = len(df)-1
    if i >= 19:
        high20 = float(df["high"].iloc[i-19:i+1].max())
        low20 = float(df["low"].iloc[i-19:i+1].min())
        feat["high_low_ratio_20"] = high20 / max(low20, 1e-6)
    else:
        feat["high_low_ratio_20"] = 0.0

    # atr_change_10: ATRã®10æœ¬å¤‰åŒ–é‡
    atr_series = atr(df, 14).fillna(0.0)
    if i >= 10:
        feat["atr_change_10"] = atr_series.iloc[i] - atr_series.iloc[i-10]
    else:
        feat["atr_change_10"] = 0.0
    features_list = [
        "ret_1", "ret_4", "atr", "touch_density", "slope_long", "tokyo", "london", "ny", "ret_8", "ret_12",
        "atr14_norm", "atr_ratio", "d_atr14", "range_pct", "body_ratio", "wick_up_ratio", "wick_dn_ratio", "slope_short_6",
        "z_close_20", "sin_hour", "cos_hour", "touch_x_atr", "slope_long_x_ny", "ret_1_v", "ret_4_v", "ret_8_v", "ret_12_v",
        "range_pct_v", "slope_short_6_v", "atr14_norm_v", "atr_ratio_56", "rv20", "reg_atr_low", "reg_atr_mid", "reg_atr_high",
        "tokyo_x_range", "tokyo_x_touch", "london_x_range", "london_x_touch", "ny_x_range", "ny_x_touch", "touch_x_atr_v",
        "touch_x_slope_v", "z_x_atr_v", "reg_low_x_z", "reg_low_x_slope_v", "reg_mid_x_z", "reg_mid_x_slope_v", "reg_high_x_z",
        "reg_high_x_slope_v", "z_close_20_sq", "slope_short_6_v_sq", "ret_8_v_sq", "ret_12_v_sq", "dir", "dist_to_level",
        "atr_slope_dir", "rsi_div_dir", "high_low_ratio_20", "atr_change_10"
    ]
    # å¿…è¦ãªç‰¹å¾´é‡ã®ã¿æŠ½å‡ºã—ã€0åŸ‹ã‚ï¼ˆå¿…ãš60å€‹ã®keyã‚’æŒã¤dictã‚’è¿”ã™ï¼‰
    filtered_feat = {k: feat.get(k, 0.0) for k in features_list}
    debug_path = r"c:\Users\daiki\OneDrive\fx2\FX-Learning-Tools\filtered_feat_debug.txt"
    try:
        with open(debug_path, "a", encoding="utf-8") as dbg:
            dbg.write(f"CALL make_features_for_level: df={df.shape}, ts={ts}, level={level}, dir_sign={dir_sign}\n")
            dbg.write(f"features_list={features_list}\n")
            dbg.write(f"filtered_feat keys={list(filtered_feat.keys())}, len={len(filtered_feat)}\n")
    except Exception as e:
        with open(debug_path, "a", encoding="utf-8") as dbg:
            dbg.write(f"EXCEPTION in make_features_for_level: {e}\n")
    # ãƒ¬ãƒ™ãƒ«ã«æœ€ã‚‚è¿‘ã„ãƒãƒ¼ã‚’æ¢ã™
    i = len(df)-1  # ç›´è¿‘ãƒãƒ¼
    # ã‚«ãƒ©ãƒ åã‚’å­¦ç¿’æ™‚ã¨æƒãˆã‚‹
    c  = float(df["close"].iloc[i])
    h  = float(df["high"].iloc[i])
    l  = float(df["low"].iloc[i])
    # æ–¹å‘æ€§ç‰¹å¾´é‡
    feat = {}
    feat["dir"] = dir_sign
    feat["dist_to_level"] = (c - level) * dir_sign
    feat["diff_level_close"] = (c - level) * dir_sign
    feat["recent_high"] = float(df["high"].iloc[max(0, i-10):i+1].max()) * dir_sign
    feat["recent_low"] = float(df["low"].iloc[max(0, i-10):i+1].min()) * dir_sign
    feat["above_level"] = 1.0 if c > level else 0.0
    feat["below_level"] = 1.0 if c < level else 0.0
    a = atr(df, 14).fillna(0.0).iloc[i]
    feat["atr_norm"] = a / max(1e-6, c)
    feat["atr_slope_dir"] = (atr(df, 14).diff().fillna(0.0).iloc[i]) * dir_sign
    # RSIãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ä¾‹ï¼ˆä»®ï¼‰
    if "rsi" in df.columns:
        feat["rsi_div_dir"] = (df["rsi"].diff().fillna(0.0).iloc[i]) * dir_sign
    else:
        feat["rsi_div_dir"] = 0.0
    dist     = abs(c - level)
    feat["near"] = 1.0 / (dist + 1e-6)
    Ntouch=200
    sub = df.iloc[max(0, i-Ntouch):i+1]
    touches = int((((sub["Low"]<=level)&(sub["High"]>=level)) | (sub["Close"].sub(level).abs()<=touch_buffer)).sum()) if "Low" in df.columns else int((((sub["low"]<=level)&(sub["high"]>=level)) | (sub["close"].sub(level).abs()<=touch_buffer)).sum())
    feat["touches"] = touches
    sess_tokyo, sess_london, sess_ny = session_onehot_feat(ts)
    feat["tokyo"] = sess_tokyo
    feat["london"] = sess_london
    feat["ny"] = sess_ny
    # meta["features"]ã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä½™è¨ˆãªã‚«ãƒ©ãƒ é™¤å¤–ï¼‰
    import json
    with open("models/break_meta.json", "r", encoding="utf-8") as f:
        meta_json = json.load(f)
        features_list = meta_json.get("features", list(feat.keys()))
    # å¿…è¦ãªç‰¹å¾´é‡ã®ã¿æŠ½å‡ºã—ã€0åŸ‹ã‚ï¼ˆå¿…ãš60å€‹ã®keyã‚’æŒã¤dictã‚’è¿”ã™ï¼‰
    filtered_feat = {k: feat.get(k, 0.0) for k in features_list}
    filtered_feat["timestamp"] = ts
    print(f"filtered_feat keys={list(filtered_feat.keys())}, len={len(filtered_feat)}")
    return filtered_feat

    # ãƒ†ã‚¹ãƒˆç”¨: make_features_for_levelã®ç›´æ¥å‘¼ã³å‡ºã—
    if __name__ == "__main__":
        import pandas as pd
        from datetime import datetime
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        df = pd.DataFrame({
            "close": [100, 101, 102, 103, 104],
            "high": [101, 102, 103, 104, 105],
            "low": [99, 100, 101, 102, 103],
            "open": [100, 100, 101, 102, 103],
        }, index=pd.date_range("2025-09-01", periods=5, freq="D"))
        ts = df.index[-1]
        level = 102
        dir_sign = 1
        touch_buffer = 0.5
        make_features_for_level(df, ts, level, dir_sign, touch_buffer)

# ====================== ãƒãƒ£ãƒ¼ãƒˆæç”» ======================
fig = go.Figure()
fig.add_trace(go.Candlestick(
    x=df.index,
    open=df["open"], high=df["high"], low=df["low"], close=df["close"],
    name="Price",
    increasing_line_color=COLOR_CANDLE_UP_EDGE, increasing_fillcolor=COLOR_CANDLE_UP_BODY,
    decreasing_line_color=COLOR_CANDLE_DN_EDGE, decreasing_fillcolor=COLOR_CANDLE_DN_BODY
))
## --- ãƒˆãƒ¬ãƒ¼ãƒ‰ãƒã‚¤ãƒ³ãƒˆæç”» ---
for pt in st.session_state.get("trade_points", []):
    if pt["type"] == "buy":
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["price"]-0.1, y1=pt["price"]+0.1,
                      line=dict(color="blue", width=3))
        fig.add_annotation(x=pt["time"], y=pt["price"], text="Buy", showarrow=True, arrowhead=2, font=dict(color="blue"))
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["tp"]-0.1, y1=pt["tp"]+0.1,
                      line=dict(color="green", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["tp"], text="TP", showarrow=True, arrowhead=1, font=dict(color="green"))
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["sl"]-0.1, y1=pt["sl"]+0.1,
                      line=dict(color="red", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["sl"], text="SL", showarrow=True, arrowhead=1, font=dict(color="red"))
    elif pt["type"] == "sell":
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["price"]-0.1, y1=pt["price"]+0.1,
                      line=dict(color="orange", width=3))
        fig.add_annotation(x=pt["time"], y=pt["price"], text="Sell", showarrow=True, arrowhead=2, font=dict(color="orange"))
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["tp"]-0.1, y1=pt["tp"]+0.1,
                      line=dict(color="green", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["tp"], text="TP", showarrow=True, arrowhead=1, font=dict(color="green"))
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["sl"]-0.1, y1=pt["sl"]+0.1,
                      line=dict(color="red", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["sl"], text="SL", showarrow=True, arrowhead=1, font=dict(color="red"))
    elif pt["type"] == "tp":
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["price"]-0.1, y1=pt["price"]+0.1,
                      line=dict(color="green", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["price"], text="TP", showarrow=True, arrowhead=1, font=dict(color="green"))
    elif pt["type"] == "sl":
        fig.add_shape(type="line", x0=pt["time"], x1=pt["time"], y0=pt["price"]-0.1, y1=pt["price"]+0.1,
                      line=dict(color="red", width=2, dash="dot"))
        fig.add_annotation(x=pt["time"], y=pt["price"], text="SL", showarrow=True, arrowhead=1, font=dict(color="red"))

# æ°´å¹³ç·š
score_df = compute_level_scores(df, levels, touch_buffer, w_touch, w_recent, w_session, w_vol)
if not score_df.empty:
    sc_min, sc_max = score_df["score"].min(), score_df["score"].max()
    rng = max(1e-9, (sc_max - sc_min))
    for _, r in score_df.iterrows():
        # ã‚¹ã‚³ã‚¢ãŒå…¨ã¦åŒã˜å ´åˆã¯å›ºå®šå€¤
        if math.isclose(sc_min, sc_max):
            alpha = 0.25
        else:
            alpha = 0.20 + 0.65 * (r["score"]-sc_min)/rng
        # alphaã®ç¯„å›²ã‚’ä¿è¨¼
        alpha = min(max(alpha, 0), 1)
        fig.add_hline(
            y=r["level"],
            opacity=float(alpha),
            line_color=COLOR_LEVEL,
            line_width=1.5
        )
else:
    for lv in levels:
        fig.add_hline(
            y=lv,
            opacity=0.35,
            line_color=COLOR_LEVEL,
            line_width=1.2
        )

# ãƒˆãƒ¬ãƒ³ãƒ‰ï¼†ãƒãƒ£ãƒãƒ«
trend = regression_trend(df, reg_lookback, use="low")
if trend:
    fig.add_shape(type="line", x0=trend["x0"], y0=trend["y0"], x1=trend["x1"], y1=trend["y1"],
                  line=dict(width=2.4, color=COLOR_TREND))
    if trend["sigma"] > 0:
        x0, x1 = trend["x0"], trend["x1"]
        y0_u = trend["y0"] + chan_k*trend["sigma"]; y1_u = trend["y1"] + chan_k*trend["sigma"]
        y0_l = trend["y0"] - chan_k*trend["sigma"]; y1_l = trend["y1"] - chan_k*trend["sigma"]
        fig.add_shape(type="line", x0=x0, y0=y0_u, x1=x1, y1=y1_u,
                      line=dict(width=1.6, color=COLOR_CH_UP, dash="dot"))
        fig.add_shape(type="line", x0=x0, y0=y0_l, x1=x1, y1=y1_l,
                      line=dict(width=1.6, color=COLOR_CH_DN, dash="dot"))

# ãƒ‘ã‚¿ãƒ¼ãƒ³æç”»

def _line_points(i1, i2, slope, intercept):
    xs = np.array([i1, i2], dtype=float)
    ys = slope * xs + intercept
    return xs.astype(int), ys

if 'tri_df' in locals():
    for _, r in tri_df.iterrows():
        s, e = int(r["start_idx"]), int(r["end_idx"])
        xs_u, ys_u = _line_points(s, e, r["upper_slope"], r["upper_intercept"])
        xs_l, ys_l = _line_points(s, e, r["lower_slope"], r["lower_intercept"])
        x_u = df.index[xs_u] if "time" not in df.columns else df["time"].iloc[xs_u]
        x_l = df.index[xs_l] if "time" not in df.columns else df["time"].iloc[xs_l]
        fig.add_scatter(x=x_u, y=ys_u, mode="lines", name=f"{r['type']} upper (q={r['quality_score']:.2f})", opacity=0.7)
        fig.add_scatter(x=x_l, y=ys_l, mode="lines", name=f"{r['type']} lower", opacity=0.7)
        fig.add_hline(y=r["entry"],  line_dash="dot", annotation_text="entry")
        fig.add_hline(y=r["stop"],   line_dash="dot", annotation_text="stop")
        fig.add_hline(y=r["target"], line_dash="dot", annotation_text="target")

if 'rect_df' in locals():
    def _line_points(i1, i2, slope, intercept):
        xs = np.array([i1, i2], dtype=float)
        ys = slope * xs + intercept
        return xs.astype(int), ys

    for _, r in rect_df.iterrows():
        s, e = int(r["start_idx"]), int(r["end_idx"])
        xs_u, ys_u = _line_points(s, e, r["upper_slope"], r["upper_intercept"])
        xs_l, ys_l = _line_points(s, e, r["lower_slope"], r["lower_intercept"])

        x_u = df.index[xs_u] if "time" not in df.columns else df["time"].iloc[xs_u]
        x_l = df.index[xs_l] if "time" not in df.columns else df["time"].iloc[xs_l]

        fig.add_scatter(x=x_u, y=ys_u, mode="lines", name=f"rect upper (q={r['quality_score']:.2f})", opacity=0.7)
        fig.add_scatter(x=x_l, y=ys_l, mode="lines", name="rect lower", opacity=0.7)

        fig.add_hline(y=r["entry"],  line_dash="dot", annotation_text="entry")
        fig.add_hline(y=r["stop"],   line_dash="dot", annotation_text="stop")
        fig.add_hline(y=r["target"], line_dash="dot", annotation_text="target")

def _draw_rectangle(fig, p: Pattern):
    sub = df.loc[p.params["sub_start"] : p.params["sub_end"]]
    up = p.params["upper"]; dn = p.params["lower"]
    fig.add_hrect(y0=dn, y1=up, x0=sub.index[0], x1=sub.index[-1],
                  line_width=0, fillcolor="rgba(67,160,71,0.12)")
    fig.add_hline(y=up, line=dict(color=COLOR_RECTANGLE, width=2, dash="dot"))
    fig.add_hline(y=dn, line=dict(color=COLOR_RECTANGLE, width=2, dash="dot"))

def _draw_double(fig, p: Pattern):
    if p.kind=="double_top":
        top = p.params["top"]; neck = p.params["neck"]
        x1 = p.t_start; x2 = p.t_end
        # 2ã¤ã®é«˜å€¤ã‚’ç·šã§çµã¶
        fig.add_trace(go.Scatter(x=[x1, x2], y=[top, top], mode="lines+markers+text",
                                 line=dict(color=COLOR_DOUBLE_TOP, width=2),
                                 marker=dict(color=COLOR_DOUBLE_TOP, size=10),
                                 text=["Top1", "Top2"], textposition="top center", showlegend=False))
        # ãƒãƒƒã‚¯ãƒ©ã‚¤ãƒ³ï¼ˆãƒ”ãƒ³ã‚¯ç‚¹ç·šï¼‰
        fig.add_trace(go.Scatter(x=[x1, x2], y=[neck, neck], mode="lines",
                                 line=dict(color=COLOR_DOUBLE_TOP, width=2, dash="dot"), showlegend=False))
        # ãƒãƒƒã‚¯ãƒ©ã‚¤ãƒ³æ³¨é‡ˆ
        fig.add_annotation(x=x1, y=neck, text="NeckL", showarrow=True, arrowhead=1, ax=0, ay=30, font=dict(color=COLOR_DOUBLE_TOP))
    elif p.kind=="double_bottom":
        bot = p.params["bottom"]; neck = p.params["neck"]
        x1 = p.t_start; x2 = p.t_end
        # 2ã¤ã®å®‰å€¤ã‚’ç·šã§çµã¶
        fig.add_trace(go.Scatter(x=[x1, x2], y=[bot, bot], mode="lines+markers+text",
                                 line=dict(color=COLOR_DOUBLE_BOTTOM, width=2),
                                 marker=dict(color=COLOR_DOUBLE_BOTTOM, size=10),
                                 text=["Bottom1", "Bottom2"], textposition="bottom center", showlegend=False))
        # ãƒãƒƒã‚¯ãƒ©ã‚¤ãƒ³ï¼ˆé’ç‚¹ç·šï¼‰
        fig.add_trace(go.Scatter(x=[x1, x2], y=[neck, neck], mode="lines",
                                 line=dict(color=COLOR_DOUBLE_BOTTOM, width=2, dash="dot"), showlegend=False))
        # ãƒãƒƒã‚¯ãƒ©ã‚¤ãƒ³æ³¨é‡ˆ
        fig.add_annotation(x=x1, y=neck, text="NeckL", showarrow=True, arrowhead=1, ax=0, ay=-30, font=dict(color=COLOR_DOUBLE_BOTTOM))

def _draw_flag(fig, p: Pattern):
    params = p['params'] if isinstance(p, dict) else getattr(p, 'params', {})
    sub = df.loc[params["sub_start"] : params["sub_end"]]
    x = np.arange(len(sub))
    m = params["slope"]; b = params["intercept"]; s = params["sigma"]; k = params["band_k"]
    y_mid = m*x + b
    y_up = y_mid + k*s
    y_dn = y_mid - k*s
    fig.add_trace(go.Scatter(x=sub.index, y=y_up, mode="lines", name="flag_up",
                             line=dict(color=COLOR_FLAG, width=2, dash="dot"), showlegend=False))
    fig.add_trace(go.Scatter(x=sub.index, y=y_dn, mode="lines", name="flag_dn",
                             line=dict(color=COLOR_FLAG, width=2, dash="dot"), showlegend=False))

def _draw_hs(fig, p: Pattern):
    neck = p.params.get("neck", None)
    if neck is not None:
        fig.add_hline(y=float(neck), line=dict(color=COLOR_HS, width=2, dash="dash"))
    # å·¦è‚©ãƒ»é ­ãƒ»å³è‚©ã®ä½ç½®ã«ãƒãƒ¼ã‚«ãƒ¼ã¨ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
    left_shoulder_x = p.params.get("left_shoulder_x")
    left_shoulder_y = p.params.get("left_shoulder_y")
    head_x = p.params.get("head_x")
    head_y = p.params.get("head")
    right_shoulder_x = p.params.get("right_shoulder_x")
    right_shoulder_y = p.params.get("right_shoulder_y")
    # ãƒãƒ¼ã‚«ãƒ¼æç”»ï¼ˆå€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    if left_shoulder_x is not None and left_shoulder_y is not None:
        fig.add_trace(go.Scatter(x=[left_shoulder_x], y=[left_shoulder_y], mode="markers+text",
            marker=dict(color=COLOR_HS, size=12, symbol="circle"),
            text=["å·¦è‚©"], textposition="top center", showlegend=False))
    if head_x is not None and head_y is not None:
        fig.add_trace(go.Scatter(x=[head_x], y=[head_y], mode="markers+text",
            marker=dict(color=COLOR_HS, size=14, symbol="diamond"),
            text=["é ­"], textposition="top center", showlegend=False))
    if right_shoulder_x is not None and right_shoulder_y is not None:
        fig.add_trace(go.Scatter(x=[right_shoulder_x], y=[right_shoulder_y], mode="markers+text",
            marker=dict(color=COLOR_HS, size=12, symbol="circle"),
            text=["å³è‚©"], textposition="top center", showlegend=False))
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åæ³¨é‡ˆ
    fig.add_annotation(x=p.t_end, y=p.params.get("head", float(df['close'].iloc[-1])),
                       text=("H&S" if p.kind=="head_shoulders" else "Inv H&S"),
                       showarrow=False, font=dict(color=COLOR_HS))

for p in patterns:
    kind = p.get('kind') if isinstance(p, dict) else getattr(p, 'kind', None)
    # ãƒˆãƒ©ã‚¤ã‚¢ãƒ³ã‚°ãƒ«ã¯tri_dfã§æç”»æ¸ˆã¿ãªã®ã§ã“ã“ã§ã¯æç”»ã—ãªã„
    if kind=="rectangle":
        _draw_rectangle(fig, p)
    elif kind in ("double_top","double_bottom"):
        _draw_double(fig, p)
    elif kind in ("flag_up","flag_dn","pennant"):
        _draw_flag(fig, p)
    elif kind in ("head_shoulders","inverse_head_shoulders"):
        _draw_hs(fig, p)

# ---- èµ¤å½±ï¼ˆé‡è¦åº¦åˆ¥ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰ã‚’é‡ã­ã‚‹ ----
if use_news_shade and not windows_df.empty:
    fig = add_news_shading_to_fig(fig, windows_df)

# --- ç–‘ä¼¼ãƒãƒ£ãƒ¼ãƒˆæŠ•å½±ï¼ˆã‚´ãƒ¼ã‚¹ãƒˆï¼‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« ---
st.sidebar.markdown("---")
st.sidebar.subheader("ç–‘ä¼¼ãƒãƒ£ãƒ¼ãƒˆæŠ•å½±ï¼ˆã‚´ãƒ¼ã‚¹ãƒˆï¼‰")
enable_ghost = st.sidebar.checkbox("ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å°†æ¥ãƒ‘ã‚¹ã‚’è–„ãé‡ã­ã‚‹", value=False)
ghost_h = st.sidebar.slider("æŠ•å½±æœ¬æ•°ï¼ˆãƒãƒ¼ï¼‰", 10, 120, 40, help="ä½•æœ¬å…ˆã¾ã§è–„ãæãã‹")
ghost_mode = st.sidebar.selectbox("æ–¹æ³•", ["EVç›´ç·š", "ãƒœãƒ©æ‰‡å½¢(å¹³å‡)"], index=0)
ghost_alpha = st.sidebar.slider("é€æ˜åº¦", 0.05, 0.6, 0.18)
ghost_fan_k = st.sidebar.slider("æ‰‡å½¢ã®å¹…kï¼ˆÏƒä¿‚æ•°ï¼‰", 0.5, 3.0, 1.5, 0.5)
ghost_sims = st.sidebar.slider("ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯æœ¬æ•°ï¼ˆä»»æ„ï¼‰", 0, 300, 0)

# --- ã‚´ãƒ¼ã‚¹ãƒˆã®ãŸã‚ã®è£œåŠ©é–¢æ•° ---
def _pattern_levels_for_prob(df, p: Pattern):
    upper_level = None; lower_level = None
    kind = p.get('kind') if isinstance(p, dict) else getattr(p, 'kind', None)
    if kind and kind.startswith("triangle"):
        subp = df.loc[p.params["sub_start"]:p.params["sub_end"]]
        m1,b1 = p.params["upper"]; m2,b2 = p.params["lower"]
        x_now  = len(subp)-1
        y_u = float(m1*x_now + b1); y_l = float(m2*x_now + b2)
        upper_level, lower_level = y_u, y_l
    elif kind=="rectangle":
        upper_level, lower_level = float(p.params["upper"]), float(p.params["lower"])
    elif kind=="double_top":
        upper_level, lower_level = None, float(p.params["neck"])
    elif kind=="double_bottom":
        upper_level, lower_level = float(p.params["neck"]), None
    elif kind in ("flag_up","flag_dn","pennant"):
        upper_level, lower_level = float(p.params["upper_now"]), float(p.params["lower_now"])
    elif kind=="head_shoulders":
        upper_level, lower_level = None, float(p.params["neck"])
    elif kind=="inverse_head_shoulders":
        upper_level, lower_level = float(p.params["neck"]), None

    return upper_level, lower_level

def _pick_best_pattern(patterns: list[Pattern]) -> Pattern | None:
    if not patterns: return None
    pats = sorted(patterns, key=lambda x: (
        x.get('quality') if isinstance(x, dict) else getattr(x, 'quality', None),
        x.get('t_end') if isinstance(x, dict) else getattr(x, 't_end', None)
    ), reverse=True)
    return pats[0]

def _ghost_path_ev(df, P_up, P_dn, tgt_up_px, tgt_dn_px, h: int):
    w_up = float(P_up) if not np.isnan(P_up) else 0.5
    w_dn = float(P_dn) if not np.isnan(P_dn) else 0.5
    if (w_up + w_dn) <= 1e-9:
        w_up = w_dn = 0.5
    else:
        s = w_up + w_dn
        w_up /= s; w_dn /= s
    c0 = float(df["close"].iloc[-1])
    xs = np.arange(h+1)
    up_line = c0 + (tgt_up_px - c0) * (xs / h)
    dn_line = c0 + (tgt_dn_px - c0) * (xs / h)
    y = w_up*up_line + w_dn*dn_line
    return xs, y

def _ghost_cone_from_atr(df, h: int, k: float):
    c0 = float(df["close"].iloc[-1])
    _atr = atr(df, 14).iloc[-1] if len(df)>=14 else (df["high"].iloc[-1]-df["low"].iloc[-1])
    xs = np.arange(h+1)
    sigma = float(_atr)
    spread = k * sigma * np.sqrt(np.maximum(0, xs))
    mid = np.full_like(xs, c0, dtype=float)
    return xs, mid, mid+spread, mid-spread

def _rand_walks(df, h: int, n: int, drift_line: np.ndarray | None):
    if n <= 0: return []
    c0 = float(df["close"].iloc[-1])
    _atr = atr(df, 14).iloc[-1] if len(df)>=14 else (df["high"].iloc[-1]-df["low"].iloc[-1])
    sigma = float(_atr) * 0.6
    paths=[]
    for _ in range(n):
        steps = np.random.normal(loc=0.0, scale=sigma, size=h)
        y = np.r_[c0, c0 + np.cumsum(steps)]
        if drift_line is not None:
            y = 0.7*y + 0.3*drift_line
        paths.append(y)
    return paths

# === ã‚´ãƒ¼ã‚¹ãƒˆæŠ•å½±ï¼ˆç–‘ä¼¼ãƒãƒ£ãƒ¼ãƒˆï¼‰ ===
if enable_ghost:
    try:
        pat = _pick_best_pattern(patterns)
        c0 = float(df["close"].iloc[-1])
        tgt_up_px = c0; tgt_dn_px = c0
        P_up = P_dn = np.nan

        if pat is not None:
            meas = measured_targets(pat)
            tgt_up_px = c0 + float(meas.get("up", 0.0))
            tgt_dn_px = c0 + float(meas.get("down", 0.0))

            model, use_cols, meta = _load_model_and_meta()
            upper_level, lower_level = _pattern_levels_for_prob(df, pat)
            ts_now = df.index[-1]
            try:
                in_news = bool(is_in_any_window(pd.Series([ts_now]), windows_df[["start","end"]]).iloc[0])
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é‡è¦åº¦/Â±åˆ†æ–¹å¼ã®é–¢æ•°ãŒã‚ã‚‹å ´åˆã¯ãã¡ã‚‰ã§ã‚‚å¯
                in_news = False
            if upper_level is not None:
                feat_up = make_features_for_level(df, ts_now, upper_level, +1, touch_buffer)
                if isinstance(feat_up, dict):
                    row_up = {k: feat_up.get(k, 0.0) for k in use_cols}
                elif isinstance(feat_up, list):
                    row_up = dict(zip(use_cols, feat_up))
                else:
                    raise TypeError("feat_up must be dict or list")
                row_up["timestamp"] = ts_now
                df_up = pd.DataFrame([row_up])
                for col in use_cols:
                    if col not in df_up.columns:
                        df_up[col] = 0.0
                df_up = df_up[use_cols]
                df_up["timestamp"] = ts_now
                # A. å…¥åŠ›ç¢ºèª
                print("df_up input:", df_up[use_cols].to_dict(orient="records")[0])
                # B. predict_proba shapeç¢ºèª
                proba_up = model.predict_proba(df_up[use_cols])
                print("predict_proba(df_up) shape:", proba_up.shape)
                print("predict_proba(df_up):", proba_up)
                pred_up = predict_with_session_theta(df_up, model, use_cols, meta)
                P_up = float(pred_up["proba"].iloc[0])
                theta_up = float(pred_up["theta"].iloc[0])
                sess_up = str(pred_up["session"].iloc[0])
            if lower_level is not None:
                feat_dn = make_features_for_level(df, ts_now, lower_level, -1, touch_buffer)
                if isinstance(feat_dn, dict):
                    row_dn = {k: feat_dn.get(k, 0.0) for k in use_cols}
                elif isinstance(feat_dn, list):
                    row_dn = dict(zip(use_cols, feat_dn))
                else:
                    raise TypeError("feat_dn must be dict or list")
                row_dn["timestamp"] = ts_now
                df_dn = pd.DataFrame([row_dn])
                for col in use_cols:
                    if col not in df_dn.columns:
                        df_dn[col] = 0.0
                df_dn = df_dn[use_cols]
                df_dn["timestamp"] = ts_now
                # A. å…¥åŠ›ç¢ºèª
                print("df_dn input:", df_dn[use_cols].to_dict(orient="records")[0])
                # B. predict_proba shapeç¢ºèª
                proba_dn = model.predict_proba(df_dn[use_cols])
                print("predict_proba(df_dn) shape:", proba_dn.shape)
                print("predict_proba(df_dn):", proba_dn)
                pred_dn = predict_with_session_theta(df_dn, model, use_cols, meta)
                P_dn = float(pred_dn["proba"].iloc[0])
                theta_dn = float(pred_dn["theta"].iloc[0])
                sess_dn = str(pred_dn["session"].iloc[0])

        idx0 = df.index[-1]
        inferred = pd.infer_freq(df.index)
        freq = inferred if inferred else "T"
        future_idx = pd.date_range(idx0, periods=ghost_h+1, freq=freq, tz=idx0.tz)

        if ghost_mode == "EVç›´ç·š":
            xs, y = _ghost_path_ev(df, P_up, P_dn, tgt_up_px, tgt_dn_px, ghost_h)
            fig.add_trace(go.Scatter(
                x=future_idx, y=y, mode="lines",
                line=dict(color="rgba(255,255,255,0.9)", width=2, dash="dot"),
                name="ghost(EV)", showlegend=False, opacity=ghost_alpha
            ))

        else:  # ãƒœãƒ©æ‰‡å½¢(å¹³å‡)
            xs, mid, up, dn = _ghost_cone_from_atr(df, ghost_h, ghost_fan_k)
            xs_ev, y_ev = _ghost_path_ev(df, P_up, P_dn, tgt_up_px, tgt_dn_px, ghost_h)
            fig.add_trace(go.Scatter(x=future_idx, y=y_ev, mode="lines",
                                     line=dict(color="rgba(255,255,255,0.95)", width=2, dash="dot"),
                                     name="ghost(drift)", showlegend=False, opacity=ghost_alpha))
            x_poly = list(future_idx) + list(future_idx[::-1])
            y_poly = list(up) + list(dn[::-1])
            fig.add_trace(go.Scatter(
                x=x_poly, y=y_poly, fill="toself",
                line=dict(width=0), fillcolor=f"rgba(255,255,255,{ghost_alpha*0.35:.3f})",
                name="ghost(cone)", showlegend=False
            ))
            if ghost_sims and ghost_sims > 0:
                walks = _rand_walks(df, ghost_h, ghost_sims, y_ev)
                for w in walks:
                    fig.add_trace(go.Scatter(
                        x=future_idx, y=w, mode="lines",
                        line=dict(width=1, color="rgba(200,200,200,0.55)"),
                        showlegend=False, opacity=ghost_alpha*0.6
                    ))
    except Exception as e:
        st.warning(f"ã‚´ãƒ¼ã‚¹ãƒˆæŠ•å½±ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# === ãƒãƒ£ãƒ¼ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ & è¡¨ç¤º ===
fig.update_layout(template="plotly_dark", paper_bgcolor=COLOR_BG, plot_bgcolor=COLOR_BG,
                  title=f"{symbol} {interval} - Auto Lines (Dark)", xaxis_rangeslider_visible=False,
                  font=dict(color=COLOR_TEXT, size=12))
fig.update_xaxes(gridcolor=COLOR_GRID, zerolinecolor=COLOR_GRID, showline=True, linecolor=COLOR_GRID)
fig.update_yaxes(gridcolor=COLOR_GRID, zerolinecolor=COLOR_GRID, showline=True, linecolor=COLOR_GRID)

# â˜… ã“ã“ã§ã‚ºãƒ¼ãƒ ä¿æŒã‚’è¿½åŠ 
fig.update_layout(uirevision="fx-live")

# æç”»
st.plotly_chart(fig, use_container_width=True)
st.caption(f"æœ€çµ‚æ›´æ–°: {pd.Timestamp.now(tz=JST).strftime('%Y-%m-%d %H:%M:%S %Z')}")

# ---------------- è¿‘å‚ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ¤å®šï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ï¼‰ ----------------
def near_news(ts: pd.Timestamp) -> bool:
    if news_filter_mode == "é‡è¦åº¦åˆ¥ï¼ˆèµ¤å½±ã¨åŒã˜ï¼‰":
        return is_suppressed(ts, windows_df)
    else:
        if news_df.empty: return False
        win = pd.Timedelta(minutes=news_win)
        cond = (news_df["importance"] >= news_imp_min) & (news_df["time"].between(ts-win, ts+win))
        return bool(cond.any())

# ---------------- ã‚·ã‚°ãƒŠãƒ«ï¼ˆç›´è¿‘ãƒãƒ¼ï¼‰ ----------------
st.subheader("ğŸ“£ ã‚·ã‚°ãƒŠãƒ«ï¼ˆç›´è¿‘ãƒãƒ¼ï¼‰")
i_last = len(df)-1
c_last = float(df["close"].iloc[i_last]); h_last = float(df["high"].iloc[i_last]); l_last = float(df["low"].iloc[i_last])
ts_last = df.index[i_last]

# ã‚½ãƒ•ãƒˆæŠ‘åˆ¶ï¼šçª“å†…ãªã‚‰ãƒ–ãƒ¬ã‚¤ã‚¯ãƒãƒƒãƒ•ã‚¡/K ã‚’å¼·åŒ–
if use_soft_suppress and near_news(ts_last):
    break_buffer = break_buffer_base + soft_break_add
    retest_wait_k = retest_wait_k_base + soft_K_add
else:
    break_buffer = break_buffer_base
    retest_wait_k = retest_wait_k_base

alerts = []
if signal_mode == "æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
    for lv in levels:
        if (c_last > lv + break_buffer) and (l_last <= lv): alerts.append(("ä¸ŠæŠœã‘", f"Lv {lv:.3f}"))
        if (c_last < lv - break_buffer) and (h_last >= lv): alerts.append(("ä¸‹æŠœã‘", f"Lv {lv:.3f}"))
elif signal_mode == "ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
    if trend:
        tl_val = trend["y1"]
        if (c_last > tl_val + break_buffer) and (l_last <= tl_val): alerts.append(("ãƒˆãƒ¬ãƒ³ãƒ‰ä¸ŠæŠœã‘", f"TL {tl_val:.3f}"))
        if (c_last < tl_val - break_buffer) and (h_last >= tl_val): alerts.append(("ãƒˆãƒ¬ãƒ³ãƒ‰ä¸‹æŠœã‘", f"TL {tl_val:.3f}"))
elif signal_mode == "ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘/ä¸‹æŠœã‘(çµ‚å€¤)":
    if trend and trend["sigma"] > 0:
        up = trend["y1"] + chan_k*trend["sigma"]
        dn = trend["y1"] - chan_k*trend["sigma"]
        if c_last > up + break_buffer: alerts.append(("ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘", f"UP {up:.3f}"))
        if c_last < dn - break_buffer: alerts.append(("ãƒãƒ£ãƒãƒ«ä¸‹æŠœã‘", f"DN {dn:.3f}"))
elif signal_mode == "ãƒªãƒ†ã‚¹ãƒˆæŒ‡å€¤(æ°´å¹³ç·š)":
    if i_last >= 1:
        c_prev = float(df["close"].iloc[i_last-1]); l_prev = float(df["low"].iloc[i_last-1]); h_prev = float(df["high"].iloc[i_last-1])
        for lv in levels:
            up_break_prev = (c_prev > lv + break_buffer) and (l_prev <= lv)
            dn_break_prev = (c_prev < lv - break_buffer) and (h_prev >= lv)
            if up_break_prev and abs(c_last - lv) <= touch_buffer: alerts.append(("ãƒªãƒ†ã‚¹ãƒˆè²·ã„å€™è£œ", f"Lv {lv:.3f}"))
            if dn_break_prev and abs(c_last - lv) <= touch_buffer: alerts.append(("ãƒªãƒ†ã‚¹ãƒˆå£²ã‚Šå€™è£œ", f"Lv {lv:.3f}"))


# --- ç›´è¿‘10æœ¬ã®ã‚·ã‚°ãƒŠãƒ«æ•°ã§æ–°è¦åˆ¶å¾¡ ---
recent_signals = alerts[-10:]  # ç›´è¿‘10æœ¬
allow = True
if len(recent_signals) >= 4:   # 4ä»¶ä»¥ä¸Šãªã‚‰æ–°è¦ã¯å‡ºã•ãªã„
    allow = False

if not alerts:
    st.info("ç›´è¿‘ãƒãƒ¼ã§ã¯ã‚·ã‚°ãƒŠãƒ«ãªã—ã€‚")
else:
    for kind, msg in alerts:
        if apply_news_filter and near_news(ts_last):
            st.warning(f"æŠ‘åˆ¶ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹è¿‘å‚ï¼‰: {kind} - {msg} @ {ts_last}")
        else:
            st.success(f"{kind}: {msg} @ {ts_last}")

# ---------------- ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆRetestæŒ‡æ•°ã¤ã / ãƒãƒ¼ãƒ‰æŠ‘åˆ¶ã¯å¾“æ¥é€šã‚Šï¼‰ ----------------
def compute_retest(series_close: pd.Series, target: float, start_idx: int, K: int, tol_abs: float):
    hits = 0; checks = 0; hit_once = False
    last = len(series_close) - 1
    for j in range(start_idx+1, min(start_idx+K+1, last+1)):
        px = float(series_close.iloc[j])
        checks += 1
        if abs(px - target) <= tol_abs:
            hits += 1
            hit_once = True
    idx_val = (hits / checks) if checks > 0 else 0.0
    return idx_val, hit_once

def backtest(df: pd.DataFrame, levels: list, fwd_n: int, break_buffer_arg: float,
             spread_pips: float, news_df: pd.DataFrame, news_win: int,
             news_imp_min: int, apply_news: bool, signal_mode: str, retest_wait_k_arg: int,
             touch_buffer: float):
    rows=[]
    if len(df) <= fwd_n+1:
        return pd.DataFrame(columns=["time","mode","level_or_val","dir","entry","exit","ret_pips","retest_index","retest_hit"])
    pv = pip_value("USDJPY")
    close_s = df["close"]
    use_imp_mode = (news_filter_mode == "é‡è¦åº¦åˆ¥ï¼ˆèµ¤å½±ã¨åŒã˜ï¼‰")
    for i in range(1, len(df)-fwd_n):
        t = df.index[i]
        c  = float(df["close"].iloc[i])
        l1 = float(df["low"].iloc[i-1]); h1 = float(df["high"].iloc[i-1])
        # ãƒãƒ¼ãƒ‰æŠ‘åˆ¶
        if apply_news:
            if use_imp_mode:
                if is_suppressed(t, windows_df): 
                    continue
            else:
                if not news_df.empty:
                    win = pd.Timedelta(minutes=news_win)
                    if ((news_df["importance"] >= news_imp_min) & (news_df["time"].between(t-win, t+win))).any():
                        continue
        # ä»¥é™ã¯å¾“æ¥é€šã‚Š
        if signal_mode == "æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
            for lv in levels:
                if (c > lv + break_buffer_arg) and (l1 <= lv):
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, lv, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="æ°´å¹³ãƒ–ãƒ¬ã‚¤ã‚¯ä¸Š", level_or_val=float(lv),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if (c < lv - break_buffer_arg) and (h1 >= lv):
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, lv, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="æ°´å¹³ãƒ–ãƒ¬ã‚¤ã‚¯ä¸‹", level_or_val=float(lv),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))
        elif signal_mode == "ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)":
            if trend:
                tl = trend["y1"]
                if (c > tl + break_buffer_arg) and (l1 <= tl):
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, tl, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="TLãƒ–ãƒ¬ã‚¤ã‚¯ä¸Š", level_or_val=float(tl),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if (c < tl - break_buffer_arg) and (h1 >= tl):
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, tl, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="TLãƒ–ãƒ¬ã‚¤ã‚¯ä¸‹", level_or_val=float(tl),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))

        elif signal_mode == "ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘/ä¸‹æŠœã‘(çµ‚å€¤)":
            if trend and trend["sigma"] > 0:
                up = trend["y1"] + chan_k*trend["sigma"]
                dn = trend["y1"] - chan_k*trend["sigma"]
                if c > up + break_buffer_arg:
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, up, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘", level_or_val=float(up),
                                     dir="long", entry=entry, exit=exitp,
                                     ret_pips=(exitp-entry)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))
                if c < dn - break_buffer_arg:
                    entry, exitp = c, float(df["close"].iloc[i+fwd_n])
                    ri, rh = compute_retest(close_s, dn, i, int(retest_wait_k_arg), float(touch_buffer))
                    rows.append(dict(time=t, mode="ãƒãƒ£ãƒãƒ«ä¸‹æŠœã‘", level_or_val=float(dn),
                                     dir="short", entry=entry, exit=exitp,
                                     ret_pips=(entry-exitp)/pv - spread_pips,
                                     retest_index=ri, retest_hit=rh))

        elif signal_mode == "ãƒªãƒ†ã‚¹ãƒˆæŒ‡å€¤(æ°´å¹³ç·š)":
            K = int(retest_wait_k_arg)
            for lv in levels:
                up_break = (c > lv + break_buffer_arg) and (l1 <= lv)
                dn_break = (c < lv - break_buffer_arg) and (h1 >= lv)

                # ä¸Šæ–¹å‘ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã€K æœ¬ä»¥å†…ã«ãƒªãƒ†ã‚¹ãƒˆâ†’ãã®â€œãƒªãƒ†ã‚¹ãƒˆæ™‚åˆ»â€ã§ç´„å®š
                if up_break:
                    for j in range(i+1, min(i+K, len(df)-fwd_n)):
                        if abs(float(df["close"].iloc[j]) - lv) <= touch_buffer:
                            entry = float(df["close"].iloc[j]); exitp = float(df["close"].iloc[j+fwd_n])
                            ri, rh = compute_retest(close_s, lv, i, K, float(touch_buffer))
                            rows.append(dict(time=df.index[j], mode="ãƒªãƒ†ã‚¹ãƒˆ(L)", level_or_val=float(lv),
                                             dir="long", entry=entry, exit=exitp,
                                             ret_pips=(exitp-entry)/pv - spread_pips,
                                             retest_index=ri, retest_hit=rh))
                            break

                # ä¸‹æ–¹å‘ãƒ–ãƒ¬ã‚¤ã‚¯å¾Œã®ãƒªãƒ†ã‚¹ãƒˆ
                if dn_break:
                    for j in range(i+1, min(i+K, len(df)-fwd_n)):
                        if abs(float(df["close"].iloc[j]) - lv) <= touch_buffer:
                            entry = float(df["close"].iloc[j]); exitp = float(df["close"].iloc[j+fwd_n])
                            ri, rh = compute_retest(close_s, lv, i, K, float(touch_buffer))
                            rows.append(dict(time=df.index[j], mode="ãƒªãƒ†ã‚¹ãƒˆ(S)", level_or_val=float(lv),
                                             dir="short", entry=entry, exit=exitp,
                                             ret_pips=(entry-exitp)/pv - spread_pips,
                                             retest_index=ri, retest_hit=rh))
                            break

    return pd.DataFrame(rows)

bt_df = None
if run_bt:
    with st.spinner("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­..."):
        bt_df = backtest_rolling(
            df=df,
            fwd_n=fwd_n,
            break_buffer_arg=break_buffer,
            spread_pips=spread_pips,
            news_win=news_win,
            news_imp_min=news_imp_min,
            apply_news=apply_news_filter,
            signal_mode=signal_mode,
            retest_wait_k_arg=retest_wait_k,
            touch_buffer=touch_buffer
        )
    if bt_df is None or bt_df.empty:
        st.warning("ãƒˆãƒ¬ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/ãƒ¢ãƒ¼ãƒ‰ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.subheader("ğŸ“Š ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœï¼ˆ" + signal_mode + "ï¼‰")
        total = len(bt_df)
        wins = int((bt_df["ret_pips"] > 0).sum())
        mean = float(bt_df["ret_pips"].mean())
        std  = float(bt_df["ret_pips"].std()) if total > 1 else float("nan")
        sharpe = mean / std if std and not math.isnan(std) and std>1e-9 else float("nan")

        c1,c2,c3,c4 = st.columns(4)
        c1.metric("ä»¶æ•°", total)
        c2.metric("å‹ç‡", f"{wins/total*100:.1f}%")
        c3.metric("å¹³å‡pips", f"{mean:.2f}")
        c4.metric("Sharpe(æ“¬ä¼¼)", f"{sharpe:.2f}" if not math.isnan(sharpe) else "n/a")

        st.write("æ–¹å‘åˆ¥ï¼š")
        st.dataframe(bt_df.groupby("dir")["ret_pips"].agg(["count","mean","std"]))

        st.write("ã‚·ã‚°ãƒŠãƒ«åˆ¥ï¼š")
        st.dataframe(bt_df.groupby("mode")["ret_pips"].agg(["count","mean","std"]).sort_values("mean", ascending=False))

        st.write("ãƒªãƒ†ã‚¹ãƒˆã‚ã‚Š/ãªã— æ¯”è¼ƒï¼š")
        comp = bt_df.copy()
        comp["retest_bucket"] = comp["retest_hit"].map({True:"ã‚ã‚Š", False:"ãªã—"})
        st.dataframe(comp.groupby("retest_bucket")["ret_pips"].agg(
            ä»¶æ•°="count", å‹ç‡=lambda s: (s>0).mean()*100, å¹³å‡pips="mean", STD="std", RetestæŒ‡æ•°å¹³å‡=lambda s: comp.loc[s.index,"retest_index"].mean()
        ).round({"å‹ç‡":1, "å¹³å‡pips":2, "STD":2, "RetestæŒ‡æ•°å¹³å‡":2}))

        ch = bt_df[bt_df["mode"].isin(["ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘","ãƒãƒ£ãƒãƒ«ä¸‹æŠœã‘"])]
        if not ch.empty:
            st.write("ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘/ä¸‹æŠœã‘ã®çµ±è¨ˆï¼š")
            st.dataframe(ch.groupby("mode")["ret_pips"].agg(["count","mean","std"]).rename(
                index={"ãƒãƒ£ãƒãƒ«ä¸ŠæŠœã‘":"ä¸ŠæŠœã‘","ãƒãƒ£ãƒãƒ«ä¸‹æŠœã‘":"ä¸‹æŠœã‘"}).sort_values("mean", ascending=False))

# ---------------- ç™ºæ³¨ï¼ˆä»»æ„ãƒ»ç°¡æ˜“ï¼‰ ----------------
st.sidebar.markdown("---")
st.subheader("ğŸ§ª ç™ºæ³¨ï¼ˆä»»æ„ï¼‰")
side = st.selectbox("æ–¹å‘", ["BUY","SELL"], index=0)
order_units = st.sidebar.number_input("ç™ºæ³¨æ•°é‡", value=1000, step=1000)
paper_trade = st.sidebar.checkbox("ç´™ãƒˆãƒ¬ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’æœ‰åŠ¹", value=True)
use_oanda = st.sidebar.checkbox("OANDAç™ºæ³¨ã‚’æœ‰åŠ¹", value=False)
oanda_token = st.sidebar.text_input("OANDA Tokenï¼ˆenv: OANDA_TOKENï¼‰", value=os.getenv("OANDA_TOKEN",""))
oanda_account = st.sidebar.text_input("OANDA AccountIDï¼ˆenv: OANDA_ACCOUNTï¼‰", value=os.getenv("OANDA_ACCOUNT",""))
oanda_env = st.sidebar.selectbox("OANDAç’°å¢ƒ", ["practice","live"], index=0)

if st.button("æˆè¡Œç™ºæ³¨"):
    price_now = float(df["close"].iloc[-1]); logs = []
    if paper_trade:
        st.success(f"[ç´™ãƒˆãƒ¬] {side} {order_units} @ {price_now:.3f}")
        logs.append({"type":"paper","side":side,"units":order_units,"price":price_now,"time":str(df.index[-1])})
    if use_oanda and oanda_token and oanda_account:
        try:
            import requests
            inst = "USD_JPY"
            base = "https://api-fxpractice.oanda.com" if oanda_env=="practice" else "https://api-fxtrade.oanda.com"
            headers = {"Authorization": f"Bearer {oanda_token}", "Content-Type":"application/json"}
            data = {"order":{"units": str(order_units if side=="BUY" else -order_units),
                             "instrument": inst, "timeInForce":"FOK","type":"MARKET","positionFill":"DEFAULT"}}
            r = requests.post(f"{base}/v3/accounts/{oanda_account}/orders", headers=headers, data=json.dumps(data), timeout=10)
            st.success(f"[OANDA] æˆè¡Œ {side} é€ä¿¡OK") if r.status_code in (200,201) else st.error(f"[OANDA] {r.status_code} {r.text}")
            logs.append({"type":"oanda","resp":r.json() if r.ok else r.text})
        except Exception as e:
            st.error(f"[OANDA] ä¾‹å¤–: {e}")
        # if use_mt5:
        #     try:
        #         import MetaTrader5 as mt5
        #         if not mt5.initialize():
        #             st.error(f"[MT5] åˆæœŸåŒ–å¤±æ•—: {mt5.last_error()}")
        #         else:
        #             symbol_mt5 = mt5_symbol
        #             mt5.symbol_select(symbol_mt5, True)
        #             lot = 0.1
        #             typ = mt5.ORDER_TYPE_BUY if side=="BUY" else mt5.ORDER_TYPE_SELL
        #             req = {"action": mt5.TRADE_ACTION_DEAL, "symbol": symbol_mt5, "volume": lot, "type": typ,
        #                    "deviation": 10, "magic": 123456, "comment": "AutoLines", "type_filling": mt5.ORDER_FILLING_FOK}
        #             res = mt5.order_send(req)
        #             st.success(f"[MT5] æˆè¡Œ {side} OK: {res.order}") if res.retcode == mt5.TRADE_RETCODE_DONE else st.error(f"[MT5] å¤±æ•—: {res.retcode}")
        #             mt5.shutdown()
        #     except Exception as e:
        #         st.error(f"[MT5] ä¾‹å¤–: {e}")
    st.session_state.setdefault("trade_logs", [])
    st.session_state["trade_logs"].extend(logs)

with st.expander("å®Ÿè¡Œãƒ­ã‚°"):
    st.write(st.session_state.get("trade_logs", []))

# ---------------- ã‚¹ã‚³ã‚¢è¡¨ ----------------
st.subheader("â­ é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆä¸Šä½ï¼‰")
if not score_df.empty:
    st.dataframe(score_df[["level","score","touches","session_ratio"]].head(15))
else:
    st.info("ãƒ¬ãƒ™ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚eps/min_samples/look ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")

# ---------------- æœŸå¾…pipsï¼ˆæ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯ã®éå»å¹³å‡ï¼‰ ----------------
def compute_expected_pips_table_for_levels(df, levels, fwd_n, break_buffer, spread_pips,
                                           news_df, news_win, news_imp_min, apply_news_filter,
                                           touch_buffer, retest_wait_k) -> tuple:
    """
    æ°´å¹³ç·šã”ã¨ã®æœŸå¾…pipsãƒ†ãƒ¼ãƒ–ãƒ«ã¨æ–¹å‘åˆ¥é›†è¨ˆã‚’è¿”ã™
    Returns:
        by_level_dir: pd.DataFrame or None
        by_dir: pd.DataFrame or None
    """
    try:
        # levels å¼•æ•°ã¯äº’æ›ã®ãŸã‚ã«æ®‹ã™ãŒã€ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã§æ¯ãƒãƒ¼å†è¨ˆç®—ã•ã‚Œã‚‹
        bt = backtest_rolling(
            df=df,
            fwd_n=fwd_n,
            break_buffer_arg=break_buffer,
            spread_pips=spread_pips,
            news_win=news_win,
            news_imp_min=news_imp_min,
            apply_news=apply_news_filter,
            signal_mode="æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)",
            retest_wait_k_arg=retest_wait_k,
            touch_buffer=touch_buffer
        )
        if bt is None or bt.empty:
            return None, None
        by_level_dir = (bt.groupby(["level_or_val","dir"])["ret_pips"]
                          .agg(avg="mean", n="count").reset_index()
                          .rename(columns={"level_or_val":"level"}))
        by_dir = (bt.groupby("dir")["ret_pips"].agg(avg="mean", n="count").reset_index())
        return by_level_dir, by_dir
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã‚’è¿”ã™ï¼ˆUIç”¨é€”ï¼‰
        return None, None

# ---------------- ã€Œä»Šã‹ã‚‰ã®ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ã€ï¼‹ æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚° ----------------
if show_break_prob:
    st.subheader("ğŸ¯ ä»Šã‹ã‚‰ã®æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ï¼ˆæ¬¡Hæœ¬ä»¥å†…ï¼‰")
    model, Xcols, meta = load_break_model("models/break_model.joblib")
    if model is None:
        st.info("å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆmodels/break_model.joblibï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« ai_train_break.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
            ts_now = df.index[-1]
            # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã§ä¸€ä½ã®æ°´å¹³ç·šã®ã¿è¡¨ç¤º
            if score_df is not None and not score_df.empty:
                top_level = float(score_df.sort_values("score", ascending=False)["level"].iloc[0])
                use_levels = [top_level]
            else:
                use_levels = [levels[0]] if levels else []
            try:
                # æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨ˆç®—
                prob_df = build_level_break_prob_table(
                    df=df,
                    ts_now=None,  # ts_nowã¯ä½¿ã‚ãªã„ãŸã‚Noneã§æ˜ç¤º
                    use_levels=use_levels,
                    use_cols=meta.get("features", use_cols),
                    touch_buffer=touch_buffer,
                    model=model,
                    meta=meta,
                    make_features_for_level=make_features_for_level,  # ç‰¹å¾´é‡ç”Ÿæˆé–¢æ•°
                    predict_with_session_theta=predict_with_session_theta,  # ãƒ¢ãƒ‡ãƒ«æ¨è«–é–¢æ•°
                )
            except Exception as e:
                import traceback
                st.error(f"ãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
                st.error(traceback.format_exc())  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚‚è¡¨ç¤º
                prob_df = None

            ev_table, ev_dir = compute_expected_pips_table_for_levels(
                df, levels, fwd_n, break_buffer, spread_pips,
                news_df, news_win, news_imp_min, apply_news_filter,
                touch_buffer, retest_wait_k
            )

            def get_expected_for(level, direction):
                if ev_table is not None and not ev_table.empty:
                    sub = ev_table[(ev_table["level"]==float(level)) & (ev_table["dir"]==direction)]
                    if not sub.empty and int(sub["n"].iloc[0]) >= ev_level_min_samples:
                        return float(sub["avg"].iloc[0]), int(sub["n"].iloc[0])
                if ev_dir is not None and not ev_dir.empty:
                    subd = ev_dir[ev_dir["dir"]==direction]
                    if not subd.empty:
                        return float(subd["avg"].iloc[0]), int(subd["n"].iloc[0])
                return 0.0, 0


            # æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®è¨ˆç®—éƒ¨ï¼ˆè©²å½“ç®‡æ‰€ï¼‰
            exp_rows=[]
            for _, r in prob_df.iterrows():
                lv = float(r["level"])
                e_up, n_up = get_expected_for(lv, "long")
                e_dn, n_dn = get_expected_for(lv, "short")

                e_up_net = e_up - extra_cost_pips
                e_dn_net = e_dn - extra_cost_pips
                ev_up = r["P_up"] * e_up_net
                ev_dn = r["P_dn"] * e_dn_net

                BUY = "BUY"
                SELL = "SELL"
                # æœ€è‰¯ã®æ–¹å‘ã¨æœŸå¾…å€¤ã‚’æ±ºå®š
                best_dir = BUY if ev_up >= ev_dn else SELL
                best_ev  = ev_up if ev_up >= ev_dn else ev_dn
                exp_rows.append({
                    "level": lv,
                    "P_up": r.get("P_up", 0),
                    "P_dn": r.get("P_dn", 0),
                    "E_pips_up": e_up_net,  # ã‚³ã‚¹ãƒˆæ§é™¤å¾Œã®å¹³å‡pips
                    "E_pips_dn": e_dn_net,
                    "EV_up": ev_up,
                    "EV_dn": ev_dn,
                    "best_action": best_dir,
                    "best_EV": best_ev,
                    "samples_up": n_up,
                    "samples_dn": n_dn,
                    "cost_pips": extra_cost_pips,  # é€æ˜æ€§ã®ãŸã‚åˆ—ã«æ®‹ã™
                })

            # æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ã®è¡¨ç¤º
            ev_df = (pd.DataFrame(exp_rows)
                       .sort_values("best_EV", ascending=False)
                       .reset_index(drop=True))

            st.dataframe(
                ev_df[["level","P_up","P_dn","E_pips_up","E_pips_dn","EV_up","EV_dn","best_action","best_EV","samples_up","samples_dn"]]
                    .style.format({
                        "level":"{:.3f}",
                        "P_up":"{:.1%}","P_dn":"{:.1%}",
                        "E_pips_up":"{:.2f}","E_pips_dn":"{:.2f}",
                        "EV_up":"{:.2f}","EV_dn":"{:.2f}",
                        "best_EV":"{:.2f}"
                    })
            )

# === äºˆæ¸¬ãƒ–ãƒ­ãƒƒã‚¯ç›´å¾Œï¼ˆP_up/P_dn ã‚’é›†è¨ˆã™ã‚‹ç®‡æ‰€ã®å¾Œï¼‰ã«ç¢ºç‡ãƒãƒƒãƒ•ã‚¡ãƒ»PSIãƒ»Î¸è¶…éç‡é›†è¨ˆ ===
def init_session_state_buffer():
    if "prob_buffer" not in st.session_state:
        st.session_state.prob_buffer = []

# ...existing code...

    # === äºˆæ¸¬ãƒ–ãƒ­ãƒƒã‚¯ç›´å¾Œï¼ˆP_up/P_dn ã‚’é›†è¨ˆã™ã‚‹ç®‡æ‰€ã®å¾Œï¼‰ã«ç¢ºç‡ãƒãƒƒãƒ•ã‚¡ãƒ»PSIãƒ»Î¸è¶…éç‡é›†è¨ˆ ===
    init_session_state_buffer()

    curr_probs = update_prob_buffer(prob_df)
    psi_val, sev, ex_rate = calc_psi_and_exrate(curr_probs, baseline_probs, theta_up, theta_dn)

    # ä¾¡æ ¼ã®æœ€çµ‚æ™‚åˆ»ï¼ˆJSTæƒ³å®šã®indexï¼‰ã‚’å–å¾—
    try:
        last_ts = df.index[-1].tz_convert("UTC")
    except Exception:
        last_ts = None

    hr = healthcheck(
        model_path="models/break_model.joblib",
        meta_path="models/break_meta.json",
        windows_df=windows_df,
        last_price_ts=last_ts,  # Noneå¯
        max_age_min=5
    )
    st.write(f"**overall**: {'âœ…OK' if hr.ok else 'âš ï¸CHECK'}  |  price_age: {hr.details.get('price_feed','?')}")
    st.write(f"- model: {hr.details.get('model')}")
    st.write(f"- meta : {hr.details.get('meta')}")
    st.write(f"- events: {hr.details.get('event_windows')}")

# === äºˆæ¸¬ãƒ–ãƒ­ãƒƒã‚¯ç›´å¾Œï¼ˆP_up/P_dn ã‚’é›†è¨ˆã™ã‚‹ç®‡æ‰€ã®å¾Œï¼‰ã«ç¢ºç‡ãƒãƒƒãƒ•ã‚¡ãƒ»PSIãƒ»Î¸è¶…éç‡é›†è¨ˆ ===

# ...existing code...

    with st.sidebar.expander("ğŸ©º Health & Drift", expanded=True):
        st.write(f"**overall**: {'âœ…OK' if hr.ok else 'âš ï¸CHECK'}  |  price_age: {hr.details.get('price_feed','?')}")
        st.write(f"- model: {hr.details.get('model')}")
        st.write(f"- meta : {hr.details.get('meta')}")
        st.write(f"- events: {hr.details.get('event_windows')}")
        st.write("---")
        st.write(f"**PSI**: {psi_val:.3f}  ({sev})")
        st.progress(min(max((0.3 - float(psi_val if not math.isnan(psi_val) else 0))/0.3, 0.0), 1.0))
        st.caption("PSI<0.10:å®‰å®š / <0.25:æ³¨æ„ / â‰¥0.25:ãƒ‰ãƒªãƒ•ãƒˆ")
        st.write(f"**Î¸ exceed rate (now)**: {ex_rate:.2%}" if not np.isnan(ex_rate) else "Î¸ exceed rate: n/a")

    ev_df = (pd.DataFrame(exp_rows)
               .sort_values("best_EV", ascending=False)
               .reset_index(drop=True))

    st.dataframe(
        ev_df[["level","P_up","P_dn","E_pips_up","E_pips_dn","EV_up","EV_dn","best_action","best_EV","samples_up","samples_dn"]]
            .style.format({
                "level":"{:.3f}",
                "P_up":"{:.1%}","P_dn":"{:.1%}",
                "E_pips_up":"{:.2f}","E_pips_dn":"{:.2f}",
                "EV_up":"{:.2f}","EV_dn":"{:.2f}",
                "best_EV":"{:.2f}"
            })
    )
    st.caption(
        f"Hï¼ˆå­¦ç¿’ï¼‰: {meta.get('H','?')} / Hï¼ˆè¡¨ç¤ºï¼‰: {break_prob_h} / "
        f"break_buffer(å­¦ç¿’): {meta.get('break_buffer','?')} / touch_buffer(å­¦ç¿’): {meta.get('touch_buffer','?')}  |  "
        f"Calibration: { 'ON' if meta.get('calibrated') else 'OFF' }"
    )
    st.caption(
        f"â€» æœŸå¾…pipsã¯ã€æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯(çµ‚å€¤)ã€ã®ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå¹³å‡ã‹ã‚‰æ¨å®š"
    )

if show_ev_rank and not show_break_prob:
    st.info("æœŸå¾…å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¯ã€Œä»Šã‹ã‚‰ã®æ°´å¹³ç·šãƒ–ãƒ¬ã‚¤ã‚¯ç¢ºç‡ã€ã‚’ONã«ã™ã‚‹ã¨è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# ---------------- ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã€Œæ¬¡ã®å‹•ãã®äºˆæ¸¬ã€ä¸€è¦§ï¼ˆæ¸¬å®šå€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ & EVï¼‰ ----------------
st.subheader("ğŸ§­ ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼šæ¬¡ã®å‹•ãã®äºˆæ¸¬ï¼ˆæ¸¬å®šå€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ & EVï¼‰")
if not patterns:
    st.info("ç¾åœ¨ã€æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚lookback/æ„Ÿåº¦ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
else:
    ev_table, ev_dir = compute_expected_pips_table_for_levels(
        df, levels, fwd_n, break_buffer, spread_pips,
        news_df, news_win, news_imp_min, apply_news_filter,
        touch_buffer, retest_wait_k
    )
    model, Xcols, meta = load_break_model(prob_model_path)

    rows=[]
    ts_now = df.index[-1]
    for p in patterns:
        meas = measured_targets(p)
        tgt_up_price = df["close"].iloc[-1] + meas["up"]
        tgt_dn_price = df["close"].iloc[-1] + meas["down"]

        upper_level = None; lower_level = None
        kind = p.get('kind') if isinstance(p, dict) else getattr(p, 'kind', None)
        params = p['params'] if isinstance(p, dict) else getattr(p, 'params', {})
        if kind and kind.startswith("triangle"):
            subp = df.loc[params["sub_start"]:params["sub_end"]]
            m1,b1 = params["upper"]; m2,b2 = params["lower"]
            x_now  = len(subp)-1
            y_u = float(m1*x_now + b1); y_l = float(m2*x_now + b2)
            upper_level, lower_level = y_u, y_l
        elif kind=="rectangle":
            upper_level, lower_level = float(params["upper"]), float(params["lower"])
        elif kind=="double_top":
            upper_level, lower_level = None, float(params["neck"])
        elif kind=="double_bottom":
            upper_level, lower_level = float(params["neck"]), None
        elif kind in ("flag_up","flag_dn","pennant"):
            upper_level, lower_level = float(params["upper_now"]), float(params["lower_now"])
        elif kind=="head_shoulders":
            upper_level, lower_level = None, float(params["neck"])
        elif kind=="inverse_head_shoulders":
            upper_level, lower_level = float(params["neck"]), None

        P_up = P_dn = np.nan
        if model is not None:
            if upper_level is not None:
                f_up = make_features_for_level(df, ts_now, upper_level, +1, touch_buffer)
                if isinstance(f_up, dict):
                    row_up = {k: f_up.get(k, 0.0) for k in use_cols}
                elif isinstance(f_up, list):
                    row_up = dict(zip(use_cols, f_up))
                else:
                    raise TypeError("f_up must be dict or list")
                row_up["timestamp"] = ts_now
                df_up = pd.DataFrame([row_up])
                for col in use_cols:
                    if col not in df_up.columns:
                        df_up[col] = 0.0
                df_up = df_up[use_cols]
                df_up["timestamp"] = ts_now
                pred_up = predict_with_session_theta(df_up, model, use_cols, meta)
                P_up = float(pred_up["proba"].iloc[0])
            if lower_level is not None:
                f_dn = make_features_for_level(df, ts_now, lower_level, -1, touch_buffer)
                if isinstance(f_dn, dict):
                    row_dn = {k: f_dn.get(k, 0.0) for k in use_cols}
                elif isinstance(f_dn, list):
                    row_dn = dict(zip(use_cols, f_dn))
                else:
                    raise TypeError("f_dn must be dict or list")
                row_dn["timestamp"] = ts_now
                df_dn = pd.DataFrame([row_dn])
                for col in use_cols:
                    if col not in df_dn.columns:
                        df_dn[col] = 0.0
                df_dn = df_dn[use_cols]
                df_dn["timestamp"] = ts_now
                pred_dn = predict_with_session_theta(df_dn, model, use_cols, meta)
                P_dn = float(pred_dn["proba"].iloc[0])

        def _ev_for(level, direction):
            if level is None: return (0.0, 0, np.nan)
            if ev_table is not None and not ev_table.empty:
                sublv = ev_table[(ev_table["dir"]==direction)]
                if not sublv.empty:
                    idx_min = (sublv["level"]-float(level)).abs().idxmin()
                    avg, n = float(sublv.loc[idx_min,"avg"]), int(sublv.loc[idx_min,"n"])
                    if n >= ev_level_min_samples:
                        p = P_up if direction=="long" else P_dn
                        avg_net = avg - extra_cost_pips
                        return (avg_net, n, (p*avg_net) if not np.isnan(p) else np.nan)
            if ev_dir is not None and not ev_dir.empty:
                dsub = ev_dir[ev_dir["dir"]==direction]
                if not dsub.empty:
                    avg, n = float(dsub["avg"].iloc[0]), int(dsub["n"].iloc[0])
                    p = P_up if direction=="long" else P_dn
                    avg_net = avg - extra_cost_pips
                    return (avg_net, n, (p*avg_net) if not np.isnan(p) else np.nan)
            return (0.0, 0, np.nan)

        E_up, N_up, EV_up = _ev_for(upper_level, "long")
        E_dn, N_dn, EV_dn = _ev_for(lower_level, "short")

        # äºˆæ¸¬å€¤ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        print(f"[DEBUG] upper_level={upper_level}, lower_level={lower_level}")
        print(f"[DEBUG] df_up: {df_up if 'df_up' in locals() else None}")
        print(f"[DEBUG] df_dn: {df_dn if 'df_dn' in locals() else None}")
        print(f"[DEBUG] P_up={P_up}, P_dn={P_dn}")
        kind = p.get('kind') if isinstance(p, dict) else getattr(p, 'kind', None)
        quality = p.get('quality') if isinstance(p, dict) else getattr(p, 'quality', None)
        rows.append(dict(
            pattern=kind, quality=round(quality,1) if quality is not None else None,
            upper_level=upper_level, lower_level=lower_level,
            P_up=P_up, P_dn=P_dn,
            E_pips_up=E_up, E_pips_dn=E_dn,
            EV_up=EV_up, EV_dn=EV_dn,
            target_up=tgt_up_price, target_dn=tgt_dn_price
        ))

    out = pd.DataFrame(rows)
    if not out.empty:
        out["best_action"] = out.apply(lambda r: "BUY" if (pd.notna(r["EV_up"]) and (r["EV_up"]>= (r["EV_dn"] if pd.notna(r["EV_dn"]) else -1e9))) else "SELL", axis=1)
        out["best_EV"] = out.apply(lambda r: r["EV_up"] if r["best_action"]=="BUY" else r["EV_dn"], axis=1)
        # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿å‹å¤‰æ›ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€strå‹ã«ã¯é©ç”¨ã—ãªã„
        num_cols = ["upper_level","lower_level","P_up","P_dn","E_pips_up","E_pips_dn","EV_up","EV_dn","best_EV","target_up","target_dn"]
        for col in num_cols:
            if col in out.columns:
                out[col] = pd.to_numeric(out[col], errors="coerce")
        st.dataframe(
            out[["pattern","quality","upper_level","lower_level","P_up","P_dn",
                 "E_pips_up","E_pips_dn","EV_up","EV_dn","best_action","best_EV",
                 "target_up","target_dn"]]
            .style.format({
                "upper_level":"{:.3f}", "lower_level":"{:.3f}",
                "P_up":"{:.1%}", "P_dn":"{:.1%}",
                "E_pips_up":"{:.2f}", "E_pips_dn":"{:.2f}",
                "EV_up":"{:.2f}", "EV_dn":"{:.2f}", "best_EV":"{:.2f}",
                "target_up":"{:.3f}", "target_dn":"{:.3f}",
            })
        )
        st.caption("â€» P_up/P_dn ã¯å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆæ°´å¹³ãƒ–ãƒ¬ã‚¤ã‚¯ï¼‰ã‚’ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¢ƒç•Œï¼ˆä¸Šè¾º/ä¸‹è¾º/ãƒãƒƒã‚¯/ãƒãƒ£ãƒãƒ«ï¼‰ã«å½“ã¦ã¯ã‚ã¦æ¨å®šã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯æ¸¬å®šå€¤ï¼ˆæ——ç«¿ãƒ»åšã¿ãƒ»ãƒ˜ãƒƒãƒ‰é«˜ã• ç­‰ï¼‰ã€‚")
    else:
        st.info("ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒã€ãƒ†ãƒ¼ãƒ–ãƒ«åŒ–ã§ãã‚‹ååˆ†ãªæŒ‡æ¨™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.markdown("---")
st.markdown("""
## å…è²¬äº‹é … / Disclaimer

æœ¬ã‚¢ãƒ—ãƒªã¯ã€éå»ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç­‰ã‚’ã‚‚ã¨ã«ä½œæˆã•ã‚ŒãŸåˆ†æãƒ„ãƒ¼ãƒ«ãƒ»æ•™è‚²ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã‚ã‚Šã€
é‡‘èå•†å“å–å¼•æ³•ã«åŸºã¥ãæŠ•è³‡åŠ©è¨€ãƒ»ä»£ç†æ¥­å‹™ã‚’è¡Œã†ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

æœ¬ã‚¢ãƒ—ãƒªãŒæä¾›ã™ã‚‹æƒ…å ±ãƒ»ã‚·ã‚°ãƒŠãƒ«ãƒ»äºˆæ¸¬çµæœã¯ã€å°†æ¥ã®æˆæœã‚„åˆ©ç›Šã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ãªãã€
æŠ•è³‡åˆ¤æ–­ã¯ã™ã¹ã¦åˆ©ç”¨è€…ã”è‡ªèº«ã®è²¬ä»»ã«ãŠã„ã¦è¡Œã£ã¦ãã ã•ã„ã€‚

å½“æ–¹ã¯ã€æœ¬ã‚¢ãƒ—ãƒªã®åˆ©ç”¨ã«ã‚ˆã‚Šç”Ÿã˜ãŸã„ã‹ãªã‚‹æå¤±ãƒ»æå®³ã«ã¤ã„ã¦ã‚‚ã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚

ã¾ãŸã€æœ¬ã‚¢ãƒ—ãƒªãŒä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ Yahoo Finance ç­‰ã®å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’é€šã˜ã¦å–å¾—ã—ã¦ã„ã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ãƒ»å®Œå…¨æ€§ã¯ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã®ã§ã”äº†æ‰¿ãã ã•ã„ã€‚

æœ¬ã‚¢ãƒ—ãƒªã®åˆ©ç”¨ã«ã‚ˆã‚Šç”Ÿã˜ã‚‹ã‚ã‚‰ã‚†ã‚‹ãƒªã‚¹ã‚¯ã¯ã€åˆ©ç”¨è€…ã”è‡ªèº«ã®è‡ªå·±è²¬ä»»ã«ãŠã„ã¦ã”å¯¾å¿œãã ã•ã„ã€‚
""")